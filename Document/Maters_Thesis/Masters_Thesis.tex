\documentclass[a4paper,11pt]{jsreport}

\usepackage{comment}
\usepackage{float}
\usepackage{color}
\usepackage{multicol}
\usepackage[dvipdfmx]{graphicx}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{bm}
\usepackage{url}
\usepackage{underscore}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage{ulem}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[ipaex]{pxchfon}
\usepackage[dvipdfmx]{hyperref} % ハイパーリンク
\usepackage{pxjahyper}
\usepackage[version=4]{mhchem} % 化学式をかくため
\usepackage[hang,small,bf]{caption}
\usepackage[subrefformat=parens]{subcaption}
\hypersetup{
setpagesize=false,
 bookmarksnumbered=true,%
 bookmarksopen=true,%
 colorlinks=true,%
 linkcolor=black,
 citecolor=black,
} % 目次の赤線を表示させないようにする
\captionsetup{compatibility=false}
\usetikzlibrary{positioning, intersections, calc, arrows.meta,math} %tikzのlibrary

\begin{document}

\thispagestyle{empty}
\begin{center}

  \vspace{20mm}
  {\Large\noindent 2024年度 修士論文}\\
  \vspace{40mm}
  {\Huge\noindent\textbf{機械学習を用いた}}\\
  \medskip
  {\Huge\noindent\textbf{イジング模型の相転移検出}}\\
  \vspace{\baselineskip}
  \vspace{40mm}

  {\Large\noindent
    2024年2月2日\\
    \vspace{\baselineskip}
    指導教員 \ 藤原高徳    \\
    \vspace{\baselineskip}
    茨城大学大学院\\
    理工学研究科 \ 量子線科学専攻 \\
    \vspace{\baselineskip}
    学籍番号 \ 22NM021S \\
    氏名 \ 須賀 勇貴\\
  }
  \vspace{40mm}

\end{center}

\thispagestyle{empty}
\clearpage

%=====================================================================================
\renewcommand{\abstractname}{要旨}

\begin{abstract}
  研究の要旨を書く．
\end{abstract}

\thispagestyle{empty}
\clearpage

%=====================================================================================

% 目次の表示
\tableofcontents

%=====================================================================================
\pagestyle{fancy}
\lhead{\rightmark}
\renewcommand{\chaptermark}[1]{\markboth{第\ \normalfont\thechapter\ 章~~#1}{}}
%=====================================================================================
\chapter{はじめに} %章
\section{研究背景}
\section{研究目的}


\chapter{イジング模型の相転移}
\section{相転移，臨界現象とは}
\subsection{物質の三態と相転移}
私たちにとって身近な物質である水(\ce{H2O})は，固体(氷)，液体(水)，気体(水蒸気)の3つの状態をとる．これら状態は温度の違いによって明確な規則性があり，1気圧の環境であれば，氷を徐々に温めていくと，ちょうど0℃で氷から水へ相転移がおき，さらにその水を温めておけば，ちょうど100℃で水から水蒸気への相転移が起きる．特に1気圧の環境では，1気圧の環境では，水と氷が共存しうるのはちょうど0℃に限られ，水と水蒸気が共存するのはちょうど100℃に限られる．これが摂氏という温度目盛りの基準になっていることは言うまでもない.\par
液体の水の温度が0℃から100℃に変化する間，密度や粘性などの水の物性は少しずつ変化する．しかし，これはあくまでも定量的な変化であり，水から氷，水から水蒸気への変化のような定性的な(あるいは質的な)変化とは違う．氷と水蒸気のそれぞれの範囲内での変化も，やはり質的な変化を伴わない，定量的な変化である．このように，定性的な変化を伴わずに移り変われるような一連の状態をひとまとめにして，相(Phase)と呼ぶ．氷，水，水蒸気はそれぞれ固相，液相，気相という三つの相に対応する．温度などのパラメータを変化させたときに，物質が異なった相の間を移り変わる現象を相転移(phase transition)である.\par
なぜ同じ\ce{H2O}という物質が，温度を変えただけで，固相，液相，気相という全く性質の異なる状態をとるのか？．素朴に考えれば，このような変化は「部品」である．\ce{H2O}分子の性質の変化からくると考えたくなる．つまり，何らかの意味での「ミクロなルール」が変化することを想定するということである．\par
しかし，相転移が起こるのは「ミクロなルール」が変化するからではない．「ミクロなルール」が不変であっても，刑を構成する要素(今回の場合なら分子)の数がきわめて大きければ，それら相互の関連が変化することで，マクロな性質の不連続な変化が生じうる．つまり，相転移は，無数の要素が複雑に絡み合ったときに全体として生じる協力現象の一種なのである．
\section{強磁性イジング模型}
この説では，強磁性体の相転移を調べるために，強磁性イジング模型を定義する．イジング模型は，実在の強磁性体のモデルとしては全く忠実ではないが，教師整体での相転移の本質をつかむためには，きわめてすぐれたモデルである．つまり，相転移や臨界現象を引き起こすために必要最低限の要素だけを持ったモデルと言える．\par
「イジング模型」という呼び名は，このモデルの一次元でのふるまいを1924年の学位論文で調べたイジングにちなんだものである．ただし，モデルの発案者は当時イジングの指導教員であったレンツである．\par
\subsection{モデルの定義}
一辺が$L$の$d$次元立方格子を考える．全格子点の数を$N=L^d$とし，それぞれの格子点に$i=1,2,\dots ,N$と番号をつけておく．各格子点には，上向きと下向きの二つの状態をとるスピンがのっている．格子点$i$のスピンを表すスピン変数を$\sigma_i = \pm{1}$とする．$+1$が上向きスピン，$-1$が下向きスピンのに対応する．また，スピン変数$\sigma_i$に対応する物理量を$\hat{\sigma}_i$と書く．\par
系のエネルギー固有状態は，すべての格子点のスピン変数$\{ \bm{\sigma} \} = (\sigma_1, \sigma_2, \dots, \sigma_N)$と列挙することで指定できる，このようなスピンの並びのことをスピン配位と呼ぶ．それぞれのスピンが二通りの状態をとるため，系の全状態数，あるいは，スピン配位の総数は$2^N$である．スピン配位$\{ \bm{\sigma} \}$に対応するエネルギー固有値を
\begin{equation}
  E(\{ \bm{\sigma} \}) = -J \sum_{\langle i, j \rangle} \sigma_i \sigma_j
  - \mu_0 H \sum_{i=1}^{N} \sigma_i \label{イジングエネルギー}
\end{equation}
とする．第一項はスピン間の相互作用を表す項で，第二項は外部磁場$H$とスピン磁気モーメント$\mu_0$の相互作用を表す項である．第一項の和$\sum_{\langle i, j \rangle}$は互いに隣り合う格子点$i,j$すべてについての和という意味である．\par
ここでは相互作用定数$J$の値は正とする．したがって，隣り合う格子点の組$i,j$に関わる相互作用を取り出すと，
\begin{equation}
  -J \sigma_i \sigma_j =
  \begin{cases}
    -J, & \sigma_i = \sigma_j \text{のとき}    \\
    J,  & \sigma_i \neq \sigma_j \text{のとき}
  \end{cases} \label{相互作用項場合分け}
\end{equation}
となる．つまり，隣り合う格子点のスピンが揃う方がエネルギーが小さくなる．このように互いにスピンを揃えようとする相互作用を，強磁性的相互作用という．\par
\subsection{平衡状態での物理量}
ここで，イジング模型の逆温度$\beta$での平衡状態を調べる．この場合，カノニカル分布で平衡状態を記述するのが自然である．\par
分配関数は
\begin{equation}
  Z_L(\beta, H) = \sum_{(\sigma_1,\dots,\sigma_N)} \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]} \label{イジング分配関数}
\end{equation}
である．和は$2^N$通りのすべてのスピン配位についてとる．後の便利のために格子サイズを$L$とした．物理量$\hat{g}$の期待値は
\begin{equation}
  \langle \hat{g} \rangle_{\beta, H}^{\text{can}}
  := \frac{1}{Z_L(\beta, H)}  \sum_{(\sigma_1,\dots,\sigma_N)} g_{(\sigma_1,\dots,\sigma_N)} \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]}
\end{equation}
である．ここで，$g_{(\sigma_1,\dots,\sigma_N)}$は状態$(\sigma_1,\dots,\sigma_N)$における$\hat{g}$の値である．スピン一つあたりの自由エネルギーを
\begin{equation}
  f_L(\beta, H) := -\frac{1}{\beta H} \ln{Z_L(\beta, H)} \label{自由エネルギー}
\end{equation}
，物理量としての磁化を
\begin{equation}
  \hat{m} := \frac{1}{N} \sum_{j=1}^{N} \mu_0 \hat{\sigma}_j
\end{equation}
と定義する．このとき，磁化の期待値は
\begin{align}
   & m_L(\beta, H) := \langle \hat{m} \rangle_{\beta, H}^{\text{can}} \notag                                                                                       \\
   & = \frac{1}{Z_L(\beta, H)} \sum_{(\sigma_1, \dots, \sigma_N)} m_{(\sigma_1,\dots,\sigma_N)} \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]} \notag                \\
   & = \frac{1}{Z_L(\beta, H)} \sum_{(\sigma_1, \dots, \sigma_N)} \frac{1}{N} \sum_{j=1}^N \mu_0 \sigma_j \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]} \notag      \\
   & = \frac{1}{Z_L(\beta, H)} \sum_{(\sigma_1, \dots, \sigma_N)} \frac{1}{\beta N} \frac{\partial}{\partial H} \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]}\notag \\
   & = \frac{1}{\beta N} \frac{1}{Z_L(\beta, H)} \frac{\partial}{\partial H} Z_L(\beta, H)  \notag                                                                 \\
   & = \frac{1}{\beta N}\frac{\partial}{\partial H} \ln{Z_L (\beta, H)} \notag                                                                                     \\
   & = -\frac{\partial}{\partial H} f_L(\beta, H) \label{磁化と自由エネルギーの関係式}
\end{align}
と書ける．これ以降，期待値$\langle \hat{m} \rangle_{\beta, H}^{\text{can}}$のことも，単に磁化と呼ぶ．磁化とは「系がどの程度磁石になっているか」の目安である．それに対して，「系がどの程度磁石になりやすいか」の目安になるのがゼロ磁場での磁化率は
\begin{equation}
  \chi_L(\beta) := \left.\frac{\partial m_L(\beta, H)}{\partial H}\right|_{H=0}
\end{equation}
である．
\subsection{絶対零度}
まずは，絶対零度での系のふるまいを見ていく．つまり，基底状態を求めるということである．\par
式(\ref{相互作用項場合分け})から，一つのスピンに着目すれば，$\sigma_i = \sigma_j$のときエネルギーが最小になる．したがって，相互作用項$-J\sigma_i \sigma_j$を最小化する状態は，すべてのスピンが等しくなっているときであることがわかる．つまり，すべての$i$について$\sigma_i = +1$もしくは$\sigma_i = -1$のいずれかの状態である．\par
一方，外部磁場とスピンの相互作用項$-\mu_0 H \sigma_i$を最小化する状態は，磁場の符号によって変わってくる．式(\ref{イジングエネルギー})の第二項を最小化するのは，$H>0$なら，すべての$i$について$\sigma_i=1$とした状態であり，$H<0$なら，すべての$i$について$\sigma_i=-1$とした状態である．$H=0$のときは，値は常に$0$になるため，すべてのスピン配位で最小値を与える．\par
以上をまとめると，$H \geq 0$のときはすべての$i$について，$\sigma_i=+1$としたものが基底状態となり，$H \leq 0$のときはすべての$i$について，$\sigma_i=-1$としたものが基底状態となる(図\ref{絶対零度スピン配位})．\par
\begin{figure}[h]
  \begin{center}
    \includegraphics[height=5cm]{image/絶対零度スピン配位.png}
    \caption{絶対零度でのイジングモデルの基底状態．$H>0$のときはスピンはすべて上向きになり，$H<0$のときはスピンがすべて下向きになる$H=0$のときは，これら両方の状態が基底状態になる． \label{絶対零度スピン配位}}
  \end{center}
\end{figure}
これに基づいて，絶対零度での磁化のふるまいを見てみる．すべてのスピンが$+1$の状態での磁化は$\mu_0$，すべてのスピンが$-1$の状態での磁化は$-\mu_0$なので，
\begin{equation}
  m_L(\infty, H) =
  \begin{cases}
    -\mu_0, & H \leq 0 \text{のとき} \\
    \mu_0,  & H \geq 0 \text{のとき}
  \end{cases}
\end{equation}
とわかる．(図\ref{絶対零度磁化})
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}[scale=1]
      \draw[->,>=stealth,semithick](-3,0)--(3,0)node[below]{$H$};%x軸
      \draw[->,>=stealth,semithick](0,-2.5)--(0,2.5)node[above]{$m(\infty, H)$};%y軸
      \draw[draw=magenta,thick,domain=0:3] plot(\x,2);
      \draw[draw=magenta,thick,domain=-3:0] plot(\x,-2);
      \draw (0,0) node[below left]{0}; %原点
      \draw (0,2) node[left]{$\mu_0$};
      \draw (0,-2) node[right]{$-\mu_0$};
    \end{tikzpicture}
    \caption{絶対零度でのイジングモデルの磁化のふるまい．$H=0$を境に，$-\mu_0$から$\mu_0$に不連続に変化する．\label{絶対零度磁化}}
  \end{center}
\end{figure}
つまり，磁化は$H$の関数として不連続である．もちろん，これはエネルギーを最小化する状態が入れ替わったことの単純な反映にすぎない．これに対して，同じような不連続性が$0$でない温度で見られるかどうかは，本質的に難しい問題である．

\subsection{無限体積の極限}
有限温度での相転移や臨界現象を調べるには，自由エネルギー$f_L(\beta, H)$や磁化$m_L(\beta, H)$といった物理量を計算し，それらの振る舞いを見ればよいと思える．しかし，このとき格子サイズ$L$はどの程度の大きさにとればよいのか？．実は格子サイズが有限である場合，相転移を起こさないことが示せる．ここではこれを証明する．\par
まず，格子サイズ$L$が任意の有限な整数とする．このとき，式(\ref{自由エネルギー})の形から明らかなように自由エネルギー$f_L(\beta, H)$は$\beta, H$について何度も微分できる．よって，磁化$m_L(\sigma, H)$は$H$の連続関数であることがわかる．そして，系の対称性を考える．あるスピン配位$(\sigma_1,\dots,\sigma_N)$が与えられたとき，これらすべてのスピンを反転させた新しいスピン配位を$(\tilde{\sigma}_1,\dots,\tilde{\sigma}_N)$で定義する．つまり，すべての$i$に対して$\tilde{\sigma}_i := -\sigma_i$とするということである．このとき，エネルギーの表式(\ref{イジングエネルギー})をスピン反転した変数を使って書き直せば，$\tilde{\sigma}_i \tilde{\sigma}_j = \sigma_i \sigma_j$が成り立つことを使って
\begin{equation}
  E_{(\sigma_1,\dots,\sigma_N)} = -J \sum_{\langle i,j \rangle} \tilde{\sigma}_i \tilde{\sigma}_j -h \sum_{i=1}^{N} \tilde{\sigma}_i
\end{equation}
のように，磁場を反転させた，元のエネルギーと同じ表式が得られる．つまり，すべてのスピンを反転させることは，磁場を反転させることと等価ということである．すべての$(\sigma_1,\dots,\sigma_N)$について足し上げることは，すべての$(\tilde{\sigma}_1,\dots,\tilde{\sigma}_N)$について足し上げることと同じなので，分配関数の表式(\ref{イジング分配関数})を，
\begin{align}
  Z_L(\beta, H)
   & = \sum_{(\tilde{\sigma}_1,\dots,\tilde{\sigma}_N)} \exp{[-\beta E_{(\sigma_1,\dots,\sigma_N)}]} \notag                                                                                               \\
   & = \sum_{(\tilde{\sigma}_1,\dots,\tilde{\sigma}_N)} \exp{\left[ \beta\left( J \sum_{\langle i,j \rangle} \tilde{\sigma}_i \tilde{\sigma}_j -H \sum_{i=1}^{N} \tilde{\sigma}_i \right) \right]} \notag \\
   & = \sum_{(\sigma_1,\dots,\sigma_N)} \exp{\left[ \beta\left( J \sum_{\langle i,j \rangle} \sigma_i \sigma_j -H \sum_{i=1}^{N} \sigma_i \right) \right]} \notag                                         \\
   & = Z_L(\beta, -H)
\end{align}
となり，磁場を反転させても分配関数は変わらないという結果になる．自由エネルギーの定義である式(\ref{自由エネルギー})より
\begin{equation}
  f_L(\beta, H) = f_L(\beta, -H)
\end{equation}
という自由エネルギーの対称性が示せる．さらに磁化は式(\ref{磁化と自由エネルギーの関係式})より，$f_L(\beta, H)$の$H$微分で書けるので，
\begin{equation}
  m_L(\beta, H) = -m_L(\beta, -H) \label{磁化反対称性}
\end{equation}
のように書け，磁化は磁場の反転について反対称になることがわかる．\par
$f_L(\beta, H)$が微分可能なので，$m_L(\beta, H)$はすべての$\beta, H$において定義されている．よって式(\ref{磁化反対称性})で$H=0$とすれば，$m_L(\beta, 0) = -m_L(\beta, 0)$となり，$m_L(\beta, 0)=0$が得られる．したがって，有限系では$0\leq \beta < \infty$の任意の逆温度$\beta$で磁化がゼロになり，相転移を示さないことがわかる．

\section{一次元イジング模型}
一次元のイジング模型を考える．ハミルトニアンは
\begin{equation}
  H = -J \sum_{i=1}^N \sigma_i \sigma_j - h \sum_{i=1}^N \sigma_i \label{一次元イジングエネルギー}
\end{equation}
なり，周期的境界条件$\sigma_{N+1} = \sigma_{1}$を課す．分配関数は
\begin{equation}
  Z = \sum_{(\sigma_i,\dots,\sigma_N)} \exp{\left[ \beta J \sum_{i=1}^N \sigma_i \sigma_j + \beta h \sum_{i=1}^N \sigma_i \right]} \label{一次元イジング分配関数}
\end{equation}
となる．ここで，次のような対称行列$\mathrm{T}$を定義する．
\begin{equation}
  (\mathrm{T})_{\sigma_i, \sigma_{i+1}} = \exp{\left[ \beta J \sigma_i \sigma_{i+1} + \frac{\beta h}{2}(\sigma_i + \sigma_{i+1}) \right]},
\end{equation}
\begin{equation}
  \mathrm{T} = \begin{pmatrix}
    (\mathrm{T})_{1,1}  & (\mathrm{T})_{1,-1}  \\
    (\mathrm{T})_{-1,1} & (\mathrm{T})_{-1,-1}
  \end{pmatrix}=
  \begin{pmatrix}
    e^{\beta J + \beta h} & e^{-\beta J}          \\
    e^{-\beta J}          & e^{\beta J - \beta h}
  \end{pmatrix}
\end{equation}
このようにすることで，分配関数(\ref{一次元イジング分配関数})を
\begin{align}
  Z
   & = \sum_{(\sigma_i,\dots,\sigma_N)} (\mathrm{T})_{\sigma_1, \sigma_2} (\mathrm{T})_{\sigma_2, \sigma_3} \cdots (\mathrm{T})_{\sigma_N, \sigma_1} \notag \\
   & = \sum_{(\sigma_i,\dots,\sigma_N)} \prod_{i = 1}^N (\mathrm{T})_{\sigma_i, \sigma_{i+1}}
\end{align}
を書くことができる．行列の積の定義より，
\begin{equation}
  \sum_{\sigma_k = \pm{1}}(\mathrm{T}^n)_{\sigma_i, \sigma_k}(\mathrm{T}^m)_{\sigma_k, \sigma_j}
  = (\mathrm{T}^{n+m})_{\sigma_i, \sigma_j}
\end{equation}
であることを用いると，分配関数は
\begin{align}
  Z
   & = \sum_{\sigma_1 = \pm{1}}\sum_{\sigma_2 = \pm{1}} \cdots \sum_{\sigma_N = \pm{1}} (\mathrm{T})_{\sigma_1, \sigma_2} (\mathrm{T})_{\sigma_2, \sigma_3} \cdots (\mathrm{T})_{\sigma_N, \sigma_1} \notag   \\
   & = \sum_{\sigma_1 = \pm{1}}\sum_{\sigma_3 = \pm{1}} \cdots \sum_{\sigma_N = \pm{1}} (\mathrm{T}^2)_{\sigma_1, \sigma_3} (\mathrm{T})_{\sigma_3, \sigma_4} \cdots (\mathrm{T})_{\sigma_N, \sigma_1} \notag \\
   & = \sum_{\sigma_1 = \pm{1}}\sum_{\sigma_4 = \pm{1}} \cdots \sum_{\sigma_N = \pm{1}} (\mathrm{T}^3)_{\sigma_1, \sigma_4} (\mathrm{T})_{\sigma_4, \sigma_5} \cdots (\mathrm{T})_{\sigma_N, \sigma_1} \notag \\
   & = \cdots \notag                                                                                                                                                                                          \\
   & = \sum_{\sigma_1 = \pm{1}} (\mathrm{T}^N)_{\sigma_1, \sigma_1} \notag                                                                                                                                    \\
   & = \mathrm{Tr}\left[ \mathrm{T}^N \right] \label{1Dイジング分配関数トレース}
\end{align}
と表すことができる．対称行列$\mathrm{T}$は，スピン間の相互作用を次々と伝える役割を果たしているため，転送行列(transfer matrix)と呼ばれる．\par
分配関数(\ref{1Dイジング分配関数トレース})は，初等的な線形代数の知識で計算することができる．実対称であるため，転送行列$\mathrm{T}$は適当な直行行列$\mathrm{O}$を用いて，
\begin{equation}
  \mathrm{O}^{-1} \mathrm{T} \mathrm{O} =
  \begin{pmatrix}
    \lambda_{+} & 0           \\
    0           & \lambda_{-}
  \end{pmatrix}
\end{equation}
と対角化できる．転送行列$\mathrm{T}$の固有値は
\begin{equation}
  \lambda_{\pm{1}}
  = e^{\beta} \left\{ \cosh{\beta h} \pm{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}} \right\}
\end{equation}
より，(\ref{1Dイジング分配関数トレース})はさらに
\begin{align}
  Z
   & = \mathrm{Tr} \left[ \left\{ \mathrm{O}
  \begin{pmatrix}
      \lambda_{+} & 0           \\
      0           & \lambda_{-}
    \end{pmatrix} \mathrm{O}^{-1} \right\}^N \right] \notag   \\
   & = \mathrm{Tr} \left[
  \begin{pmatrix}
      \lambda_{+} & 0           \\
      0           & \lambda_{-}
    \end{pmatrix}^N \right] \notag                            \\
   & = (\lambda_{+})^N + (\lambda_{-})^N \label{1Dイジング分配関数}
\end{align}
と表すことができる．\par
(\ref{1Dイジング分配関数})より，スピン一つあたりの自由エネルギー$f$は
\begin{align}
  f
   & = -\frac{1}{\beta L} \ln{Z} \notag                                                                                              \\
   & = -\frac{1}{\beta L} \ln{(\lambda_{+})^N + (\lambda_{-})^N} \notag                                                              \\
   & = -\frac{1}{\beta} \ln{\lambda_{+}} - \frac{1}{\beta L} \left\{ 1 + \left( \frac{\lambda_{-}}{\lambda_{+}} \right)^{L} \right\}
\end{align}
と表すことができる．ここで，$\lambda{-}/\lambda_{+}<1$に注意して，$L \nearrow \infty$とすると，
\begin{align}
  f
   & = \lim_{L \nearrow \infty} f = -\frac{1}{\beta} \ln{\lambda_{+}} \notag                                                   \\
   & = -\frac{1}{\beta} \ln{e^{\beta} \left\{ \cosh{\beta h} \pm{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}}\right\}} \notag    \\
   & = -1 -\frac{1}{\beta} \ln{\left\{\cosh{\beta h} \pm{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}}\right\}} \label{1D自由エネルギー}
\end{align}
となり，無限体積極限での自由エネルギーを厳密に計算することができる．\par
自由エネルギーの表式(\ref{1D自由エネルギー})から，磁化を計算すると，
\begin{align}
  m(\beta, h)
   & = -\frac{\partial}{\partial h} f(\beta, h) \notag                                                                                                                                                   \\
   & = \frac{\partial}{\partial (\beta h)} \ln{\left\{ \cosh{\beta h} + \sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}} \right\}} \notag                                                                       \\
   & = \frac{1}{\cosh{\beta h} + \sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}} \frac{\partial}{\partial (\beta h)}\left\{ \cosh{\beta h} + \sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}} \right\} \notag        \\
   & =  \frac{1}{\cosh{\beta h} + \sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}} \left\{ \sinh{\beta h} + \frac{2\sinh{\beta h} \cosh{\beta h}}{2 \sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}} \right\} \notag \\
   & = \frac{\sinh{\beta h}}{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}}
\end{align}
となる．これは明らかに$\beta$と$h$について連続な関数である．特に$h=0$のとすれば$m(\beta, 0) = 0$となる．つまり，有限温度の一次元イジング模型での自発磁化は$0$であり，この系は相転移を示さないことがわかる．\par
さらに，この結果を使って磁化率$\chi(\beta)$を求めると，
\begin{align}
  \chi(\beta)
   & = \left.\frac{\partial}{\partial h} m(\beta, h)\right|_{h=0} \notag                                                                                                                                       \\
   & = \left.\frac{\partial}{\partial h} \frac{\sinh{\beta h}}{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}}\right|_{h=0} \notag                                                                                  \\
   & = \beta \left.\frac{\partial}{\partial (\beta h)} \frac{\sinh{\beta h}}{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}}\right|_{h=0} \notag                                                                    \\
   & = \beta \left.\left[ \frac{\cosh{\beta h}}{\sqrt{(\sinh{\beta h})^2 + e^{-4\beta J}}} +\frac{(\sinh{\beta h})^2 \cosh{\beta h}}{\{(\sinh{\beta h})^2 + e^{-4\beta J}\}^{3/2}} \right]\right|_{h=0} \notag \\
   & = \beta e^{2\beta J}
\end{align}
となる．$\beta J \ll 1$が成り立つ高温領域では，$\chi(\beta) \simeq \beta$となり，相互作用のない場合の振る舞い(キュリーの法則)が成り立つ．一方，低温に向かい$\beta \rightarrow \infty$となると，相互作用のない場合の磁化率との比$e^{2\beta J}$は限りなく大きくなる．これは，低温で無限個のスピンが互いにそろい合おうとすることの現れとみることができる．\par

% \section{イジング模型の平均場近似}




\begin{figure}[htbp]
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \begin{tikzpicture}[scale=1,samples=300]
      \draw[->,>=stealth,semithick](-3,0)--(3,0)node[below]{$\psi$};%x軸
      \draw[->,>=stealth,semithick](0,-2)--(0,2)node[left]{$y$};%y軸
      \draw[draw=gray,semithick,domain=-2:2] plot(\x,\x);
      \draw[draw=magenta,thick,domain=-3:3] plot(\x,{1.5*tanh(0.5*\x)});
    \end{tikzpicture}
    \subcaption{$\beta zJ \leq 1$}
  \end{minipage}
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \begin{tikzpicture}[scale=1,samples=300]
      \draw[->,>=stealth,semithick](-3,0)--(3,0)node[below]{$\psi$};%x軸
      \draw[->,>=stealth,semithick](0,-2)--(0,2)node[left]{$y$};%y軸
      \draw[draw=gray,semithick,domain=-2:2] plot(\x,\x);
      \draw[draw=magenta,thick,domain=-3:3] plot(\x,{1.5*tanh(2*\x)});
    \end{tikzpicture}
    \subcaption{$\beta zJ > 1$}
  \end{minipage}
  \caption{$h=0$での自己整合方程式$\psi = \tanh{(\beta ZJ\psi)}$}
\end{figure}

\section{二次元イジング模型}
\section{二次元イジング模型の厳密解の計算}
二次元イジング模型の厳密解はOnsagerによって求められた．Onsagerは，転送行列を対角化することで磁場のないときの自由エネルギーを求め，比熱をある温度で発散することを示した．いくつかの解法があるが，ここでは，高温展開を用いた解法を説明する．高温展開は，温度が高いとして自由エネルギーを$\beta J$のべきで展開する方法である．十分高温であれば，その展開を数項で打ち切って自由エネルギーの近似値とする．計算が比較的簡単なため，汎用的な手法として用いられているが，近似の正当性に十分に注意する必要がある．この近似のみを用いて相転移を調べることはできない．有限項の和から特異性が生じることはないからである．本節では，無限和を計算することにより厳密解を求め，特異性が生じることを示す．\par
\subsection{高温展開}
$h=0$の場合に高温展開を行う．二次元イジング模型の分配関数は
\begin{equation}
  Z = \mathrm{Tr}\prod_{\langle i, j \rangle} \exp{(\beta J \sigma_i \sigma_j)}
\end{equation}
と書ける．積はスピンの最近近接についてとる．スピン変数に関する恒等式
\begin{align}
  e^{x\sigma_i \sigma_j}
   & = \frac{e^x + e^{-x}}{2} + \sigma_i \sigma_j \frac{e^x + e^{-x}}{2} \notag \\
   & = \cosh{x} + \sigma_i \sigma_j \sinh{x}
\end{align}
を用いれば，
\begin{align}
  Z
   & = \mathrm{Tr}\prod_{\langle i, j \rangle}(\cosh{\beta J} + \sigma_i \sigma_j \sinh{\beta J}) \notag                \\
   & = (\cosh{\beta J})^{N_B}\mathrm{Tr}\prod_{\langle i, j \rangle}(1 + \sigma_i \sigma_j \tanh{\beta J}) \label{分配関数}
\end{align}
と書ける．$N_B$は最近接の数を表す．$v = \tanh{\beta J}$は有限温度で$1$より有限温度で$1$より小さい非負の量なので，(\ref{分配関数})を$v$について次のように展開する．
\begin{equation}
  \prod_{\langle i, j \rangle}(1 + v \sigma_i \sigma_j)
  = 1 + v \sum_{\langle i, j \rangle} \sigma_i \sigma_j
  + v^2 \sum_{\langle i, j \rangle, \langle k, l \rangle} \sigma_i \sigma_j \sigma_k \sigma_l + \cdots
\end{equation}
ここで，$\langle i, j \rangle \neq \langle k, l \rangle$である．

\begin{equation}
  v\sigma_i \sigma_j \longleftrightarrow
  \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \draw[very thick](0,0)--(0,-1);
    \fill(0,0)circle(0.1)node[left]{$i$};
    \fill(0,-1)circle(0.1)node[left]{$j$};
  \end{tikzpicture}
\end{equation}

\begin{equation}
  v^2\sigma_i \sigma_j \sigma_k \sigma_l \longleftrightarrow
  \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \draw[very thick](0,0)--(0,-1);
    \draw[very thick](1,0)--(1,-1);
    \fill(0,0)circle(0.1)node[left]{$i$};
    \fill(0,-1)circle(0.1)node[left]{$j$};
    \fill(1,0)circle(0.1)node[right]{$k$};
    \fill(1,-1)circle(0.1)node[right]{$l$};
  \end{tikzpicture}, \ \ \
  v^2\sigma_i \sigma_j^2 \sigma_k \longleftrightarrow
  \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \draw[very thick](0,0)--(0,-1);
    \draw[very thick](0,0)--(1,0);
    \fill(0,0)circle(0.1)node[left]{$i$};
    \fill(0,-1)circle(0.1)node[left]{$j$};
    \fill(1,0)circle(0.1)node[right]{$k$};
  \end{tikzpicture}
\end{equation}

\begin{equation}
  Z = (\cosh{\beta J})^{N_B}\mathrm{Tr}
  \left[
    1 + \sum
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
      \draw[very thick](0,0)--(0,-1);
      \fill(0,0)circle(0.1);
      \fill(0,-1)circle(0.1);
    \end{tikzpicture}
    + \sum
    \left(
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
      \draw[very thick](0,0)--(0,-1);
      \draw[very thick](1,0)--(1,-1);
      \fill(0,0)circle(0.1);
      \fill(0,-1)circle(0.1);
      \fill(1,0)circle(0.1);
      \fill(1,-1)circle(0.1);
    \end{tikzpicture} +
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
      \draw[very thick](0,0)--(0,-1);
      \draw[very thick](0,0)--(1,0);
      \fill(0,0)circle(0.1);
      \fill(0,-1)circle(0.1);
      \fill(1,0)circle(0.1);
    \end{tikzpicture}
    \right) + \cdots
    \right]
\end{equation}


\chapter{機械学習と深層学習}
ここでは，イジングモデルに対して機械学習，深層学習による手法を応用する上で必要となる機械学習と深層学習の基礎知識について説明する．
\section{機械学習とは}
機械学習(machine learning)とは，人間がこなすような学習や知的作業を計算機に実行させるためのアプローチの研究，あるいはその手法そのもののことを意味する．機械学習では，知識を人間が直接アルゴリズムに具体的に書き込んだり教え込んだりせず，データという具体例の集まりから計算機に自動的に学ばせるという方法をとる．\par
機械学習の定義としてT. M. ミッチェル(Tom Michael Mitchell)の書籍\cite{Tom1997Machine}で書かれている定義が有名である．それは
\begin{quote}
  "コンピュータプログラムが，ある種のタスクTとパフォーマンス評価尺度Pにおいて，経験Eから学習するとは，タスクTにおけるその性能をPによって評価した際に，経験Eによってそれが改善されている場合である．"
  \hfill T. M. ミッチェル
\end{quote}
である．ここで，タスクTとは解きたい問題のこと，パフォーマンス評価尺度Pは精度，誤差率などの評価指標のこと，経験Eはデータセットのことを指す．\par
機械学習では，何かしらのモデルを構築する必要がある．このモデルというのは，なにかしらの入力が与えられたときに出力を返すものであり，その正体は多数のパラメータを持った非線形関数$f_{\bm{\theta}}(\bm{x}), \ \bm{\theta} = \{ \theta_1, \theta_2, \theta_3, \cdots \}$である．
\begin{figure}[b]
  \begin{center}
    \includegraphics[height=1.5cm]{image/モデル概略図.png}
    \caption{モデルの概略図}
  \end{center}
\end{figure}
このパラメータの値は最初はランダムに初期化されており，入力を与えると出力が返ってくるが，このときの出力結果は本来出力してほしい正解の値からは外れた値が帰ってくる．そこで，出力と正解の誤差を測るような評価基準を決めておき，算出された評価結果を良くするようにモデルのパラメータを更新してあげることによってモデルを改善していく．この一連の流れを繰り返し行うことで，モデルを改善して，正解に近い出力結果を得られるモデルを作成すること機械学習とよぶ．ここで注目してほしいのは，機械学習ではモデルの改善を機械に行わせているという点である．\par
\begin{figure}[t]
  \begin{center}
    \includegraphics[height=6cm]{image/機械学習概要図.png}
    \caption{機械学習の大まかな流れ}
  \end{center}
\end{figure}
メトロポリス法とメトロポリス・ヘイスティング法は同じですか
\subsection{代表的なタスク}
ここでは，機械学習で解きたい問題であるタスクの代表的な例として「クラス分類」と「回帰」を紹介する．
\subsection*{クラス分類}
分類(classification)とは，データをいくつかのクラスに仕分ける作業のことである．例えば送られてきた電子メールをみて，それがスパムメールか通常メールを判別するような作業は，2クラス分類と呼べる．ここでのクラスというのは「スパムメールor通常メール」といった分類先のことを指す．ここで，スパムメールを$C_0$，通常メールを$C_1$とラベル付けしよう．また，入力データを$\bm{x}$とする．このとき，クラス分類というのは，与えられた$\bm{x}$が$C_0$と$C_1$のどちらに属するかを決定する作業である．数値的な変数$y = 0, 1$を導入すると，$\bm{x}$をクラス$C_{y}$へ分類するということは，$\bm{x}$の所属クラスを表す離散ベクトル$y(\bm{x})$の値を決めることであると言い換えられる．
\begin{equation}
  \bm{x} \longrightarrow y(\bm{x}) \in \{0, 1\}
\end{equation}
分類先のクラスが多数にわたる場合，多クラス分類とよび，分類先が$C_1,C_2,\dots,C_K$となり，ラベル$y(\bm{x})$も$1$から$K$の整数値をとる．これは先ほどの例でいえば，電子メールを「仕事」「家族」「友人」「スパム」と複数のファルダに分けることに対応する．
\subsection*{回帰}
データに対するクラス$y$は離散的とは限らない．例えば，過去数日の気象データ$\bm{x}$から明日の気温を予測したとき，$y$は温度の数値に相当し，連続的な実数値をもつ変数になる．このようにデータから，それに対応する実数値(を並べたベクトル)$\bm{y}$を予測作業のことを回帰(regression)という．つまり，回帰とは，与えられた$\bm{x}$を，対応する$\bm{y}$に変換するために関数$\bm{y}(\bm{x})$を決定する作業である．
\begin{equation}
  \bm{x} \longrightarrow \bm{y}(\bm{x}) \in \mathbb{R}
\end{equation}
\par
回帰による応用的なタスクとして，言語を翻訳する機械翻訳や音声を文字に起こす音声認識，状況が通常と異なることを自動的に検知させる異常検知，データのサイズを圧縮させるデータ次元削除などがあげられる．
\section{深層学習とは}
\section{統計入門}
機械学習とは，データ(経験)をもとにしてプログラムがいろいろなタスクをこなせるようにさせることであった．ではどのようにしたらプログラムはデータからタスクをこなすための知識を学びとれるか？データを科学的に分析する数理的手法といえば，統計学である．機械学習の手法もまた統計を基礎として構築される．そのような視点から特に統計的機械学習と呼ばれている．ここではまず，統計の基礎を確認し，どのようにして学習アルゴリズムや評価指標を設計できるかについて説明する．
\subsection{標本と推定}
まず，データ(集合)(data(set))やサンプル，標本(sample)という用語についてきちんと定めておく．これらはデータ点(data point)の集まりからなる．手書き文字による画像認識の例でいえば，データは統計分析に用いるために用意した画像の集合のことで，1枚1枚の画像がそれぞれデータ点とよばれる．ただしデータ点も略してデータとよばれる．さらにはサンプルをサンプルの要素であるデータ点の意味でも用いられる．これらは用語の乱用であり，全く正しくない用法であるが，しばしば用いられているのが現状であるため，本論文でも文脈に応じてこの使い方に準じることにする．\par
このようなデータ点(サンプル)の集まりからなるデータを分析するのが統計である．ここでは推定について考える．まず統計解析に用いるデータは，母集団から抽出(sampling)されたものとみなす．データの要素を1つ取り出すことは，正確には抜き取り(draw)という．そしてデータの分析から，データ自身ではなくその背後にある母集団についての知識を獲得することを目標とする．これは例えば疫学者が飲酒量と健康の関係を調べたいときに，地球上のすべての(=母集団)を調査する必要はないということと同じである．その代わりにランダムに選び取った(抽出した)少人数に対する調査結果(データ)を分析することで，人類すべてに通用する飲酒量と健康の相関関係を読み取ろうとするということである．\par
母集団の性質はデータ生成分布(generative distribution)$P_{\text{data}}(\mathrm{x})$により特徴づけられているものとする．つまり，不確実性を伴う現象を確率的にモデル化するということである．これは我々の手にするデータは，自然界におけるさまざまな物理的過程の結果，この宇宙に存在することになったわけだが，我々がその過程すべてやそこに寄与する因子のすべてを知ることはできないため，データを何らかの確率論的な過程に従ってランダムに生じているとみなしているということである．「サイコロを振ってどの目が出るか」という古典的な試行の例でも，もしサイコロやその周囲のすべての物理的情報を把握できるのであれば，原理的には出る目は力学で計算できるはずである．しかし人間には有限の認識・計算能力しかないため，どの目も1/6の確率で出るようにしか見えない．そこで今後はどの場合でも，$\bm{x}$という具体的なサンプルはデータ生成分布から抽出されたものであると仮定する．
\begin{equation}
  \bm{x} \sim P_{\text{data}}(\mathrm{x})
\end{equation}
このようにモデル化することでさまざまな現象を確率論的に予測することができる．ここで$x \sim P(\mathrm{x})$は，ある変数$\mathrm{x}$の実現値$x$が分布$P(\mathrm{x})$から生成している(生成された)ことを表す．\par
ここで抽出について1つ注意が必要である．我々は母集団について知りたいが，サンプルの抽出方法によって，結果に偏りが生じてしまっては困る．そこで基本的には無作為抽出されたデータのみを考えていくことにする．より正確には，すべてのサンプルは同一分布から独立に抽出されたものとする．\par
したがって母集団について知識を得るということは，データ生成分布を知ることを意味する．データを特徴づける十分な統計量をパラメータと呼ぶ．実際にはデータの生成の過程は極めて複雑なため，本当の$P(\mathrm{x})$は無数のパラメータをもっており，分布を完全に知ることはほぼ不可能である．そこで通常は$P(\mathrm{x})$をよく近似できると期待できるモデル分布$P(\mathrm{x};\bm{\theta})$を仮定し，そのモデルのパラメータ$\bm{\theta}$の最適値$\bm{\theta}^*$をデータから推定する．これはパラメトリックはアプローチと呼ばれる．パラメトリックなモデルを仮定したことで，分布を特徴付ける少数のパラメータの値を推定すればよいことになる．このようにデータ生成の過程について推論できれば，その結果を使うことで新規のデータに関してもいろいろと予測することができる.
\subsection{点推定}
データがある道の分布から生成していると考え，この確率分布のパラメータを決定しようとするのが総計的推定である．我々は母集団に直接アクセスすることはできないため，手持ちの有限要素のデータ集合$\mathcal{D} = \{ x_1, x_2,\dots, x_N \}$からパラメータの尤もらしい値を計算するしかない．これを点推定という．点推定のため，データを決める確率変数$\{ \mathrm{x}_1, \mathrm{x}_2, \dots, \mathrm{x}_N \}$の関数である推定量
\begin{equation}
  \hat{\bm{\theta}}( \mathrm{x}_1, \mathrm{x}_2, \dots, \mathrm{x}_N )
\end{equation}
を作る．推定量のまた確率変数なので，データが具体的に与えられて，はじめて数値としてパラメータの推定値
\begin{equation}
  \hat{\bm{\theta}}^*( x_1, x_2, \dots, x_N )
  =\hat{\bm{\theta}}( x_1, x_2, \dots, x_N )
\end{equation}
を与えることになる．いずれにせよこの推定値は，いま考えているパラメータをよく近似するように作る．それによってデータの数値から，背後にある分布について知ることができる．\par
よい推定量の作り方にはいくつもの指標がある．そこで，ここでは推定量に推奨される性質をいくつか紹介する．


\subsection{教師あり学習と教師なし学習，強化学習}
機械学習には大きく3つに分類することができる．それは教師あり学習と教師なし学習，そして強化学習である．そのうち，ここでは本研究に関係する教師あり学習と教師なし学習について説明する．
\subsection*{教師あり学習}

\subsection*{教師なし学習}




\section{ニューラルネットワーク}
\subsection{神経細胞のネットワーク}
人間の脳は，1000億個以上ものニューロンと呼ばれる細胞が寄り集まって構成されている．このニューロンの形状は1873年にC.ゴルジが銀とクロムを用いた細胞の染色法によって観察に成功したことで明らかになった．\par
ニューロンは，馴染みのある普通の細胞とは大きく異なる形状をしており，図\ref{}のようになっている．中央の膨らみは細胞体とよばれ，細胞核を担う中核となる部分であり，約10μmほどの大きさである．そして，ここから2種類の突起が伸びている．その1つは軸索とよばれる，1本の細長くまっすぐな突起である．ヒトの場合，その全長は短くて1mm，長いものだと1mを越えることもある．長い軸索からはいくつもの分岐が出ており，これらを軸索側枝とよぶ．側枝はせいぜい数十μmほどの長さしかない．側枝の終端を軸索終末といい，重要な役割を果たす．\par
もう1つの突起が樹状突起である．樹状突起はまさに木の枝のように，幹である細胞体からたくさん伸びている．樹状突起はさらにいくつもに枝分かれし，枝というよりも植物の根のようにも見える．それらの全長はせいぜい数mmであり，軸索と比べるとはるかに短い．\par
このように突起がたくさん飛び出したニューロンがたくさん集まり，ある規則的な接合を繰り返して脳はできている．図\ref{}の丸で囲まれた部分に注目しよう．このシナプスとよばれる部位は，軸索から分岐した側枝の終端(軸索終末)が他のシナプスと接合する部分である．軸索終末は主に他のニューロンの樹状突起や細胞体とシナプスを形成する．\par
では，軸索，樹状突起，そしてシナプスはどのような働きをしているかといいうと，まず軸索も樹状突起も，電気信号(パルス)を伝える電線のような役割を果たしている．神経回路上の電気信号は，図\ref{}のように極めて短い時間だけ立ち上がるパルスとして伝播する．このパルスの振幅は決まっているため，パルスの波の高さが信号の大きさを決めるわけではない．パルスは短い時間に細かく密集して伝わるが，信号強度に対応するのはそのパルスの密度である．\par
シナプス部ではまず，軸索に伝わってきた電気信号を合図に，軸索終末がシナプス小胞というカプセルに詰め込まれた化学物質を外にばらまく．その化学物質は神経伝達物質とよばれ，放出後はシナプスの樹状突起側で受け止められる．この場所には多数のレセプターがあり，そこに神経伝達物質が結合すると，その刺激から新たな電気信号が生み出される仕組みになっている．シナプスは樹状突起上にたくさんあり，各シナプスで樹状突起は他のさまざまなニューロンから電気信号の出力を受け入れる．この電気信号は細胞体へと向かって伝播する．そして数多くの樹状突起から伝わってきた電気信号は，細胞体に到達しすべて合算される．\par
細胞体は，ある一定以上の大きさ，つまり閾値を越える電気信号を受けると，軸索に向かって電気信号を出力する．電気信号は各軸索終末にあるシナプスにおいて他のニューロンの樹状突起に入力し，同じ伝播のパターンを繰り返していく．このように相互作用を複数に繰り返すことで，神経細胞のネットワークはできている．


\subsection{形式ニューロン}
深層学習の起源は，1943年のW.マカロックとW.ピッツによる論文が源流と言われている．彼らはニューロンの活動が数理論理学的な手法でモデル化できることを発見した．それにより，神経活動の数理モデルと論理回路との対応が明らかになた．彼らは，形式ニューロンや人工ニューロンとよばれる素子を定義した．形式ニューロンは実際のニューロンの活動をモデル化したものである．実際のニューロンのように，形式ニューロンも他の多数の形式ニューロン$i=1,2,\dots,$から入力信号$x_i$を受け入れる．図では入り込んでくる複数の矢印として入力を表している．ただし，ニューロンを出す信号はオンとオフの情報しかない物として，$x_i$の値は$0$か$1$しかとらないものとする．しかしシナプスごとに，ニューロン同士の結合の強さを表す重み$w_i$を導入して，層への総入力を
\begin{equation}
  u = \sum_{i} w_i x_i
\end{equation}
と定義する．$u$は細胞体に実際に入ってくる電気信号の総量に相当する．\par
この総入力を受けて，ニューロンは「軸索」方向へ出力を出すが，そこには閾値があるはずである．その状況をヘビサイドの階段行列
\begin{equation}
  \theta(x+b) = \begin{cases}
    1 & (x \geq -b) \\
    0 & (x \leq -b) \\
  \end{cases}
\end{equation}
でモデル化する．$b$は閾値を与えるパラメータである．すると，この形式ニューロンの出力は結局
\begin{equation}
  z = \theta(u + b) = \theta\left( \sum_{i} w_i x_i + b \right)
\end{equation}
となる．ここで用いた階段関数のように，総入力$u$を出力$z$へ変換する関数を一般的に活性化関数という．$z$の値もまた，$0$か$1$の2値で，それを再び他のニューロンへの入力とすることができる．\par
マカロックとピッツの形式ニューロンを多数組み合わせることで，計算機と同じようにどんな論理演算でも実現することができる．任意の論理回路がNANDゲートの組み合わせで実現できることは，計算機科学でよく知られている．NANDゲートとは，2つの2値入力$x_1, x_2$に対する出力値の関係が次のようになっている論理回路のことである．
\begin{equation*}
  \begin{tabular}{c|cccc}
    $(x_1, x_2)$ & $(0,0)$ & $(1,0)$ & $(0,1)$ & $(1,1)$ \\ \hline
    $y$          & $1$     & $1$     & $1$     & $0$     \\
  \end{tabular}    
\end{equation*}
NANDゲートの出力を実現する形式ニューロンの回路を考えてみる．回路には，まず2つの入力値$x_1, x_2$を放出するだけの形式ニューロンが2つある．これらを入力ユニットと呼んでおく．入力ニューロンからの入力を受けてNANDと同じ出力値$y(x_1, x_2)$を出すニューロンを出力ニューロンと呼ぶことにする．このような回路全体を書いたのが図\ref{}である．\par
このような簡単なニューロン回路でNANDを再現するには，例えば
\begin{equation}
  y = \theta(-x_1 - x_2 + 1.5)
\end{equation}
とすればよい．実際
\begin{align*}
  y(0,0) &= \theta(1.5) = 1  \\
  y(1,0) &= \theta(0.5) = 1  \\
  y(0,1) &= \theta(0.5) = 1  \\
  y(1,1) &= \theta(-0.5) = 0  \\
\end{align*}
となり，これは論理回路そのものであることが確かめられる．\par
このようなニューロン回路を複雑に組み合わせることで，形式ニューロンを使ってどんな論理演算も実現できる．
\begin{figure}[htbp]
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \begin{tikzpicture}[scale=1/3]
      \draw[->,>=stealth,thick](-7,0)--(-2,0);
      \draw[->,>=stealth,thick](-7,3.5)--(-1.8,0.9);
      \draw[->,>=stealth,thick](-7,-3.5)--(-1.8,-0.9);
      \draw[->,>=stealth,thick](2,0)--(7,0);
      \draw(0,2)--(0,-2);
      \draw (0,0) circle[radius=2];
      \draw (-1,0) node{$u$};
      \draw (1,0) node{$z$};
      \draw (8,0) node{$z$};
      \draw (-8,0) node{$x_2$};
      \draw (-8,4) node{$x_1$};
      \draw (-8,-4) node{$x_3$};
      \draw (-4,3) node{$w_1$};
      \draw (-4,0.7) node{$w_2$};
      \draw (-4,-3) node{$w_3$};
    \end{tikzpicture}
    \subcaption{$\beta zJ \leq 1$}
  \end{minipage}
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \begin{tikzpicture}[scale=1/3]
      \draw[->,>=stealth,thick](-7,3.5)--(-1.8,0.9);
      \draw[->,>=stealth,thick](-7,-3.5)--(-1.8,-0.9);
      \draw[->,>=stealth,thick](2,0)--(7,0);
      \draw (0,0) circle[radius=2];
      \draw (-8,4) circle[radius=1];
      \draw (-8,-4) circle[radius=1];
      \draw (8,0) node{$y$};
      \draw (-8,4) node{$x_1$};
      \draw (-8,-4) node{$x_3$};
    \end{tikzpicture}
    \subcaption{$\beta zJ > 1$}
  \end{minipage}
  \caption{形式ニューロン(左)と簡単なニューロン回路(右)}
\end{figure}


\subsection{パーセプトロン}

\subsection{順伝播型ニューラルネットワーク}

\subsection{活性化関数}

\section{畳み込みニューラルネットワーク}

\section{勾配降下法}
ニューラルネットワークの学習は損失関数の最小化によって行われる．つまり，ネットワークパラメータの空間上でスカラー関数$L(\bm{\theta})$の最小値を探すという最適化問題を解くことになる．
\begin{equation}
  \bm{\theta}^* = \underset{\bm{\theta}} {\operatorname{argmin}} \ L(\bm{\theta})
\end{equation}
損失関数は基本的に多くのパラメータを含む複雑な関数であるため，このような最適化問題を厳密に解くことはできない．そのため，計算機によって数値的に近似解を求めることになる．\par

\subsection{勾配降下法}
誤差関数$E(\bm{\theta})$の最小点を見つける手法のうち，最も直観的で単純なアイデアは図\ref{}のように，誤差関数のグラフの形状をした凹みにおいて．上のほうからボールを転がり落してボールの一番低いところにたどり着くまで待つ方法である．これがまさに勾配降下法の考え方である．\par

\begin{figure}
  \begin{center}
    \includegraphics[height=5cm]{image/gradient_decent.png}
    \caption{勾配降下法による極小値の見つけ方}
  \end{center}
\end{figure}

ボールを転がり始めさせる位置に対応して，勾配降下法ではパラメータの初期値$\theta^{(0)}$を用意する．この初期値から始めて，坂道でボールを転がすような操作を，離散的な時間$t=0,1,2,\dots$を用いて定式化しよう．坂を転がすというのは，現在位置におけるグラフの勾配
\begin{equation}
  \nabla E(\bm{\theta})
  = \frac{\partial E(\bm{\theta})}{\partial \bm{\theta}}
  \equiv \left(  \frac{\partial E(\bm{\theta})}{\partial \theta_1}, \cdots,  \frac{\partial E(\bm{\theta})}{\partial \theta_D} \right)
\end{equation}
の逆方向に動かすことを意味する．ここで表記を簡単にするため，ニューラルネットワークの重みパラメータを下付き添え字$\theta_1,\theta_2,\dots,\theta_D$でラベル付けした．$D$はニューラルネットワークの全パラメータ数である．すると時刻$t$で位置$\bm{\theta^{(t)}}$にあったボールを勾配の逆方向に動かす際のルールは次のようになる．
\begin{equation}
  \bm{\theta^{(t+1)}} = \bm{\theta^{(t)}} + \Delta \bm{\theta^{(t)}} , \ \  \Delta \bm{\theta^{(t)}} = - \eta \nabla E\left( \bm{\theta}^{(t)} \right)
\end{equation}
右辺で次時刻の$\theta^{(t+1)}$を定義する操作を$t=0,1,2,\dots$と順次繰り返していくことで，どんどんと誤差関数のグラフの底のほうに降りていくことができる．$1$ステップでの移動距離$\Delta \bm{\theta^{(t)}}$の大きさを決めるハイパーパラメータ$\eta$は学習率(learning late)と呼ばれる．この操作が収束し，もはや動けなくなった点が勾配が消える極小値である．ここでいう収束とは，計算機の数値制度の範囲内でもはや$\theta^{(t)}$の変化が見られなくなる状況のことである．\par
学習率の取り方には一般論は存在せず，現状ではトライアルアンドエラーに基づかざるを得ない．しかし学習率をあまり大きくすると，$1$ステップの刻みが荒らすぎて誤差関数の形状をうまく捉えられずに，収束に問題を起こす．その一方であまり小さくしすぎると学習が一向に進まなくなる．したがって，程よい大きさの学習率を見つけることが，学習をうまく進めるために重要となる．\par

\subsection{局所的最小値の問題}
いままでは最小値と極小値の区別に注意を払わなかった．誤差関数が下に凸な関数である簡単な状況では任意の極小値は必ず最小値に一致するので区別する必要がない．しかしながらディープラーニングにおける誤差関数は一般的にとても複雑な非凸関数なので，本当の極小値である大域的極小値以外にも，膨大な数の局所的極小値をもつ．\par
さらにニューラルネットワークには高い対称性と極小値の重複がある．例えば，第$l$層の2つのユニット$j=1,2$に注目すると，この2つのユニットを入れ替えても最終層の出力は変わらない．なぜなら，$\theta_{1i}^{(l)},\theta_{k1}^{(l+1)}$と$\theta_{2i}^{(l)},\theta_{k2}^{(l+1)}$をすべて同時に入れ替えれば何もしていないことと同じになるからである．各層でこのような入れ替えは${}_{d_l} C_2$通りだけある．したがってニューラルネットワーク全体では$\prod_l {}_{d_l} C_2$通りだけの入れ替え対称性があることがわかる．つまり，局所的極小値が1つでも見つかれば，自動的に$\prod_l {}_{d_l} C_2$個の極小値が重複して存在することになる．このように，深層モデルでは極小値の数は膨大になる．\par
このような場合，勾配降下法で大域的極小値を探すのは，干草の中から針を探すようなものである．したがってディープラーニングでは極小値にはたどり着けても，真の極小値にはまずたどり着けない．通常の機械学習の文脈ではこれは深刻な問題であり，局所的最小値の問題や局所的最適解の問題と呼ばれる．\par
ところが不思議なことに，ディープラーニングでは真の最小値を見つけずとも，誤差関数のよい極小値さえ見つけられれば十分であると予想されている．これはディープラーニングを他の機械学習とは一線を画す画期的な手法にしていると同時に，ディープラーニングにおける大きな謎の1つである．この点に関しては現在でもさまざまな研究がなされている．\par

\subsection{確率的勾配降下法}
ディープラーニングが真の最小値を必要としないとはいっても，誤差関数の値があまりに大きい臨界点にはまり込んでしまっては全く使い物にならない．そこで臨界点にトラップされることをできるだけ回避するためにランダムな要素を取り入れて，はまり込んだ場所から弾き出す効果を生み出すことで勾配降下法を改良していく．\par
ランダムな要素を入れるためには，学習の仕組みを復習する必要がある．学習データ$\mathcal{D}=\{(\bm{x}_n,\bm{y}_n)\}_{n=1,2,\dots,N}$が与えられたとき，誤差関数は各学習サンプル要素$(\bm{x}_n,\bm{y}_n)$で計算した誤差の和として表現できた．
\begin{equation}
  E(\bm{\theta}) = \frac{1}{N}\sum_{n=1}^{N}E_n(\bm{\bm{\theta}}) \label{勾配降下法}
\end{equation}
例えば平均二乗誤差を用いるならば
\begin{equation}
  E_n(\bm{\bm{\theta}}) = \frac{1}{2}\left( \bm{y}(\bm{x}_n ; \bm{\theta}) - \bm{y}_n \right)^2
\end{equation}
であり，$K$クラス分類ならば交差エントロピー
\begin{equation}
  E_n(\bm{\bm{\theta}}) = - \sum_{k=1}^{K} t_{nk}\log{y_k(\bm{x}_n ; \bm{\theta})}
\end{equation}
を用いた．先ほどの勾配降下法では式(\ref{勾配降下法})のように毎回の更新ですべての訓練サンプルを用いていた．このような方法はバッチ学習と呼ばれる．\par
しかし勾配によるパラメータ更新は何回も繰り返すことになるので，1回の更新で毎回すべてのサンプルを用いる必要がない．各時間ステップで，一部の訓練サンプルだけを用いる方法をミニバッチ学習という．ミニバッチ学習ではまず，各時間$t$で用いる訓練サンプルの部分集合$\mathcal{B}^{(t)}$を用意する．この$\mathcal{B}^{(t)}$のことをミニバッチと呼び，通常は学習前にランダムに作成しておく，そして時刻$t$における更新ではミニバッチ上で平均した誤差関数
\begin{equation}
  E^{(t)}(\bm{\theta}) = \frac{1}{\mathcal{B}^{(t)}} \sum_{n\in \mathcal{B}^{(t)}} E_n(\bm{\theta})
\end{equation}
を用いる．ここで$n\in \mathcal{B}^{(t)}$はミニバッチに含まれる訓練サンプルのラベルを表し，$|\mathcal{B}^{(t)}|$はミニバッチの中のサンプル要素の総数である．これを用いてバッチ学習同様，パラメータを更新する．
\begin{equation}
  \bm{\theta^{(t+1)}} = \bm{\theta^{(t)}} + \Delta \bm{\theta^{(t)}} , \ \  \Delta \bm{\theta^{(t)}} = - \varepsilon \nabla E^{(t)}\left( \bm{\theta}^{(t)} \right)
\end{equation}
特に各時刻のミニバッチに１つの訓練サンプルしか含まない$|\mathcal{B}^{(t)}|=1$という場合をオンライン学習や確率的勾配降下法と呼ぶ．\par
ミニバッチ学習ではランダムにミニバッチを選んだことにより，時刻ごとに誤差関数$E^{(t)}(\bm{\theta})$の形もランダムに変化する．したがって，ずっと同じ関数$E(\bm{\theta})$を使い続けるバッチ学習とは違い，望ましくない臨界点にはまり込む可能性がかなり小さくなる．これがミニバッチ学習が重宝される大きな理由である．\par
さらにサンプルを効率的に使う観点からもミニバッチは好まれる．訓練データのサイズが大きくなると，似たようなサンプルが含まれる可能性が高くなる．そこで訓練データ集合全体ではなく，ミニバッチを使用することで，1回の更新ステップにおいて似たデータを重複して使用する無駄が省けるようになる．\par
またミニバッチでの勾配降下法では，各勾配$\nabla E_n(\bm{\theta})$の計算が独立で容易に並列化できる．したがって，コアのたくさんあるGPGPUなどの並列計算環境がある場合には，ある程度のサイズのミニバッチを利用するほうが理にかなっている．

\subsection{ミニバッチの作り方}
(ミニ)バッチ学習では，学習時間をエポック(epoch)という単位ごとに分けて考える．1エポックという単位は，訓練データ全体を1周すべて使い切る時間単位を意味する．\par
まずはじめのエポックでは，データを適当なサイズのミニバッチにランダムに分割する．そして，これらのミニバッチを用いて勾配を更新していく．すべてのミニバッチを使い切ったら，このエポックは終了になる．しかし通常は，１エポックでは不十分で，次のエポックに進み，改めてランダムにミニバッチを作り，同じプロセスを繰り返していく，そして，誤差関数が十分小さくなるまでエポックを繰り返したら，学習終了になる．\par
また，バッチ学習では毎回データを使い切るため，エポックと更新時間は一致する．

\section{改良された勾配降下法}
\subsection{勾配降下法の課題}
前節で勾配法には，局所最適解の問題があることをみたが，それ以外にも多くの課題がある．\par
図\ref{勾配法の課題}(a)のように誤差関数が深い谷を作っている状況を考える．このような谷に対して勾配降下法で勾配の方向にパラメータを更新するだけでは谷に沿って激しく振動するばかりで，いつまでたってもパラメータが収束しない．\par
一方，図\ref{勾配法の課題}(b)のように，平らな領域があった場合を考える．このような場所はプラトーと呼ばれる．一度プラトーに入り込んでしまうと勾配が消えるため，パラメータの更新も止まってしまう．ミニバッチアドでランダムな要素を入れたとしても，深層学習の誤差関数に現れるプラトーは学習の進みを遅くする原因になってしまう．\par
さらに，図\ref{勾配法の課題}(c)のような急に切り立った絶壁の存在も危険である．それまで緩やかな坂を下ってきたとしても，絶壁に当たった瞬間に極めて大きな勾配によって吹き飛ばされてしまう．
\begin{figure}[htbp]
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[keepaspectratio, scale=0.5]{image/勾配法課題(a)}
    \subcaption{}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[keepaspectratio, scale=0.5]{image/勾配法課題(b)}
    \subcaption{}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[keepaspectratio, scale=0.5]{image/勾配法課題(c)}
    \subcaption{}
  \end{minipage}
  \caption{(a)谷で振動.(b)プラトーで停止．(c)絶壁で反射}
  \label{勾配法の課題}
\end{figure}


\subsection{モーメンタム法}
振動による問題を改善する手法としてモーメンタム(momentum)というものがある．モーメンタムは「慣性」とも訳され．前時刻での勾配の影響を引きずらせることで振動を防ぐことができる．\par
振動の原因は，深い谷の底周りで急激な勾配が正負交互に発生するためであった．そこで，1個前のステップでの勾配の影響を，現在の勾配に加えることを考える．前回のパラメータ更新量$\Delta \bm{\theta}^{(t-1)}$が負の値で，現在の勾配$\nabla E(\bm{\theta}^{(t)})$が正の大きな値であるとする．すると前回の更新量を現在の勾配に少し加えることで，図\ref{モーメンタム法による勾配抑制図}のように今回の更新量$\Delta \bm{\theta}^{(t)}$を比較的小さな値に抑えることができる．つまり，手法で勾配が大きく正負に振れること防ぐことができる．
\begin{align}
  \bm{\theta}^{(t+1)}
   & = \bm{\theta}^{(t)} + \Delta \bm{\theta}^{(t)}                              \\
  \Delta \bm{\theta}^{(t)}
   & = \mu \Delta \bm{\theta}^{(t-1)} - (1-\mu) \eta \nabla E(\bm{\theta}^{(t)})
\end{align}
\begin{figure}
  \begin{center}
    \includegraphics[height=8cm]{image/モーメンタム法.png}
    \caption{モーメンタム法による谷での振動の抑制}
    \label{モーメンタム法による勾配抑制図}
  \end{center}
\end{figure}
初期値は$\Delta \bm{\theta}^{(0)} = 0$にとる．また，$\mu$は比較的$1$に近い$0.5 \sim 0.99$程度の値をとる．\par
モーメンタム法は振動を防ぐ効果だけではなく，普通の斜面において勾配法でのパラメータ更新の加速も可能にする．これを見るために誤差関数の傾き$\nabla E(\bm{\theta}^{(t)})$が一定の領域を考える．この場合，式(\ref{})の終端速度は$\Delta \bm{\theta}^{(t)} = \Delta \bm{\theta}^{(t+1)} = \Delta \bm{\theta}$として更新式を解くことで，
\begin{equation}
  \Delta \bm{\theta} = -\eta \nabla E(\bm{\theta})
\end{equation}
となる．つまり傾きの一定の斜面では，もともとの式(\ref{})においては$(1-\mu)\eta$であった学習率が$\eta$にまで増える加速効果を与える．また，$\mu=0$にとるとモーメンタムの効果は消え，通常の勾配法に帰着する．

\subsection{AdaGrad}
モーメンタム法では勾配法の振動を防いだり，パラメータ更新を加速させたりする効果があった．ここでは，異なる側面に注目した改善策を説明する．\par
勾配降下法にはこれまで1つの学習率$\eta$しかなかった．しかし実際には，パラメータ空間の座標方向によって誤差関数の勾配に大きな違いが現れることが想定できる．例えば，$w_1$方向には急激な勾配を持つ一方，$w_2$方向には緩やかな傾きしか持たない誤差関数があったとする．すると$w_1$方向には大きな勾配に従い大きくパラメータ値が更新される一方，$w_2$方向には一向にパラメータ更新が進まない．これは勾配法をコントロールする学習率が1つしかないからである．\par
もし各パラメータ方向に応じて複数の学習率が導入できれば，どの方向にも均等な速度で学習を進めることができ，勾配降下法の収束性が良くなるはずである．ただし，むやみにやたらに学習率を増やしてしまっては，それらを我々がトライアンドエラーによって決めなくてはいけなくなり都合がよくない．そこでここでは，パラメータをあまり増やさずに，各方向に適切な有効学習率を持たせる手法を説明する．\par
このような手法のうち，早くから用いられていたものがAdaGrad(adaptive subgradient descent)である．
\begin{equation}
  \Delta \bm{\theta}_i^{(t)}
  = -\frac{\eta}{\sqrt{\sum_{s=1}^{t}\left(\nabla E(\bm{\theta}^{(s)})_i\right)^2}} \nabla E(\bm{\theta}^{(t)})
\end{equation}
左辺は$\Delta \bm{\theta}^{(t)}$の第$i$成分であり，$\nabla E(\bm{\theta}^{(t)})_i$は勾配の第$i$成分である．AdaGradでは，過去の勾配成分の二乗和の平方根で学習率を割っている．それにより，すでに大きな勾配値をとってきた方向に対しては勾配を小さく減少させ，いままで勾配の小さかった方向への学習率を増大させる効果がある．これにより，学習が一向に進まない方向が生じてしまうことを防ぐことができる\par
AdaGradの欠点をしては，学習の初期に勾配が大きいとすぐさま更新量$\Delta \bm{\theta}_i^{(t)}$が小さくなってしまうことである．そのままだと学習がストップするので，適切な程度にまで$\eta$を大きく選んであげる必要がある．そのためにAdaGradは学習率の選び方に敏感で使いにくい方法である．また，はじめから勾配が大きすぎるとすぐさま更新が進まなくなるため，重みの初期値にも敏感である．

\subsection{RMSprop}
RMSpropはヒントンにより講義の中で紹介された手法である．論文として出版していないにも関わらず，世界中で広く用いられている手法として有名である．\par
AdaGradの問題は，ひとたび更新が$0$になってしまったら二度と有限の値に戻らないことであった．これは過去のすべての勾配の情報を収集してしまっていたためである．そこで十分過去の勾配の情報を指数的な減衰因子によって消滅させるように，二乗和ではなく指数的な移動平均$v_{i,t}$から決まるroot mean square(RMS)を用いることにする．
\begin{align}
   & v_{i,t}
  = \rho v_{i,t-1} + (1-\rho) (\nabla E(\bm{\theta^{(t)}})_i)^2 \\
   & \Delta \bm{\theta}_i^{(t)}
  = - \frac{\eta}{\sqrt{v_{i,t}+\epsilon}} \nabla E(\bm{\theta^{(t)}})_i
\end{align}
初期値は$v_{i,0}=0$とする．また，$\epsilon$は分母が$0$とならないように導入した．よく$\epsilon = 10^{-6}$の値が用いられる．また簡単のために
\begin{equation}
  \mathrm{RMS}[\nabla E_i]_t = \sqrt{v_{i,t} + \epsilon}
\end{equation}
という記法を今後用いる．\par
RMSpropでは最近の勾配の履歴のみが影響するため，更新量が完全に消えてしまうことはない．また，深層学習では，うまく鞍点を抜け出した阿多は更新を加速したいため，モーメンタム法などと組み合わせて用いられる．RMSpropの有効性は広く実証されている．

\subsection{AdaDelta}
RMSpropはAdaGradを改善したとても良い方法である．しかしながら，全体の学習率$\eta$の値に敏感であるという事実にはあまり改善が見られない．その理由の1つは，次元性のミスマッチである．物理における次元解析を思い出そう．いま誤差関数$E$は無次元であるとする．つまりパラメータ$\bm{\theta}$を測るスケールを何倍しても不変な関数とする．その一方$\Delta \bm{\theta}$は長さの次元を持っており，$E$の微分は長さの逆数の次元を持つ．
\begin{equation}
  [\Delta \bm{\theta}] = \mathrm{L}, \ \ \ [\nabla E(\bm{\theta})] = \mathrm{L}^{-1}
\end{equation}
\subsection{Adam}
しかし，勾配法では，この次元性を合わない両者を学習率で比例させてしまっている．このミスマッチが学習率に押し付けられてしまうために，適切な学習率のスケールが問題に応じてさまざまに変化してしまうのである．\par
ロバストな学習率を実現させるためには，このミスマッチを解消しなくてはいけない．そこで

\section{誤差逆伝播法}


% \section{ボルツマンマシンの基礎}

\chapter{機械学習によるイジング模型の相転移検出}
\section{配位データの生成方法}
\section{相の分類器による相転移検出}
\section{温度測定器による相転移検出}
\section{相転移検出がなぜ可能なのか？}

% \chapter{機械学習と繰り込み群}
% \section{繰り込み群と特徴抽出}


\chapter*{まとめ}
研究のまとめを書く．

%=====================================================================================
\chapter*{謝辞} %章を付けずにタイトル表示
\addcontentsline{toc}{chapter}{謝辞} %章立てせずに目次に追加するおまじない
謝辞を書く．

%=====================================================================================

% \addcontentsline{toc}{chapter}{参考文献} %章立てせずに目次に追加するおまじない
\renewcommand{\bibname}{参考文献} %これがないと，タイトルが「関連図書」になってしまう
\bibliography{reference} %bibtexファイルの読み込み
\bibliographystyle{junsrt} %本文に\cite{}を入れることで，参考文献表示

% 付録
\appendix
\renewcommand{\thechapter}{\Alph{chapter}}
\renewcommand{\thesection}{\Alph{chapter}.\arabic{section}}
\setcounter{section}{0}
\renewcommand{\theequation}{\Alph{chapter}.\arabic{equation}}
\setcounter{equation}{0}

\chapter{aaa}
aaa
\section{bbb}
bbb


\end{document}