{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cudaが使えるか確認\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "data = pd.read_csv('data_v3.csv')\n",
    "X = data.iloc[:, :-1].values  # 入力データ (スピン配位)\n",
    "y = data.iloc[:, -1].values   # 教師データ (温度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 100), (75,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データのサイズを確認\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, ..., 6.  , 6.  , 6.  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy配列からPyTorchのテンソルに変換\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # (25000,) -> (25000, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)    # (6250,) -> (6250, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000],\n",
       "        [0.5000],\n",
       "        [2.2500],\n",
       "        ...,\n",
       "        [0.0100],\n",
       "        [3.7500],\n",
       "        [5.7500]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解データはone-hot表現にする必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# y_trainテンソルを新しいテンソルに変換する関数を定義\n",
    "def to_one_hot(y_train, num_classes=25):\n",
    "    # one-hotベクトルの初期化\n",
    "    one_hot = torch.zeros(len(y_train), num_classes)\n",
    "    \n",
    "    # 各要素を25次元のone-hotベクトルに変換\n",
    "    for i, val in enumerate(y_train):\n",
    "        index = int((val - 0.01) / 0.24)  # 正しいインデックスの計算\n",
    "        one_hot[i, index] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# y_train,y_testをone-hotベクトルに変換\n",
    "one_hot_y_train = to_one_hot(y_train, num_classes=25)\n",
    "one_hot_y_test = to_one_hot(y_test, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y_train[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = TensorDataset(X_train, one_hot_y_train)\n",
    "test_dataset = TensorDataset(X_test, one_hot_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "          1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "          1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像を回転させるような前処理を実行したいができていない，\n",
    "\n",
    "原因：各データが1次元に形を変形させてしまっているため，画像回転の前処理がエラーになってしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理を定義\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.RandomHorizontalFlip(p=0.5),\n",
    "#    transforms.RandomVerticalFlip(p=0.5),\n",
    "#])\n",
    "#\n",
    "#train_dataset = transform(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの設定（バッチサイズ12500）\n",
    "train_loader = DataLoader(train_dataset, batch_size=125, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([125, 100]), torch.Size([125, 25]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, t = next(iter(train_loader))\n",
    "x.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([125, 25])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "torch.Size([125, 25])\n"
     ]
    }
   ],
   "source": [
    "for x, t in train_loader:\n",
    "    print(t)\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークモデルの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの初期化\n",
    "input_size = 100\n",
    "hidden_size = 64\n",
    "output_size = 25\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルの中身を確認\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数と最適化アルゴリズムの設定\n",
    "criterion = nn.CrossEntropyLoss()   # クロスエントロピー誤差を採用\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)     # Adamという最適化手法を採用,L2正則化を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 3.2042586731910707, acc: 0.06287999451160431, test loss: 3.202255129814148, test acc: 0.062400009483098984\n",
      "epoch: 1, loss: 3.196511435508728, acc: 0.07232000678777695, test loss: 3.199682652950287, test acc: 0.07008000463247299\n",
      "epoch: 2, loss: 3.190005135536194, acc: 0.0841599851846695, test loss: 3.2001706075668337, test acc: 0.07048001885414124\n",
      "epoch: 3, loss: 3.187231111526489, acc: 0.08767999708652496, test loss: 3.2031736612319945, test acc: 0.06640001386404037\n",
      "epoch: 4, loss: 3.1840082597732544, acc: 0.09400000423192978, test loss: 3.2018570375442503, test acc: 0.0724799856543541\n",
      "epoch: 5, loss: 3.1795245957374574, acc: 0.10064002871513367, test loss: 3.2008014631271364, test acc: 0.07231999933719635\n",
      "epoch: 6, loss: 3.182195188999176, acc: 0.09584000706672668, test loss: 3.2009595370292665, test acc: 0.0703199952840805\n",
      "epoch: 7, loss: 3.1752475452423097, acc: 0.1045600175857544, test loss: 3.198463051319122, test acc: 0.072720006108284\n",
      "epoch: 8, loss: 3.1733437466621397, acc: 0.10832001268863678, test loss: 3.199672291278839, test acc: 0.07223999500274658\n",
      "epoch: 9, loss: 3.1711834716796874, acc: 0.11223999410867691, test loss: 3.2018116760253905, test acc: 0.07311999797821045\n",
      "epoch: 10, loss: 3.1671335434913637, acc: 0.11648000776767731, test loss: 3.197347059249878, test acc: 0.07567999511957169\n",
      "epoch: 11, loss: 3.1663578081130983, acc: 0.11736001074314117, test loss: 3.203766505718231, test acc: 0.06928000599145889\n",
      "epoch: 12, loss: 3.1632216691970827, acc: 0.12000001966953278, test loss: 3.1998833227157593, test acc: 0.07328000664710999\n",
      "epoch: 13, loss: 3.15862508058548, acc: 0.1258399784564972, test loss: 3.1997954297065734, test acc: 0.07552001625299454\n",
      "epoch: 14, loss: 3.161228723526001, acc: 0.12296000868082047, test loss: 3.2001462507247926, test acc: 0.07584001123905182\n",
      "epoch: 15, loss: 3.1595601177215578, acc: 0.12584002315998077, test loss: 3.1987108516693117, test acc: 0.07816000282764435\n",
      "epoch: 16, loss: 3.1570581007003784, acc: 0.12912000715732574, test loss: 3.1994993615150453, test acc: 0.07480001449584961\n",
      "epoch: 17, loss: 3.1563552451133727, acc: 0.1310400515794754, test loss: 3.1981712889671328, test acc: 0.075360007584095\n",
      "epoch: 18, loss: 3.15508731842041, acc: 0.13352002203464508, test loss: 3.1988541507720947, test acc: 0.07575998455286026\n",
      "epoch: 19, loss: 3.152519142627716, acc: 0.1356000006198883, test loss: 3.1990940260887144, test acc: 0.07344000786542892\n",
      "epoch: 20, loss: 3.1495707416534424, acc: 0.13976004719734192, test loss: 3.2020353293418884, test acc: 0.07255998253822327\n",
      "epoch: 21, loss: 3.150754814147949, acc: 0.13784001767635345, test loss: 3.1989438796043395, test acc: 0.07592001557350159\n",
      "epoch: 22, loss: 3.147527647018433, acc: 0.1446400135755539, test loss: 3.198284342288971, test acc: 0.07552000135183334\n",
      "epoch: 23, loss: 3.1483600401878356, acc: 0.1398400217294693, test loss: 3.198237507343292, test acc: 0.07751999795436859\n",
      "epoch: 24, loss: 3.1464420795440673, acc: 0.14144004881381989, test loss: 3.19681028842926, test acc: 0.07896000891923904\n",
      "epoch: 25, loss: 3.146319260597229, acc: 0.14320001006126404, test loss: 3.1975094532966613, test acc: 0.0751200020313263\n",
      "epoch: 26, loss: 3.1443285655975344, acc: 0.14584003388881683, test loss: 3.199187443256378, test acc: 0.0769600048661232\n",
      "epoch: 27, loss: 3.1440406799316407, acc: 0.1501600295305252, test loss: 3.196998426914215, test acc: 0.07808001339435577\n",
      "epoch: 28, loss: 3.143218231201172, acc: 0.147039994597435, test loss: 3.1958564829826357, test acc: 0.07832001149654388\n",
      "epoch: 29, loss: 3.1419685530662536, acc: 0.14816004037857056, test loss: 3.1967219924926757, test acc: 0.07719999551773071\n",
      "epoch: 30, loss: 3.139409682750702, acc: 0.1520000547170639, test loss: 3.1996977806091307, test acc: 0.07776002585887909\n",
      "epoch: 31, loss: 3.1390562772750856, acc: 0.1539200246334076, test loss: 3.1960964250564574, test acc: 0.08071999996900558\n",
      "epoch: 32, loss: 3.1386527156829835, acc: 0.15344002842903137, test loss: 3.1976907515525816, test acc: 0.07544001191854477\n",
      "epoch: 33, loss: 3.136042776107788, acc: 0.15584000945091248, test loss: 3.1983900022506715, test acc: 0.07624001801013947\n",
      "epoch: 34, loss: 3.1365201616287233, acc: 0.15720002353191376, test loss: 3.1957406210899353, test acc: 0.0812000036239624\n",
      "epoch: 35, loss: 3.134210467338562, acc: 0.1592000126838684, test loss: 3.1972083830833435, test acc: 0.07704000174999237\n",
      "epoch: 36, loss: 3.1333236622810365, acc: 0.15984001755714417, test loss: 3.1970632386207583, test acc: 0.07736001163721085\n",
      "epoch: 37, loss: 3.134469573497772, acc: 0.15967999398708344, test loss: 3.199454095363617, test acc: 0.0751200020313263\n",
      "epoch: 38, loss: 3.134499328136444, acc: 0.15904004871845245, test loss: 3.198672125339508, test acc: 0.07808000594377518\n",
      "epoch: 39, loss: 3.133541965484619, acc: 0.16183999180793762, test loss: 3.197420372962952, test acc: 0.07848002761602402\n",
      "epoch: 40, loss: 3.1328488969802857, acc: 0.16239997744560242, test loss: 3.1989437794685363, test acc: 0.07608001679182053\n",
      "epoch: 41, loss: 3.1344951748847962, acc: 0.1594400256872177, test loss: 3.200442214012146, test acc: 0.07552000135183334\n",
      "epoch: 42, loss: 3.134223563671112, acc: 0.15824000537395477, test loss: 3.1966825795173643, test acc: 0.0793600007891655\n",
      "epoch: 43, loss: 3.1337640357017515, acc: 0.15984001755714417, test loss: 3.1964535975456236, test acc: 0.07799999415874481\n",
      "epoch: 44, loss: 3.1314308500289916, acc: 0.16224002838134766, test loss: 3.1976984882354738, test acc: 0.07872001081705093\n",
      "epoch: 45, loss: 3.130316045284271, acc: 0.16312000155448914, test loss: 3.199304287433624, test acc: 0.07712001353502274\n",
      "epoch: 46, loss: 3.1341771197319033, acc: 0.1588800698518753, test loss: 3.198735728263855, test acc: 0.07424001395702362\n",
      "epoch: 47, loss: 3.131384401321411, acc: 0.16512005031108856, test loss: 3.1992340064048768, test acc: 0.07888000458478928\n",
      "epoch: 48, loss: 3.131428360939026, acc: 0.16359999775886536, test loss: 3.19689733505249, test acc: 0.08032002300024033\n",
      "epoch: 49, loss: 3.12891729593277, acc: 0.16640004515647888, test loss: 3.1982589840888975, test acc: 0.07896000891923904\n",
      "epoch: 50, loss: 3.1296264052391054, acc: 0.16672000288963318, test loss: 3.1955916047096253, test acc: 0.07920001447200775\n",
      "epoch: 51, loss: 3.1289359498023988, acc: 0.1666399985551834, test loss: 3.1974986672401426, test acc: 0.08031999319791794\n",
      "epoch: 52, loss: 3.1293418622016906, acc: 0.1668800264596939, test loss: 3.1979541325569154, test acc: 0.0761600062251091\n",
      "epoch: 53, loss: 3.1277086091041566, acc: 0.16800004243850708, test loss: 3.19737211227417, test acc: 0.07512001693248749\n",
      "epoch: 54, loss: 3.127802700996399, acc: 0.16791999340057373, test loss: 3.19699250459671, test acc: 0.07984001189470291\n",
      "epoch: 55, loss: 3.130952296257019, acc: 0.16312001645565033, test loss: 3.1961093831062315, test acc: 0.07903998345136642\n",
      "epoch: 56, loss: 3.1284626150131225, acc: 0.16512000560760498, test loss: 3.196322100162506, test acc: 0.07952001690864563\n",
      "epoch: 57, loss: 3.1275979113578796, acc: 0.1690400242805481, test loss: 3.1965025663375854, test acc: 0.07896001636981964\n",
      "epoch: 58, loss: 3.1271265149116516, acc: 0.16624002158641815, test loss: 3.197245869636536, test acc: 0.07736001163721085\n",
      "epoch: 59, loss: 3.1296801018714904, acc: 0.16335999965667725, test loss: 3.1969662952423095, test acc: 0.07560000568628311\n",
      "epoch: 60, loss: 3.127207622528076, acc: 0.16832000017166138, test loss: 3.195458536148071, test acc: 0.07952000200748444\n",
      "epoch: 61, loss: 3.1262448143959047, acc: 0.16968005895614624, test loss: 3.196122944355011, test acc: 0.08056002110242844\n",
      "epoch: 62, loss: 3.126743881702423, acc: 0.1695999801158905, test loss: 3.1966459608078, test acc: 0.07936002314090729\n",
      "epoch: 63, loss: 3.127568790912628, acc: 0.17016002535820007, test loss: 3.19739497423172, test acc: 0.07863999903202057\n",
      "epoch: 64, loss: 3.126086676120758, acc: 0.16975998878479004, test loss: 3.198325831890106, test acc: 0.0764000192284584\n",
      "epoch: 65, loss: 3.1292122316360476, acc: 0.16359998285770416, test loss: 3.198655228614807, test acc: 0.07752001285552979\n",
      "epoch: 66, loss: 3.1250261092185974, acc: 0.17160002887248993, test loss: 3.19678448677063, test acc: 0.07928000390529633\n",
      "epoch: 67, loss: 3.127263910770416, acc: 0.16847999393939972, test loss: 3.1944439435005187, test acc: 0.0804000273346901\n",
      "epoch: 68, loss: 3.1267275619506836, acc: 0.168240025639534, test loss: 3.1971281480789187, test acc: 0.07936003059148788\n",
      "epoch: 69, loss: 3.1249699068069456, acc: 0.17127999663352966, test loss: 3.195625264644623, test acc: 0.07968001067638397\n",
      "epoch: 70, loss: 3.12712406873703, acc: 0.16896003484725952, test loss: 3.1943850040435793, test acc: 0.08208000659942627\n",
      "epoch: 71, loss: 3.1261540627479554, acc: 0.16712000966072083, test loss: 3.195856301784515, test acc: 0.0812000185251236\n",
      "epoch: 72, loss: 3.1278780484199524, acc: 0.1680000275373459, test loss: 3.1966176629066467, test acc: 0.08008000254631042\n",
      "epoch: 73, loss: 3.1254058861732483, acc: 0.17103998363018036, test loss: 3.197600555419922, test acc: 0.07816001027822495\n",
      "epoch: 74, loss: 3.125307033061981, acc: 0.17096003890037537, test loss: 3.1956306076049805, test acc: 0.07888003438711166\n",
      "epoch: 75, loss: 3.124654552936554, acc: 0.17056001722812653, test loss: 3.195933759212494, test acc: 0.07944000512361526\n",
      "epoch: 76, loss: 3.1260600423812868, acc: 0.16728000342845917, test loss: 3.195813627243042, test acc: 0.07952000200748444\n",
      "epoch: 77, loss: 3.1245445370674134, acc: 0.1735999882221222, test loss: 3.196630387306213, test acc: 0.07968001067638397\n",
      "epoch: 78, loss: 3.1250216245651243, acc: 0.17080001533031464, test loss: 3.198922345638275, test acc: 0.07744000107049942\n",
      "epoch: 79, loss: 3.128973903656006, acc: 0.16496001183986664, test loss: 3.2019892287254335, test acc: 0.06736000627279282\n",
      "epoch: 80, loss: 3.1251808071136473, acc: 0.1692800223827362, test loss: 3.1988981699943544, test acc: 0.07720000296831131\n",
      "epoch: 81, loss: 3.126251678466797, acc: 0.17127999663352966, test loss: 3.1969456839561463, test acc: 0.07728000730276108\n",
      "epoch: 82, loss: 3.1254721903800964, acc: 0.17000003159046173, test loss: 3.196463634967804, test acc: 0.07736001908779144\n",
      "epoch: 83, loss: 3.122808690071106, acc: 0.17392002046108246, test loss: 3.1978280568122863, test acc: 0.0788000077009201\n",
      "epoch: 84, loss: 3.1229879117012023, acc: 0.17504003643989563, test loss: 3.199777660369873, test acc: 0.07487999647855759\n",
      "epoch: 85, loss: 3.1245025849342345, acc: 0.1720000058412552, test loss: 3.196705713272095, test acc: 0.08048000186681747\n",
      "epoch: 86, loss: 3.1251619815826417, acc: 0.1707199215888977, test loss: 3.1964171957969665, test acc: 0.07920000702142715\n",
      "epoch: 87, loss: 3.124016330242157, acc: 0.17360003292560577, test loss: 3.195724883079529, test acc: 0.0788000077009201\n",
      "epoch: 88, loss: 3.120966031551361, acc: 0.1759200096130371, test loss: 3.1958077144622803, test acc: 0.07815999537706375\n",
      "epoch: 89, loss: 3.123354618549347, acc: 0.1743200272321701, test loss: 3.1954380249977112, test acc: 0.08128000795841217\n",
      "epoch: 90, loss: 3.122018611431122, acc: 0.1748800277709961, test loss: 3.197092905044556, test acc: 0.07688000798225403\n",
      "epoch: 91, loss: 3.1232662487030027, acc: 0.17423994839191437, test loss: 3.196323630809784, test acc: 0.07912000268697739\n",
      "epoch: 92, loss: 3.1233276391029356, acc: 0.17336001992225647, test loss: 3.195974915027618, test acc: 0.078560009598732\n",
      "epoch: 93, loss: 3.122590801715851, acc: 0.17471997439861298, test loss: 3.196904683113098, test acc: 0.07671999931335449\n",
      "epoch: 94, loss: 3.124231059551239, acc: 0.17320001125335693, test loss: 3.1969250941276552, test acc: 0.07583999633789062\n",
      "epoch: 95, loss: 3.1221484637260435, acc: 0.17375999689102173, test loss: 3.1954726505279543, test acc: 0.07847999036312103\n",
      "epoch: 96, loss: 3.122997043132782, acc: 0.17376001179218292, test loss: 3.194902727603912, test acc: 0.07967999577522278\n",
      "epoch: 97, loss: 3.122633247375488, acc: 0.1727999895811081, test loss: 3.1968010449409485, test acc: 0.07560000568628311\n",
      "epoch: 98, loss: 3.1225169682502747, acc: 0.17423994839191437, test loss: 3.1958556151390076, test acc: 0.07952000200748444\n",
      "epoch: 99, loss: 3.1222511315345765, acc: 0.17535996437072754, test loss: 3.196845190525055, test acc: 0.07712001353502274\n",
      "epoch: 100, loss: 3.1230545711517332, acc: 0.1727200299501419, test loss: 3.196182198524475, test acc: 0.07823999971151352\n",
      "epoch: 101, loss: 3.1218408155441284, acc: 0.17456001043319702, test loss: 3.1970910167694093, test acc: 0.07848000526428223\n",
      "epoch: 102, loss: 3.123220458030701, acc: 0.17231997847557068, test loss: 3.1976478338241576, test acc: 0.07991998642683029\n",
      "epoch: 103, loss: 3.1240858268737792, acc: 0.1738400012254715, test loss: 3.198949248790741, test acc: 0.07815999537706375\n",
      "epoch: 104, loss: 3.1218289375305175, acc: 0.17607997357845306, test loss: 3.1972291469573975, test acc: 0.07560001313686371\n",
      "epoch: 105, loss: 3.1229919242858886, acc: 0.17208005487918854, test loss: 3.1956197667121886, test acc: 0.07656001299619675\n",
      "epoch: 106, loss: 3.122688126564026, acc: 0.173520028591156, test loss: 3.1964501309394837, test acc: 0.07728000730276108\n",
      "epoch: 107, loss: 3.1207927227020265, acc: 0.1746399849653244, test loss: 3.1961934661865232, test acc: 0.07847999036312103\n",
      "epoch: 108, loss: 3.121003510951996, acc: 0.17535993456840515, test loss: 3.1959829926490784, test acc: 0.07840000092983246\n",
      "epoch: 109, loss: 3.121274101734161, acc: 0.17528001964092255, test loss: 3.1953168869018556, test acc: 0.08128000050783157\n",
      "epoch: 110, loss: 3.1205755615234376, acc: 0.17520004510879517, test loss: 3.197191002368927, test acc: 0.07847999781370163\n",
      "epoch: 111, loss: 3.122254796028137, acc: 0.17472000420093536, test loss: 3.1987958407402037, test acc: 0.0769600048661232\n",
      "epoch: 112, loss: 3.121968722343445, acc: 0.17296001315116882, test loss: 3.1981398487091064, test acc: 0.07872001081705093\n",
      "epoch: 113, loss: 3.1225007486343386, acc: 0.1730399876832962, test loss: 3.195905020236969, test acc: 0.07912001758813858\n",
      "epoch: 114, loss: 3.122029576301575, acc: 0.1754399985074997, test loss: 3.197712962627411, test acc: 0.07903999835252762\n",
      "epoch: 115, loss: 3.1227204275131224, acc: 0.17159996926784515, test loss: 3.196023693084717, test acc: 0.07871998846530914\n",
      "epoch: 116, loss: 3.1213863015174867, acc: 0.17544002830982208, test loss: 3.197601087093353, test acc: 0.0769600048661232\n",
      "epoch: 117, loss: 3.1216788053512574, acc: 0.17479999363422394, test loss: 3.195926060676575, test acc: 0.07887998223304749\n",
      "epoch: 118, loss: 3.1237191009521483, acc: 0.17320001125335693, test loss: 3.195710070133209, test acc: 0.07992000132799149\n",
      "epoch: 119, loss: 3.123047547340393, acc: 0.17351998388767242, test loss: 3.196955392360687, test acc: 0.07943999022245407\n",
      "epoch: 120, loss: 3.1235024523735047, acc: 0.1737600415945053, test loss: 3.1958582401275635, test acc: 0.07928001135587692\n",
      "epoch: 121, loss: 3.1208579325675965, acc: 0.17520004510879517, test loss: 3.1977443742752074, test acc: 0.07647999376058578\n",
      "epoch: 122, loss: 3.1225783920288084, acc: 0.1743200272321701, test loss: 3.199017720222473, test acc: 0.07479999959468842\n",
      "epoch: 123, loss: 3.120671286582947, acc: 0.17624002695083618, test loss: 3.196312246322632, test acc: 0.08064001798629761\n",
      "epoch: 124, loss: 3.121736695766449, acc: 0.17648003995418549, test loss: 3.195930707454681, test acc: 0.08111997693777084\n",
      "epoch: 125, loss: 3.1229332542419432, acc: 0.17160002887248993, test loss: 3.195707621574402, test acc: 0.07759999483823776\n",
      "epoch: 126, loss: 3.1209865760803224, acc: 0.17544002830982208, test loss: 3.196717019081116, test acc: 0.0785600021481514\n",
      "epoch: 127, loss: 3.121065089702606, acc: 0.175120010972023, test loss: 3.1969050359725952, test acc: 0.07904001325368881\n",
      "epoch: 128, loss: 3.1213661766052248, acc: 0.17583999037742615, test loss: 3.197476313114166, test acc: 0.07912001013755798\n",
      "epoch: 129, loss: 3.1215397214889524, acc: 0.17520001530647278, test loss: 3.196898913383484, test acc: 0.07967999577522278\n",
      "epoch: 130, loss: 3.121672024726868, acc: 0.17511998116970062, test loss: 3.1989206314086913, test acc: 0.0769600123167038\n",
      "epoch: 131, loss: 3.122397403717041, acc: 0.17351998388767242, test loss: 3.1946110582351683, test acc: 0.08064001053571701\n",
      "epoch: 132, loss: 3.12156986951828, acc: 0.1745600402355194, test loss: 3.1981070399284364, test acc: 0.07752001285552979\n",
      "epoch: 133, loss: 3.120419361591339, acc: 0.1772800236940384, test loss: 3.197343683242798, test acc: 0.08128000050783157\n",
      "epoch: 134, loss: 3.1206730031967163, acc: 0.17815999686717987, test loss: 3.197572660446167, test acc: 0.07832002639770508\n",
      "epoch: 135, loss: 3.119404809474945, acc: 0.17776000499725342, test loss: 3.1970281481742857, test acc: 0.07600000500679016\n",
      "epoch: 136, loss: 3.122562441825867, acc: 0.17216002941131592, test loss: 3.198329281806946, test acc: 0.07520001381635666\n",
      "epoch: 137, loss: 3.1220088243484496, acc: 0.1775999367237091, test loss: 3.1966681933403014, test acc: 0.07863999158143997\n",
      "epoch: 138, loss: 3.1216463446617126, acc: 0.1756800264120102, test loss: 3.197096703052521, test acc: 0.07728001475334167\n",
      "epoch: 139, loss: 3.118637726306915, acc: 0.18007999658584595, test loss: 3.1984815049171447, test acc: 0.07688000798225403\n",
      "epoch: 140, loss: 3.1201055073738098, acc: 0.17712002992630005, test loss: 3.1962873458862306, test acc: 0.07864000648260117\n",
      "epoch: 141, loss: 3.1220297503471373, acc: 0.17600001394748688, test loss: 3.19702716588974, test acc: 0.0777600109577179\n",
      "epoch: 142, loss: 3.1205761528015135, acc: 0.17800000309944153, test loss: 3.198854002952576, test acc: 0.07495999336242676\n",
      "epoch: 143, loss: 3.119745512008667, acc: 0.17680001258850098, test loss: 3.198006489276886, test acc: 0.07671999931335449\n",
      "epoch: 144, loss: 3.1204335141181945, acc: 0.1797599494457245, test loss: 3.1992025136947633, test acc: 0.07400001585483551\n",
      "epoch: 145, loss: 3.121781668663025, acc: 0.17296002805233002, test loss: 3.1955121874809267, test acc: 0.07823999226093292\n",
      "epoch: 146, loss: 3.1214488124847413, acc: 0.17720003426074982, test loss: 3.1959604978561402, test acc: 0.08055999875068665\n",
      "epoch: 147, loss: 3.1208670806884764, acc: 0.17688007652759552, test loss: 3.1977274417877197, test acc: 0.0785599872469902\n",
      "epoch: 148, loss: 3.121224055290222, acc: 0.17640002071857452, test loss: 3.1974708580970765, test acc: 0.07888001203536987\n",
      "epoch: 149, loss: 3.1211016511917116, acc: 0.1756799966096878, test loss: 3.197755753993988, test acc: 0.07647998631000519\n",
      "epoch: 150, loss: 3.1214092350006104, acc: 0.175120010972023, test loss: 3.200095338821411, test acc: 0.07520001381635666\n",
      "epoch: 151, loss: 3.1233268713951112, acc: 0.17328006029129028, test loss: 3.1972538352012636, test acc: 0.07688000798225403\n",
      "epoch: 152, loss: 3.1209432315826415, acc: 0.1756800264120102, test loss: 3.1951133012771606, test acc: 0.08239999413490295\n",
      "epoch: 153, loss: 3.12084890127182, acc: 0.17640002071857452, test loss: 3.1957498955726624, test acc: 0.0788000151515007\n",
      "epoch: 154, loss: 3.1196182131767274, acc: 0.1780800223350525, test loss: 3.1972361636161803, test acc: 0.07840000838041306\n",
      "epoch: 155, loss: 3.1187839198112486, acc: 0.1796800196170807, test loss: 3.197918756008148, test acc: 0.07863999903202057\n",
      "epoch: 156, loss: 3.120576248168945, acc: 0.1759999543428421, test loss: 3.195746035575867, test acc: 0.07880000025033951\n",
      "epoch: 157, loss: 3.120777225494385, acc: 0.17839999496936798, test loss: 3.198021159172058, test acc: 0.07760000228881836\n",
      "epoch: 158, loss: 3.1204581379890444, acc: 0.1786399483680725, test loss: 3.195218846797943, test acc: 0.07895999401807785\n",
      "epoch: 159, loss: 3.1205442428588865, acc: 0.17735998332500458, test loss: 3.1962920570373536, test acc: 0.07768002152442932\n",
      "epoch: 160, loss: 3.1209081912040713, acc: 0.17687995731830597, test loss: 3.194334909915924, test acc: 0.08031999319791794\n",
      "epoch: 161, loss: 3.120507254600525, acc: 0.17680004239082336, test loss: 3.197929337024689, test acc: 0.07687999308109283\n",
      "epoch: 162, loss: 3.1224957132339477, acc: 0.17511998116970062, test loss: 3.1974267840385435, test acc: 0.07888000458478928\n",
      "epoch: 163, loss: 3.1207676792144774, acc: 0.1780800074338913, test loss: 3.19604989528656, test acc: 0.08055999875068665\n",
      "epoch: 164, loss: 3.120736961364746, acc: 0.1759200096130371, test loss: 3.1962153124809265, test acc: 0.0793600082397461\n",
      "epoch: 165, loss: 3.120312168598175, acc: 0.1793600171804428, test loss: 3.2003058886528013, test acc: 0.07544001191854477\n",
      "epoch: 166, loss: 3.1203761863708497, acc: 0.17920005321502686, test loss: 3.196126220226288, test acc: 0.07968000322580338\n",
      "epoch: 167, loss: 3.119638936519623, acc: 0.17904001474380493, test loss: 3.196458921432495, test acc: 0.07911999523639679\n",
      "epoch: 168, loss: 3.119416584968567, acc: 0.17872001230716705, test loss: 3.196855568885803, test acc: 0.07848002016544342\n",
      "epoch: 169, loss: 3.118193485736847, acc: 0.1796800196170807, test loss: 3.196533043384552, test acc: 0.07744000107049942\n",
      "epoch: 170, loss: 3.1182206511497497, acc: 0.17984001338481903, test loss: 3.1995001006126405, test acc: 0.07472001761198044\n",
      "epoch: 171, loss: 3.12005007982254, acc: 0.17856000363826752, test loss: 3.196905479431152, test acc: 0.07888000458478928\n",
      "epoch: 172, loss: 3.120297849178314, acc: 0.1786399930715561, test loss: 3.196699583530426, test acc: 0.0785599946975708\n",
      "epoch: 173, loss: 3.119967608451843, acc: 0.17576001584529877, test loss: 3.1953185296058653, test acc: 0.08256000280380249\n",
      "epoch: 174, loss: 3.120473518371582, acc: 0.1786399930715561, test loss: 3.197657265663147, test acc: 0.07751999795436859\n",
      "epoch: 175, loss: 3.1180731892585754, acc: 0.1785600483417511, test loss: 3.196668472290039, test acc: 0.07864000648260117\n",
      "epoch: 176, loss: 3.1177434134483337, acc: 0.18167999386787415, test loss: 3.197600553035736, test acc: 0.0796000063419342\n",
      "epoch: 177, loss: 3.119116404056549, acc: 0.17784003913402557, test loss: 3.2002814650535583, test acc: 0.07496000081300735\n",
      "epoch: 178, loss: 3.121596462726593, acc: 0.17767997086048126, test loss: 3.196697995662689, test acc: 0.07711999118328094\n",
      "epoch: 179, loss: 3.121128978729248, acc: 0.1762399971485138, test loss: 3.1968133568763735, test acc: 0.07848000526428223\n",
      "epoch: 180, loss: 3.118124976158142, acc: 0.17968003451824188, test loss: 3.1991124296188356, test acc: 0.07632001489400864\n",
      "epoch: 181, loss: 3.1195554900169373, acc: 0.1775200068950653, test loss: 3.197611527442932, test acc: 0.07680000364780426\n",
      "epoch: 182, loss: 3.1214518785476684, acc: 0.17559999227523804, test loss: 3.1976454734802244, test acc: 0.07664000988006592\n",
      "epoch: 183, loss: 3.1186074590682984, acc: 0.17896001040935516, test loss: 3.1969218254089355, test acc: 0.08128000795841217\n",
      "epoch: 184, loss: 3.117431845664978, acc: 0.18079999089241028, test loss: 3.2002416682243346, test acc: 0.07391999661922455\n",
      "epoch: 185, loss: 3.118776001930237, acc: 0.17679999768733978, test loss: 3.1968707132339476, test acc: 0.07832001894712448\n",
      "epoch: 186, loss: 3.11942129611969, acc: 0.1783200055360794, test loss: 3.1964582300186155, test acc: 0.0764000192284584\n",
      "epoch: 187, loss: 3.118207960128784, acc: 0.1807199865579605, test loss: 3.1960168409347536, test acc: 0.08032001554965973\n",
      "epoch: 188, loss: 3.1194386100769043, acc: 0.17999999225139618, test loss: 3.196515896320343, test acc: 0.07815999537706375\n",
      "epoch: 189, loss: 3.119891104698181, acc: 0.1761600524187088, test loss: 3.1949744629859924, test acc: 0.08263999968767166\n",
      "epoch: 190, loss: 3.1192743945121766, acc: 0.17975999414920807, test loss: 3.1981005477905273, test acc: 0.07727999240159988\n",
      "epoch: 191, loss: 3.1208872270584105, acc: 0.17656001448631287, test loss: 3.1997787857055666, test acc: 0.07416000962257385\n",
      "epoch: 192, loss: 3.11991201877594, acc: 0.1796799749135971, test loss: 3.1973951864242554, test acc: 0.07680001854896545\n",
      "epoch: 193, loss: 3.119223473072052, acc: 0.17911994457244873, test loss: 3.197046980857849, test acc: 0.07840003073215485\n",
      "epoch: 194, loss: 3.1198834228515624, acc: 0.17736005783081055, test loss: 3.195018036365509, test acc: 0.07928000390529633\n",
      "epoch: 195, loss: 3.117819845676422, acc: 0.18095999956130981, test loss: 3.19782080411911, test acc: 0.07568000257015228\n",
      "epoch: 196, loss: 3.1185906982421874, acc: 0.1812800168991089, test loss: 3.1964036798477173, test acc: 0.07863999903202057\n",
      "epoch: 197, loss: 3.1199488687515258, acc: 0.17896005511283875, test loss: 3.197892758846283, test acc: 0.07680001109838486\n",
      "epoch: 198, loss: 3.1168338298797607, acc: 0.1812000423669815, test loss: 3.196503345966339, test acc: 0.078560009598732\n",
      "epoch: 199, loss: 3.1189348220825197, acc: 0.17800003290176392, test loss: 3.1960399174690246, test acc: 0.07735999673604965\n",
      "epoch: 200, loss: 3.1177049922943114, acc: 0.18000005185604095, test loss: 3.196000680923462, test acc: 0.07904002070426941\n",
      "epoch: 201, loss: 3.1161623549461366, acc: 0.18239998817443848, test loss: 3.196882276535034, test acc: 0.07927998900413513\n",
      "epoch: 202, loss: 3.115683522224426, acc: 0.18264001607894897, test loss: 3.1974913930892943, test acc: 0.0785599946975708\n",
      "epoch: 203, loss: 3.1169551682472227, acc: 0.18079997599124908, test loss: 3.195203011035919, test acc: 0.07943999767303467\n",
      "epoch: 204, loss: 3.116889536380768, acc: 0.1815200001001358, test loss: 3.1959692287445067, test acc: 0.0796000063419342\n",
      "epoch: 205, loss: 3.1209844374656677, acc: 0.17680004239082336, test loss: 3.1991383838653564, test acc: 0.07504001259803772\n",
      "epoch: 206, loss: 3.120854814052582, acc: 0.17639994621276855, test loss: 3.1972967004776, test acc: 0.0788000151515007\n",
      "epoch: 207, loss: 3.119249222278595, acc: 0.1801600158214569, test loss: 3.199787018299103, test acc: 0.07416001707315445\n",
      "epoch: 208, loss: 3.1207809758186342, acc: 0.17648003995418549, test loss: 3.196453006267548, test acc: 0.07951999455690384\n",
      "epoch: 209, loss: 3.1180881333351134, acc: 0.18160001933574677, test loss: 3.1960533452033997, test acc: 0.07872000336647034\n",
      "epoch: 210, loss: 3.117158100605011, acc: 0.1804799735546112, test loss: 3.196544597148895, test acc: 0.0783199891448021\n",
      "epoch: 211, loss: 3.1176908206939697, acc: 0.17975999414920807, test loss: 3.1976210856437683, test acc: 0.07639999687671661\n",
      "epoch: 212, loss: 3.1172100615501406, acc: 0.18152004480361938, test loss: 3.197392210960388, test acc: 0.07528001815080643\n",
      "epoch: 213, loss: 3.116482379436493, acc: 0.18160001933574677, test loss: 3.1978589582443235, test acc: 0.07744001597166061\n",
      "epoch: 214, loss: 3.1182176947593687, acc: 0.18080002069473267, test loss: 3.196528956890106, test acc: 0.07735998928546906\n",
      "epoch: 215, loss: 3.117857084274292, acc: 0.17928002774715424, test loss: 3.1956008887290954, test acc: 0.08056000620126724\n",
      "epoch: 216, loss: 3.1177898836135864, acc: 0.181520015001297, test loss: 3.196729426383972, test acc: 0.07648000866174698\n",
      "epoch: 217, loss: 3.116968283653259, acc: 0.18024003505706787, test loss: 3.1971398305892946, test acc: 0.0783199816942215\n",
      "epoch: 218, loss: 3.1178562593460084, acc: 0.182559996843338, test loss: 3.196074678897858, test acc: 0.07872000336647034\n",
      "epoch: 219, loss: 3.1172265934944154, acc: 0.18135999143123627, test loss: 3.1976487183570863, test acc: 0.0788000151515007\n",
      "epoch: 220, loss: 3.1180326414108275, acc: 0.18071994185447693, test loss: 3.1957220387458802, test acc: 0.07975999265909195\n",
      "epoch: 221, loss: 3.116762254238129, acc: 0.1815199851989746, test loss: 3.1972794365882873, test acc: 0.07552000880241394\n",
      "epoch: 222, loss: 3.118066942691803, acc: 0.17815998196601868, test loss: 3.1973466777801516, test acc: 0.07712000608444214\n",
      "epoch: 223, loss: 3.118888912200928, acc: 0.1796799749135971, test loss: 3.197478678226471, test acc: 0.0777600109577179\n",
      "epoch: 224, loss: 3.116070921421051, acc: 0.18079999089241028, test loss: 3.1975420904159546, test acc: 0.07808000594377518\n",
      "epoch: 225, loss: 3.1189490222930907, acc: 0.17799998819828033, test loss: 3.1974152398109434, test acc: 0.07680001854896545\n",
      "epoch: 226, loss: 3.117866351604462, acc: 0.18000003695487976, test loss: 3.197934830188751, test acc: 0.07600000500679016\n",
      "epoch: 227, loss: 3.1170202922821044, acc: 0.18080002069473267, test loss: 3.198216314315796, test acc: 0.07991999387741089\n",
      "epoch: 228, loss: 3.1178421211242675, acc: 0.181520015001297, test loss: 3.1951647782325745, test acc: 0.07832001894712448\n",
      "epoch: 229, loss: 3.1170443201065066, acc: 0.18144004046916962, test loss: 3.1994580268859862, test acc: 0.07592001557350159\n",
      "epoch: 230, loss: 3.117886242866516, acc: 0.18056003749370575, test loss: 3.1973470664024353, test acc: 0.07864001393318176\n",
      "epoch: 231, loss: 3.117211322784424, acc: 0.1817599982023239, test loss: 3.1948038601875304, test acc: 0.08136000484228134\n",
      "epoch: 232, loss: 3.1188273286819457, acc: 0.17855997383594513, test loss: 3.196364245414734, test acc: 0.08056000620126724\n",
      "epoch: 233, loss: 3.1191149044036863, acc: 0.17960000038146973, test loss: 3.197144730091095, test acc: 0.07712000608444214\n",
      "epoch: 234, loss: 3.1163749432563783, acc: 0.18272000551223755, test loss: 3.1970204591751097, test acc: 0.08104001730680466\n",
      "epoch: 235, loss: 3.117157881259918, acc: 0.18343999981880188, test loss: 3.1961636686325074, test acc: 0.07808001339435577\n",
      "epoch: 236, loss: 3.1182563710212707, acc: 0.17960000038146973, test loss: 3.196154637336731, test acc: 0.07968001812696457\n",
      "epoch: 237, loss: 3.117090265750885, acc: 0.18032002449035645, test loss: 3.195454285144806, test acc: 0.07992001622915268\n",
      "epoch: 238, loss: 3.115729260444641, acc: 0.18399997055530548, test loss: 3.1970552611351013, test acc: 0.0788000077009201\n",
      "epoch: 239, loss: 3.116675786972046, acc: 0.1828000247478485, test loss: 3.196583423614502, test acc: 0.07752000540494919\n",
      "epoch: 240, loss: 3.114489893913269, acc: 0.186319962143898, test loss: 3.1957782220840456, test acc: 0.08056001365184784\n",
      "epoch: 241, loss: 3.116372628211975, acc: 0.1828799992799759, test loss: 3.1963065218925477, test acc: 0.07952001690864563\n",
      "epoch: 242, loss: 3.1167712116241457, acc: 0.1815200001001358, test loss: 3.1960994100570677, test acc: 0.07703998684883118\n",
      "epoch: 243, loss: 3.1170212602615357, acc: 0.1826399564743042, test loss: 3.197559199333191, test acc: 0.07704002410173416\n",
      "epoch: 244, loss: 3.1172804498672484, acc: 0.1802399605512619, test loss: 3.1993054723739625, test acc: 0.07527998089790344\n",
      "epoch: 245, loss: 3.1175512409210206, acc: 0.18191999197006226, test loss: 3.196640396118164, test acc: 0.07968001067638397\n",
      "epoch: 246, loss: 3.116796779632568, acc: 0.18032002449035645, test loss: 3.1970354747772216, test acc: 0.07968001812696457\n",
      "epoch: 247, loss: 3.1170066571235657, acc: 0.18112000823020935, test loss: 3.197431125640869, test acc: 0.07720000296831131\n",
      "epoch: 248, loss: 3.117068531513214, acc: 0.18384003639221191, test loss: 3.196413242816925, test acc: 0.07768001407384872\n",
      "epoch: 249, loss: 3.1144451260566712, acc: 0.18480001389980316, test loss: 3.1953356409072877, test acc: 0.07911999523639679\n",
      "epoch: 250, loss: 3.117350995540619, acc: 0.1815199851989746, test loss: 3.196756196022034, test acc: 0.08064001053571701\n",
      "epoch: 251, loss: 3.1166194248199464, acc: 0.18192005157470703, test loss: 3.1977886748313904, test acc: 0.07823999971151352\n",
      "epoch: 252, loss: 3.116613738536835, acc: 0.1831200122833252, test loss: 3.199255485534668, test acc: 0.07600000500679016\n",
      "epoch: 253, loss: 3.118225290775299, acc: 0.17687998712062836, test loss: 3.197020101547241, test acc: 0.0777600035071373\n",
      "epoch: 254, loss: 3.1175087571144102, acc: 0.18032000958919525, test loss: 3.1965104722976685, test acc: 0.08063998818397522\n",
      "epoch: 255, loss: 3.116520035266876, acc: 0.18215997517108917, test loss: 3.195336465835571, test acc: 0.07832001894712448\n",
      "epoch: 256, loss: 3.118104679584503, acc: 0.17984004318714142, test loss: 3.197149097919464, test acc: 0.07728000730276108\n",
      "epoch: 257, loss: 3.1161957740783692, acc: 0.18112000823020935, test loss: 3.1956330275535585, test acc: 0.0820000022649765\n",
      "epoch: 258, loss: 3.1154400753974913, acc: 0.18360000848770142, test loss: 3.1972154808044433, test acc: 0.07823999971151352\n",
      "epoch: 259, loss: 3.1165330100059507, acc: 0.18168003857135773, test loss: 3.1956311154365538, test acc: 0.08248000591993332\n",
      "epoch: 260, loss: 3.117476122379303, acc: 0.18184001743793488, test loss: 3.1962671375274656, test acc: 0.07768000662326813\n",
      "epoch: 261, loss: 3.1148328137397767, acc: 0.18672002851963043, test loss: 3.195364694595337, test acc: 0.07807999104261398\n",
      "epoch: 262, loss: 3.1155819368362425, acc: 0.18248000741004944, test loss: 3.1974191451072693, test acc: 0.07872000336647034\n",
      "epoch: 263, loss: 3.1158398294448855, acc: 0.18327996134757996, test loss: 3.1945841813087466, test acc: 0.08240000903606415\n",
      "epoch: 264, loss: 3.11610342502594, acc: 0.18135997653007507, test loss: 3.1964655351638793, test acc: 0.07968000322580338\n",
      "epoch: 265, loss: 3.1156807231903074, acc: 0.1841600239276886, test loss: 3.1981680631637572, test acc: 0.07768001407384872\n",
      "epoch: 266, loss: 3.1157532048225405, acc: 0.184640035033226, test loss: 3.1975219011306764, test acc: 0.07904001325368881\n",
      "epoch: 267, loss: 3.115515990257263, acc: 0.1828799843788147, test loss: 3.1985632395744323, test acc: 0.07752000540494919\n",
      "epoch: 268, loss: 3.1169756650924683, acc: 0.18104000389575958, test loss: 3.1970600152015685, test acc: 0.08047999441623688\n",
      "epoch: 269, loss: 3.1164631724357603, acc: 0.18407998979091644, test loss: 3.1960121726989748, test acc: 0.08208000659942627\n",
      "epoch: 270, loss: 3.115310115814209, acc: 0.18439997732639313, test loss: 3.196354932785034, test acc: 0.07816001027822495\n",
      "epoch: 271, loss: 3.1166314029693605, acc: 0.1823199838399887, test loss: 3.196825137138367, test acc: 0.08056002110242844\n",
      "epoch: 272, loss: 3.1164906859397887, acc: 0.18351998925209045, test loss: 3.1965367889404295, test acc: 0.07928001880645752\n",
      "epoch: 273, loss: 3.1157056760787962, acc: 0.181599959731102, test loss: 3.1964741921424866, test acc: 0.07840000092983246\n",
      "epoch: 274, loss: 3.1148231410980225, acc: 0.18592004477977753, test loss: 3.194810230731964, test acc: 0.07887998223304749\n",
      "epoch: 275, loss: 3.11453572511673, acc: 0.18584005534648895, test loss: 3.1962525749206545, test acc: 0.07919999957084656\n",
      "epoch: 276, loss: 3.114432902336121, acc: 0.18440000712871552, test loss: 3.1973995757102966, test acc: 0.07759998738765717\n",
      "epoch: 277, loss: 3.1159557843208314, acc: 0.18351998925209045, test loss: 3.195927453041077, test acc: 0.08008001744747162\n",
      "epoch: 278, loss: 3.1162173891067506, acc: 0.18320001661777496, test loss: 3.1989378261566164, test acc: 0.07712001353502274\n",
      "epoch: 279, loss: 3.1146393322944643, acc: 0.1847200095653534, test loss: 3.1966034364700318, test acc: 0.07840001583099365\n",
      "epoch: 280, loss: 3.1131231117248537, acc: 0.18504008650779724, test loss: 3.1977485156059267, test acc: 0.07807999849319458\n",
      "epoch: 281, loss: 3.1149701857566834, acc: 0.18343999981880188, test loss: 3.199444832801819, test acc: 0.0761600062251091\n",
      "epoch: 282, loss: 3.1145262598991392, acc: 0.18480001389980316, test loss: 3.193878688812256, test acc: 0.08216000348329544\n",
      "epoch: 283, loss: 3.1151688528060912, acc: 0.18568000197410583, test loss: 3.197392747402191, test acc: 0.07864000648260117\n",
      "epoch: 284, loss: 3.115060088634491, acc: 0.18344001471996307, test loss: 3.195964813232422, test acc: 0.07903999835252762\n",
      "epoch: 285, loss: 3.115134036540985, acc: 0.1841599941253662, test loss: 3.197675657272339, test acc: 0.07896000146865845\n",
      "epoch: 286, loss: 3.114275827407837, acc: 0.18512003123760223, test loss: 3.197724230289459, test acc: 0.0777599960565567\n",
      "epoch: 287, loss: 3.116274960041046, acc: 0.18264001607894897, test loss: 3.1967937111854554, test acc: 0.07984000444412231\n",
      "epoch: 288, loss: 3.1162560749053956, acc: 0.18448005616664886, test loss: 3.1995343160629273, test acc: 0.07624000310897827\n",
      "epoch: 289, loss: 3.1165918421745302, acc: 0.18184000253677368, test loss: 3.1961501598358155, test acc: 0.07968001067638397\n",
      "epoch: 290, loss: 3.1144839549064636, acc: 0.18584001064300537, test loss: 3.1981717228889464, test acc: 0.07920002937316895\n",
      "epoch: 291, loss: 3.114980173110962, acc: 0.1857600212097168, test loss: 3.194229428768158, test acc: 0.08023998886346817\n",
      "epoch: 292, loss: 3.1141999530792237, acc: 0.1865600198507309, test loss: 3.1945810317993164, test acc: 0.08151999115943909\n",
      "epoch: 293, loss: 3.116129071712494, acc: 0.18215996026992798, test loss: 3.195710983276367, test acc: 0.07952000200748444\n",
      "epoch: 294, loss: 3.1161255049705505, acc: 0.18512006103992462, test loss: 3.1947621393203733, test acc: 0.08295998722314835\n",
      "epoch: 295, loss: 3.1160420107841493, acc: 0.1842399686574936, test loss: 3.198092839717865, test acc: 0.0769600048661232\n",
      "epoch: 296, loss: 3.1179548382759092, acc: 0.18024003505706787, test loss: 3.195698981285095, test acc: 0.07896000891923904\n",
      "epoch: 297, loss: 3.1159781932830812, acc: 0.18400001525878906, test loss: 3.1929512548446657, test acc: 0.08320000767707825\n",
      "epoch: 298, loss: 3.11556756734848, acc: 0.18448001146316528, test loss: 3.1957495617866516, test acc: 0.08119998872280121\n",
      "epoch: 299, loss: 3.1166673564910887, acc: 0.18119999766349792, test loss: 3.196059596538544, test acc: 0.08104000240564346\n",
      "epoch: 300, loss: 3.1160931992530823, acc: 0.18544000387191772, test loss: 3.196125979423523, test acc: 0.0804000198841095\n",
      "epoch: 301, loss: 3.115792119503021, acc: 0.18136003613471985, test loss: 3.1962670135498046, test acc: 0.07840001583099365\n",
      "epoch: 302, loss: 3.1141150069236754, acc: 0.18648000061511993, test loss: 3.195578899383545, test acc: 0.0769600123167038\n",
      "epoch: 303, loss: 3.1154990768432618, acc: 0.18479998409748077, test loss: 3.1969997835159303, test acc: 0.07799999415874481\n",
      "epoch: 304, loss: 3.1173206686973574, acc: 0.18264000117778778, test loss: 3.19741770029068, test acc: 0.07847999781370163\n",
      "epoch: 305, loss: 3.1150718998908995, acc: 0.18472005426883698, test loss: 3.1968474197387695, test acc: 0.07808000594377518\n",
      "epoch: 306, loss: 3.116100583076477, acc: 0.1865600198507309, test loss: 3.195980496406555, test acc: 0.07911999523639679\n",
      "epoch: 307, loss: 3.1148879313468933, acc: 0.1844799816608429, test loss: 3.1949985671043395, test acc: 0.08232001960277557\n",
      "epoch: 308, loss: 3.115780007839203, acc: 0.1828000247478485, test loss: 3.1950893139839174, test acc: 0.08239999413490295\n",
      "epoch: 309, loss: 3.115290973186493, acc: 0.18479998409748077, test loss: 3.1959986686706543, test acc: 0.07976001501083374\n",
      "epoch: 310, loss: 3.114995496273041, acc: 0.18664002418518066, test loss: 3.1958257365226745, test acc: 0.07896001636981964\n",
      "epoch: 311, loss: 3.114803664684296, acc: 0.18503999710083008, test loss: 3.1971108388900755, test acc: 0.07903999835252762\n",
      "epoch: 312, loss: 3.1137701082229614, acc: 0.18535996973514557, test loss: 3.1956464076042175, test acc: 0.07872001081705093\n",
      "epoch: 313, loss: 3.114440584182739, acc: 0.18648004531860352, test loss: 3.195490925312042, test acc: 0.08295998722314835\n",
      "epoch: 314, loss: 3.1154277658462526, acc: 0.18320003151893616, test loss: 3.1971806335449218, test acc: 0.08008000999689102\n",
      "epoch: 315, loss: 3.116152591705322, acc: 0.18440000712871552, test loss: 3.195829064846039, test acc: 0.07927999645471573\n",
      "epoch: 316, loss: 3.116864192485809, acc: 0.18215999007225037, test loss: 3.196491765975952, test acc: 0.07912001013755798\n",
      "epoch: 317, loss: 3.113746244907379, acc: 0.18536002933979034, test loss: 3.1934052085876465, test acc: 0.08432000130414963\n",
      "epoch: 318, loss: 3.1141520524024964, acc: 0.18624000251293182, test loss: 3.197372736930847, test acc: 0.07959999144077301\n",
      "epoch: 319, loss: 3.11481698513031, acc: 0.18392004072666168, test loss: 3.1961182165145874, test acc: 0.08008000254631042\n",
      "epoch: 320, loss: 3.11441287279129, acc: 0.18480001389980316, test loss: 3.196827392578125, test acc: 0.07679999619722366\n",
      "epoch: 321, loss: 3.1147811126708986, acc: 0.18727999925613403, test loss: 3.1971799111366273, test acc: 0.07911998778581619\n",
      "epoch: 322, loss: 3.114153344631195, acc: 0.18616001307964325, test loss: 3.197526738643646, test acc: 0.07727999985218048\n",
      "epoch: 323, loss: 3.1157705044746398, acc: 0.18304000794887543, test loss: 3.1972017455101014, test acc: 0.07792000472545624\n",
      "epoch: 324, loss: 3.1162676405906677, acc: 0.18192005157470703, test loss: 3.197894208431244, test acc: 0.0745600014925003\n",
      "epoch: 325, loss: 3.115701837539673, acc: 0.18424002826213837, test loss: 3.1969144201278685, test acc: 0.07847999036312103\n",
      "epoch: 326, loss: 3.114562437534332, acc: 0.1849600225687027, test loss: 3.1974737215042115, test acc: 0.07760000228881836\n",
      "epoch: 327, loss: 3.1168353867530825, acc: 0.18136008083820343, test loss: 3.1942151975631714, test acc: 0.08263998478651047\n",
      "epoch: 328, loss: 3.113735399246216, acc: 0.1849600225687027, test loss: 3.19648469209671, test acc: 0.07968000322580338\n",
      "epoch: 329, loss: 3.115456643104553, acc: 0.1839999556541443, test loss: 3.1947654390335085, test acc: 0.08224000036716461\n",
      "epoch: 330, loss: 3.1155500864982604, acc: 0.18488000333309174, test loss: 3.1965706825256346, test acc: 0.07983999699354172\n",
      "epoch: 331, loss: 3.1140325617790223, acc: 0.18671995401382446, test loss: 3.200113761425018, test acc: 0.07512001693248749\n",
      "epoch: 332, loss: 3.115814666748047, acc: 0.18375997245311737, test loss: 3.197003438472748, test acc: 0.07944000512361526\n",
      "epoch: 333, loss: 3.115365822315216, acc: 0.18464002013206482, test loss: 3.1994571447372437, test acc: 0.07464002072811127\n",
      "epoch: 334, loss: 3.115658767223358, acc: 0.18296000361442566, test loss: 3.1968596816062926, test acc: 0.07792001217603683\n",
      "epoch: 335, loss: 3.1146159410476684, acc: 0.18615998327732086, test loss: 3.194315819740295, test acc: 0.08192002028226852\n",
      "epoch: 336, loss: 3.115683991909027, acc: 0.1849600374698639, test loss: 3.1986560893058775, test acc: 0.07664002478122711\n",
      "epoch: 337, loss: 3.1156803655624388, acc: 0.18560001254081726, test loss: 3.195584764480591, test acc: 0.08216000348329544\n",
      "epoch: 338, loss: 3.1141209053993224, acc: 0.18543998897075653, test loss: 3.194736478328705, test acc: 0.08192000538110733\n",
      "epoch: 339, loss: 3.1143233442306517, acc: 0.18575997650623322, test loss: 3.1979948115348815, test acc: 0.07752001285552979\n",
      "epoch: 340, loss: 3.1140399384498596, acc: 0.18648004531860352, test loss: 3.19822851896286, test acc: 0.0761600211262703\n",
      "epoch: 341, loss: 3.114656615257263, acc: 0.18464000523090363, test loss: 3.1960402274131776, test acc: 0.07968000322580338\n",
      "epoch: 342, loss: 3.1132908296585082, acc: 0.1855199933052063, test loss: 3.197000432014465, test acc: 0.07944000512361526\n",
      "epoch: 343, loss: 3.1143555545806887, acc: 0.18560007214546204, test loss: 3.1949880123138428, test acc: 0.08160000294446945\n",
      "epoch: 344, loss: 3.113964891433716, acc: 0.18552003800868988, test loss: 3.1959989285469055, test acc: 0.07792000472545624\n",
      "epoch: 345, loss: 3.114087963104248, acc: 0.18599997460842133, test loss: 3.195828015804291, test acc: 0.08151999115943909\n",
      "epoch: 346, loss: 3.1161433243751526, acc: 0.1831199824810028, test loss: 3.2013025784492495, test acc: 0.07416000962257385\n",
      "epoch: 347, loss: 3.1146581172943115, acc: 0.18535998463630676, test loss: 3.196369550228119, test acc: 0.07727998495101929\n",
      "epoch: 348, loss: 3.1164248752593995, acc: 0.1823199987411499, test loss: 3.197477173805237, test acc: 0.07824001461267471\n",
      "epoch: 349, loss: 3.1147474312782286, acc: 0.18591997027397156, test loss: 3.1979046726226805, test acc: 0.07727999240159988\n",
      "epoch: 350, loss: 3.1133267092704773, acc: 0.1882399469614029, test loss: 3.1956638145446776, test acc: 0.08072001487016678\n",
      "epoch: 351, loss: 3.117499849796295, acc: 0.18368002772331238, test loss: 3.199122312068939, test acc: 0.07744000852108002\n",
      "epoch: 352, loss: 3.1149209928512573, acc: 0.18360000848770142, test loss: 3.1952535057067872, test acc: 0.08152000606060028\n",
      "epoch: 353, loss: 3.1146877932548525, acc: 0.18615996837615967, test loss: 3.196004238128662, test acc: 0.07919999212026596\n",
      "epoch: 354, loss: 3.1147248792648314, acc: 0.18352003395557404, test loss: 3.19549747467041, test acc: 0.07791999727487564\n",
      "epoch: 355, loss: 3.1138299322128296, acc: 0.18576005101203918, test loss: 3.1955491495132446, test acc: 0.0796000137925148\n",
      "epoch: 356, loss: 3.1149721717834473, acc: 0.18464002013206482, test loss: 3.1955892276763915, test acc: 0.08032000064849854\n",
      "epoch: 357, loss: 3.1133384490013123, acc: 0.18720002472400665, test loss: 3.1994719886779786, test acc: 0.07520001381635666\n",
      "epoch: 358, loss: 3.1150053048133852, acc: 0.18495996296405792, test loss: 3.1951225662231444, test acc: 0.08088000118732452\n",
      "epoch: 359, loss: 3.113431830406189, acc: 0.18640002608299255, test loss: 3.19576140165329, test acc: 0.08072001487016678\n",
      "epoch: 360, loss: 3.117686977386475, acc: 0.1820000261068344, test loss: 3.1966275930404664, test acc: 0.0788000151515007\n",
      "epoch: 361, loss: 3.1141686463356018, acc: 0.18591998517513275, test loss: 3.1978694701194765, test acc: 0.07832001149654388\n",
      "epoch: 362, loss: 3.115282917022705, acc: 0.18640004098415375, test loss: 3.1969080805778503, test acc: 0.08088002353906631\n",
      "epoch: 363, loss: 3.114160659313202, acc: 0.18407998979091644, test loss: 3.195153214931488, test acc: 0.07959999144077301\n",
      "epoch: 364, loss: 3.1156367087364196, acc: 0.18512000143527985, test loss: 3.1949884247779847, test acc: 0.08104000240564346\n",
      "epoch: 365, loss: 3.11529260635376, acc: 0.1839199960231781, test loss: 3.1968579077720642, test acc: 0.07824000716209412\n",
      "epoch: 366, loss: 3.113013892173767, acc: 0.18856003880500793, test loss: 3.1967833113670348, test acc: 0.07648000121116638\n",
      "epoch: 367, loss: 3.1130761170387267, acc: 0.18775996565818787, test loss: 3.1977381682395936, test acc: 0.07863999903202057\n",
      "epoch: 368, loss: 3.115477523803711, acc: 0.18375998735427856, test loss: 3.197843677997589, test acc: 0.0759199932217598\n",
      "epoch: 369, loss: 3.1155657625198363, acc: 0.18367999792099, test loss: 3.199069435596466, test acc: 0.07600000500679016\n",
      "epoch: 370, loss: 3.113927273750305, acc: 0.18423999845981598, test loss: 3.198453657627106, test acc: 0.07752001285552979\n",
      "epoch: 371, loss: 3.115560781955719, acc: 0.18160003423690796, test loss: 3.1963068056106567, test acc: 0.0796000212430954\n",
      "epoch: 372, loss: 3.115010621547699, acc: 0.18408000469207764, test loss: 3.19443115234375, test acc: 0.08112001419067383\n",
      "epoch: 373, loss: 3.114759747982025, acc: 0.18504005670547485, test loss: 3.1983450436592102, test acc: 0.07968001067638397\n",
      "epoch: 374, loss: 3.1168687534332276, acc: 0.18144002556800842, test loss: 3.1963982224464416, test acc: 0.07736000418663025\n",
      "epoch: 375, loss: 3.1160392689704897, acc: 0.18319997191429138, test loss: 3.1970048689842225, test acc: 0.07784002274274826\n",
      "epoch: 376, loss: 3.114458427429199, acc: 0.18304000794887543, test loss: 3.1989388418197633, test acc: 0.07688000053167343\n",
      "epoch: 377, loss: 3.1143600130081177, acc: 0.18511998653411865, test loss: 3.1948898816108704, test acc: 0.08000002056360245\n",
      "epoch: 378, loss: 3.115260891914368, acc: 0.18320003151893616, test loss: 3.1946400713920595, test acc: 0.0828000083565712\n",
      "epoch: 379, loss: 3.1145480918884276, acc: 0.18407998979091644, test loss: 3.195848534107208, test acc: 0.07968000322580338\n",
      "epoch: 380, loss: 3.112457962036133, acc: 0.18847998976707458, test loss: 3.1962753319740296, test acc: 0.07807999849319458\n",
      "epoch: 381, loss: 3.114445745944977, acc: 0.18575997650623322, test loss: 3.1945536851882936, test acc: 0.07968001067638397\n",
      "epoch: 382, loss: 3.1146461534500123, acc: 0.1832800656557083, test loss: 3.196577024459839, test acc: 0.07896002382040024\n",
      "epoch: 383, loss: 3.114272792339325, acc: 0.186800017952919, test loss: 3.1962084984779358, test acc: 0.0796000063419342\n",
      "epoch: 384, loss: 3.11558345079422, acc: 0.18440000712871552, test loss: 3.1950553369522097, test acc: 0.08072001487016678\n",
      "epoch: 385, loss: 3.1130753207206725, acc: 0.18728001415729523, test loss: 3.196019105911255, test acc: 0.08192002028226852\n",
      "epoch: 386, loss: 3.1124922800064088, acc: 0.18728001415729523, test loss: 3.1971056604385377, test acc: 0.07823999971151352\n",
      "epoch: 387, loss: 3.1165683031082154, acc: 0.18279998004436493, test loss: 3.196529362201691, test acc: 0.07823999971151352\n",
      "epoch: 388, loss: 3.115484380722046, acc: 0.18335996568202972, test loss: 3.1946333193778993, test acc: 0.08160000294446945\n",
      "epoch: 389, loss: 3.115673098564148, acc: 0.18488000333309174, test loss: 3.19565336227417, test acc: 0.08192002773284912\n",
      "epoch: 390, loss: 3.1146420288085936, acc: 0.18664002418518066, test loss: 3.195516004562378, test acc: 0.08088000863790512\n",
      "epoch: 391, loss: 3.1149910926818847, acc: 0.1847200095653534, test loss: 3.1948502612113954, test acc: 0.08128000050783157\n",
      "epoch: 392, loss: 3.1145487689971922, acc: 0.18312005698680878, test loss: 3.195158824920654, test acc: 0.08232001960277557\n",
      "epoch: 393, loss: 3.115522198677063, acc: 0.18440000712871552, test loss: 3.196836395263672, test acc: 0.07888000458478928\n",
      "epoch: 394, loss: 3.115028693675995, acc: 0.18423999845981598, test loss: 3.196519651412964, test acc: 0.0777600035071373\n",
      "epoch: 395, loss: 3.1156045031547546, acc: 0.18280000984668732, test loss: 3.1956520223617555, test acc: 0.08008000999689102\n",
      "epoch: 396, loss: 3.1162889575958252, acc: 0.1801600605249405, test loss: 3.1951505732536316, test acc: 0.07784000039100647\n",
      "epoch: 397, loss: 3.11562819480896, acc: 0.1823200136423111, test loss: 3.1958931493759155, test acc: 0.08016001433134079\n",
      "epoch: 398, loss: 3.114427044391632, acc: 0.18367999792099, test loss: 3.1950160098075866, test acc: 0.07999999076128006\n",
      "epoch: 399, loss: 3.1172059845924376, acc: 0.17952001094818115, test loss: 3.1980553603172304, test acc: 0.07792001217603683\n",
      "epoch: 400, loss: 3.115399103164673, acc: 0.18351997435092926, test loss: 3.1959235668182373, test acc: 0.08000000566244125\n",
      "epoch: 401, loss: 3.1132955360412597, acc: 0.18664000928401947, test loss: 3.197951490879059, test acc: 0.07728000730276108\n",
      "epoch: 402, loss: 3.114698872566223, acc: 0.18647998571395874, test loss: 3.1972302532196046, test acc: 0.07904001325368881\n",
      "epoch: 403, loss: 3.11378267288208, acc: 0.18567998707294464, test loss: 3.196560764312744, test acc: 0.07944001257419586\n",
      "epoch: 404, loss: 3.1138591980934143, acc: 0.18519997596740723, test loss: 3.1956433773040773, test acc: 0.08071999251842499\n",
      "epoch: 405, loss: 3.113873405456543, acc: 0.18647997081279755, test loss: 3.195951073169708, test acc: 0.0803999975323677\n",
      "epoch: 406, loss: 3.112647142410278, acc: 0.18623998761177063, test loss: 3.1976494884490965, test acc: 0.07688002288341522\n",
      "epoch: 407, loss: 3.1147711157798765, acc: 0.18479996919631958, test loss: 3.1954288196563723, test acc: 0.07919999212026596\n",
      "epoch: 408, loss: 3.1141609859466555, acc: 0.1860799938440323, test loss: 3.19690025806427, test acc: 0.08016001433134079\n",
      "epoch: 409, loss: 3.113025674819946, acc: 0.18688000738620758, test loss: 3.197889506816864, test acc: 0.0780000165104866\n",
      "epoch: 410, loss: 3.115068619251251, acc: 0.18568001687526703, test loss: 3.196491074562073, test acc: 0.07992000132799149\n",
      "epoch: 411, loss: 3.112938437461853, acc: 0.18704000115394592, test loss: 3.1968577766418456, test acc: 0.07903998345136642\n",
      "epoch: 412, loss: 3.113727960586548, acc: 0.18647998571395874, test loss: 3.196718780994415, test acc: 0.0812000185251236\n",
      "epoch: 413, loss: 3.1149138450622558, acc: 0.18407995998859406, test loss: 3.198578178882599, test acc: 0.07872001826763153\n",
      "epoch: 414, loss: 3.1129239201545715, acc: 0.18768006563186646, test loss: 3.1973026847839354, test acc: 0.07799999415874481\n",
      "epoch: 415, loss: 3.1149577927589416, acc: 0.18408000469207764, test loss: 3.194460813999176, test acc: 0.08431999385356903\n",
      "epoch: 416, loss: 3.11473087310791, acc: 0.1848800778388977, test loss: 3.1968194031715393, test acc: 0.07736001908779144\n",
      "epoch: 417, loss: 3.113241500854492, acc: 0.18672004342079163, test loss: 3.1973254251480103, test acc: 0.08000000566244125\n",
      "epoch: 418, loss: 3.1143340635299683, acc: 0.18399998545646667, test loss: 3.195122408866882, test acc: 0.07864001393318176\n",
      "epoch: 419, loss: 3.1145147109031677, acc: 0.18544000387191772, test loss: 3.195687472820282, test acc: 0.08303998410701752\n",
      "epoch: 420, loss: 3.1153949236869813, acc: 0.18448001146316528, test loss: 3.195854060649872, test acc: 0.08087999373674393\n",
      "epoch: 421, loss: 3.112250950336456, acc: 0.18775998055934906, test loss: 3.195582354068756, test acc: 0.0809599980711937\n",
      "epoch: 422, loss: 3.113451497554779, acc: 0.18632005155086517, test loss: 3.198207538127899, test acc: 0.07871998846530914\n",
      "epoch: 423, loss: 3.113547971248627, acc: 0.18672004342079163, test loss: 3.1963584852218627, test acc: 0.08047999441623688\n",
      "epoch: 424, loss: 3.112928330898285, acc: 0.18568001687526703, test loss: 3.1958134317398073, test acc: 0.08159999549388885\n",
      "epoch: 425, loss: 3.1138680243492125, acc: 0.18752005696296692, test loss: 3.1939344096183775, test acc: 0.08208000659942627\n",
      "epoch: 426, loss: 3.111727187633514, acc: 0.1871199756860733, test loss: 3.1946016907691956, test acc: 0.08360002189874649\n",
      "epoch: 427, loss: 3.1145906043052674, acc: 0.1858399659395218, test loss: 3.197512547969818, test acc: 0.07896000891923904\n",
      "epoch: 428, loss: 3.114333145618439, acc: 0.18432003259658813, test loss: 3.1961217617988584, test acc: 0.08055999875068665\n",
      "epoch: 429, loss: 3.1134248208999633, acc: 0.18560002744197845, test loss: 3.1949692797660827, test acc: 0.08184000849723816\n",
      "epoch: 430, loss: 3.111830415725708, acc: 0.18879999220371246, test loss: 3.197395532131195, test acc: 0.07680000364780426\n",
      "epoch: 431, loss: 3.1141460871696474, acc: 0.1847200095653534, test loss: 3.1968054437637328, test acc: 0.08184000104665756\n",
      "epoch: 432, loss: 3.1121783328056334, acc: 0.18856000900268555, test loss: 3.1955041456222535, test acc: 0.07888000458478928\n",
      "epoch: 433, loss: 3.1139930367469786, acc: 0.18504002690315247, test loss: 3.1979195189476015, test acc: 0.07584001123905182\n",
      "epoch: 434, loss: 3.1133875679969787, acc: 0.1880800426006317, test loss: 3.196243717670441, test acc: 0.07808000594377518\n",
      "epoch: 435, loss: 3.115780973434448, acc: 0.18272005021572113, test loss: 3.194229919910431, test acc: 0.08295999467372894\n",
      "epoch: 436, loss: 3.1131701564788816, acc: 0.18535998463630676, test loss: 3.193770899772644, test acc: 0.08152000606060028\n",
      "epoch: 437, loss: 3.1120408749580384, acc: 0.1892000138759613, test loss: 3.1961219549179076, test acc: 0.0836000069975853\n",
      "epoch: 438, loss: 3.113079228401184, acc: 0.18671996891498566, test loss: 3.1956867933273316, test acc: 0.08000000566244125\n",
      "epoch: 439, loss: 3.1143376231193542, acc: 0.18656004965305328, test loss: 3.193786814212799, test acc: 0.0828000083565712\n",
      "epoch: 440, loss: 3.112222158908844, acc: 0.18639999628067017, test loss: 3.196366758346558, test acc: 0.07896002382040024\n",
      "epoch: 441, loss: 3.1131157231330873, acc: 0.18784001469612122, test loss: 3.197257900238037, test acc: 0.07871999591588974\n",
      "epoch: 442, loss: 3.113405632972717, acc: 0.18783998489379883, test loss: 3.195990686416626, test acc: 0.08208002150058746\n",
      "epoch: 443, loss: 3.114740104675293, acc: 0.18463996052742004, test loss: 3.1947692036628723, test acc: 0.08112002164125443\n",
      "epoch: 444, loss: 3.1129814434051513, acc: 0.18743999302387238, test loss: 3.1945917916297915, test acc: 0.08304000645875931\n",
      "epoch: 445, loss: 3.1137348866462706, acc: 0.18615998327732086, test loss: 3.19668212890625, test acc: 0.07728000730276108\n",
      "epoch: 446, loss: 3.115361795425415, acc: 0.18223997950553894, test loss: 3.1978965282440184, test acc: 0.07944001257419586\n",
      "epoch: 447, loss: 3.1137140417099, acc: 0.1863199919462204, test loss: 3.198225374221802, test acc: 0.07872001826763153\n",
      "epoch: 448, loss: 3.1147943902015687, acc: 0.18456001579761505, test loss: 3.1961832523345945, test acc: 0.07992001622915268\n",
      "epoch: 449, loss: 3.1143949532508852, acc: 0.186800017952919, test loss: 3.196507830619812, test acc: 0.0804000049829483\n",
      "epoch: 450, loss: 3.1156788730621336, acc: 0.1836000382900238, test loss: 3.1938218450546265, test acc: 0.0836000069975853\n",
      "epoch: 451, loss: 3.115842866897583, acc: 0.18456001579761505, test loss: 3.197097086906433, test acc: 0.07959999144077301\n",
      "epoch: 452, loss: 3.112857041358948, acc: 0.18807999789714813, test loss: 3.1974031734466553, test acc: 0.0793599933385849\n",
      "epoch: 453, loss: 3.116039183139801, acc: 0.18592002987861633, test loss: 3.1987244701385498, test acc: 0.07656000554561615\n",
      "epoch: 454, loss: 3.1146749210357667, acc: 0.18352003395557404, test loss: 3.19540935754776, test acc: 0.07904001325368881\n",
      "epoch: 455, loss: 3.116558828353882, acc: 0.18136005103588104, test loss: 3.1970002603530885, test acc: 0.08024001866579056\n",
      "epoch: 456, loss: 3.1144366383552553, acc: 0.18568000197410583, test loss: 3.1959108662605287, test acc: 0.08176001161336899\n",
      "epoch: 457, loss: 3.113357677459717, acc: 0.18663997948169708, test loss: 3.1958518981933595, test acc: 0.0788000077009201\n",
      "epoch: 458, loss: 3.1129258918762206, acc: 0.18775998055934906, test loss: 3.197153024673462, test acc: 0.07903999835252762\n",
      "epoch: 459, loss: 3.114221258163452, acc: 0.185279980301857, test loss: 3.1966103768348693, test acc: 0.07831999659538269\n",
      "epoch: 460, loss: 3.1098141503334045, acc: 0.19056005775928497, test loss: 3.2001422214508057, test acc: 0.0751200020313263\n",
      "epoch: 461, loss: 3.1137791419029237, acc: 0.185760036110878, test loss: 3.1938548922538756, test acc: 0.08199996501207352\n",
      "epoch: 462, loss: 3.114318208694458, acc: 0.1862400323152542, test loss: 3.1940269923210143, test acc: 0.0835999920964241\n",
      "epoch: 463, loss: 3.1123907494544985, acc: 0.18775998055934906, test loss: 3.1944472217559814, test acc: 0.0819999948143959\n",
      "epoch: 464, loss: 3.1155189538002013, acc: 0.183119997382164, test loss: 3.194878478050232, test acc: 0.07944001257419586\n",
      "epoch: 465, loss: 3.1124826669692993, acc: 0.1880800426006317, test loss: 3.1955267572402954, test acc: 0.08016001433134079\n",
      "epoch: 466, loss: 3.11043475151062, acc: 0.19072000682353973, test loss: 3.1966107177734373, test acc: 0.0788000077009201\n",
      "epoch: 467, loss: 3.1148341298103333, acc: 0.1834399700164795, test loss: 3.1956279063224793, test acc: 0.07863999158143997\n",
      "epoch: 468, loss: 3.1137183475494385, acc: 0.1844799816608429, test loss: 3.196163251399994, test acc: 0.07952000200748444\n",
      "epoch: 469, loss: 3.1134437680244447, acc: 0.1865599900484085, test loss: 3.198968195915222, test acc: 0.07672002911567688\n",
      "epoch: 470, loss: 3.113700816631317, acc: 0.187360018491745, test loss: 3.196883955001831, test acc: 0.07824001461267471\n",
      "epoch: 471, loss: 3.1138209223747255, acc: 0.18536001443862915, test loss: 3.1982559323310853, test acc: 0.07744000852108002\n",
      "epoch: 472, loss: 3.1166569137573243, acc: 0.18264007568359375, test loss: 3.1967756056785586, test acc: 0.07791999727487564\n",
      "epoch: 473, loss: 3.113705849647522, acc: 0.18608003854751587, test loss: 3.194255003929138, test acc: 0.08152001351118088\n",
      "epoch: 474, loss: 3.1146993589401246, acc: 0.1841600239276886, test loss: 3.1930868148803713, test acc: 0.08463999629020691\n",
      "epoch: 475, loss: 3.1140075063705446, acc: 0.18592000007629395, test loss: 3.1978922986984255, test acc: 0.07584001868963242\n",
      "epoch: 476, loss: 3.113966507911682, acc: 0.1855199933052063, test loss: 3.1969152379035948, test acc: 0.07912000268697739\n",
      "epoch: 477, loss: 3.1118526601791383, acc: 0.1899999976158142, test loss: 3.196096806526184, test acc: 0.08016000688076019\n",
      "epoch: 478, loss: 3.1135960817337036, acc: 0.18703995645046234, test loss: 3.1944273614883425, test acc: 0.08080000430345535\n",
      "epoch: 479, loss: 3.1129221630096438, acc: 0.18744002282619476, test loss: 3.1978742051124573, test acc: 0.07840000838041306\n",
      "epoch: 480, loss: 3.112610809803009, acc: 0.18856006860733032, test loss: 3.1978958368301393, test acc: 0.07688002288341522\n",
      "epoch: 481, loss: 3.114297351837158, acc: 0.18528005480766296, test loss: 3.1959960675239563, test acc: 0.08167999237775803\n",
      "epoch: 482, loss: 3.111811420917511, acc: 0.1884799748659134, test loss: 3.1951511311531067, test acc: 0.08296000957489014\n",
      "epoch: 483, loss: 3.1122393345832826, acc: 0.18959999084472656, test loss: 3.197061822414398, test acc: 0.07967999577522278\n",
      "epoch: 484, loss: 3.1139654898643494, acc: 0.18480004370212555, test loss: 3.1968488621711733, test acc: 0.08008001744747162\n",
      "epoch: 485, loss: 3.1147070693969727, acc: 0.1857600212097168, test loss: 3.196997117996216, test acc: 0.08128000050783157\n",
      "epoch: 486, loss: 3.1136350321769712, acc: 0.1852000206708908, test loss: 3.19552561044693, test acc: 0.08047999441623688\n",
      "epoch: 487, loss: 3.113706157207489, acc: 0.1863199919462204, test loss: 3.1964708042144774, test acc: 0.0793600082397461\n",
      "epoch: 488, loss: 3.1146610951423646, acc: 0.18480002880096436, test loss: 3.1963212585449217, test acc: 0.07807999104261398\n",
      "epoch: 489, loss: 3.112328805923462, acc: 0.18952003121376038, test loss: 3.194287369251251, test acc: 0.08104000985622406\n",
      "epoch: 490, loss: 3.112397711277008, acc: 0.18719999492168427, test loss: 3.1983274388313294, test acc: 0.078000009059906\n",
      "epoch: 491, loss: 3.113997707366943, acc: 0.1871199905872345, test loss: 3.198667986392975, test acc: 0.0788000077009201\n",
      "epoch: 492, loss: 3.1127319812774656, acc: 0.1879199743270874, test loss: 3.195344088077545, test acc: 0.08335999399423599\n",
      "epoch: 493, loss: 3.114287564754486, acc: 0.18616004288196564, test loss: 3.195523805618286, test acc: 0.07992001622915268\n",
      "epoch: 494, loss: 3.1133506655693055, acc: 0.18776002526283264, test loss: 3.1966136932373046, test acc: 0.07928001135587692\n",
      "epoch: 495, loss: 3.113706953525543, acc: 0.18672002851963043, test loss: 3.197255117893219, test acc: 0.08032000064849854\n",
      "epoch: 496, loss: 3.113499467372894, acc: 0.18511997163295746, test loss: 3.1980971455574037, test acc: 0.07648000121116638\n",
      "epoch: 497, loss: 3.1137219667434692, acc: 0.18672002851963043, test loss: 3.1967813420295714, test acc: 0.0769600123167038\n",
      "epoch: 498, loss: 3.1149863314628603, acc: 0.18352003395557404, test loss: 3.1969091725349426, test acc: 0.07840000092983246\n",
      "epoch: 499, loss: 3.113280394077301, acc: 0.1852799952030182, test loss: 3.1959044909477234, test acc: 0.07992000132799149\n",
      "epoch: 500, loss: 3.112774088382721, acc: 0.1873599737882614, test loss: 3.198479039669037, test acc: 0.07824000716209412\n",
      "epoch: 501, loss: 3.113107535839081, acc: 0.1879199892282486, test loss: 3.196452615261078, test acc: 0.0793600082397461\n",
      "epoch: 502, loss: 3.113696603775024, acc: 0.1879199892282486, test loss: 3.1970320749282837, test acc: 0.07888001948595047\n",
      "epoch: 503, loss: 3.113147511482239, acc: 0.18528001010417938, test loss: 3.196147787570953, test acc: 0.07896000146865845\n",
      "epoch: 504, loss: 3.1142241621017455, acc: 0.1855199635028839, test loss: 3.1962948274612426, test acc: 0.08064001053571701\n",
      "epoch: 505, loss: 3.112805926799774, acc: 0.18799996376037598, test loss: 3.1971330523490904, test acc: 0.0761600211262703\n",
      "epoch: 506, loss: 3.113096570968628, acc: 0.187360018491745, test loss: 3.199970369338989, test acc: 0.07760001718997955\n",
      "epoch: 507, loss: 3.112722990512848, acc: 0.18632003664970398, test loss: 3.195086829662323, test acc: 0.0820000022649765\n",
      "epoch: 508, loss: 3.1127958250045777, acc: 0.18855994939804077, test loss: 3.1968127274513245, test acc: 0.08071999996900558\n",
      "epoch: 509, loss: 3.1140182065963744, acc: 0.18719999492168427, test loss: 3.1981695175170897, test acc: 0.07760000973939896\n",
      "epoch: 510, loss: 3.1126669359207155, acc: 0.1879199743270874, test loss: 3.194435796737671, test acc: 0.08367996662855148\n",
      "epoch: 511, loss: 3.1123535299301146, acc: 0.18904004991054535, test loss: 3.196223590373993, test acc: 0.07815999537706375\n",
      "epoch: 512, loss: 3.114281370639801, acc: 0.18448001146316528, test loss: 3.195629241466522, test acc: 0.0828000158071518\n",
      "epoch: 513, loss: 3.114004526138306, acc: 0.18464000523090363, test loss: 3.196990098953247, test acc: 0.0777600109577179\n",
      "epoch: 514, loss: 3.1132755041122437, acc: 0.1900000274181366, test loss: 3.196688508987427, test acc: 0.07968001067638397\n",
      "epoch: 515, loss: 3.114741396903992, acc: 0.1862400621175766, test loss: 3.197588505744934, test acc: 0.07791999727487564\n",
      "epoch: 516, loss: 3.114644055366516, acc: 0.184719979763031, test loss: 3.1980538249015806, test acc: 0.07631998509168625\n",
      "epoch: 517, loss: 3.1152065777778626, acc: 0.1847200095653534, test loss: 3.196419758796692, test acc: 0.08080000430345535\n",
      "epoch: 518, loss: 3.1143366265296937, acc: 0.18568000197410583, test loss: 3.195716824531555, test acc: 0.07912000268697739\n",
      "epoch: 519, loss: 3.1140578389167786, acc: 0.18632005155086517, test loss: 3.1984254574775695, test acc: 0.07688000798225403\n",
      "epoch: 520, loss: 3.115103304386139, acc: 0.1839200109243393, test loss: 3.196966850757599, test acc: 0.078560009598732\n",
      "epoch: 521, loss: 3.1139923095703126, acc: 0.18567992746829987, test loss: 3.195859525203705, test acc: 0.08016001433134079\n",
      "epoch: 522, loss: 3.1117306756973266, acc: 0.1876799613237381, test loss: 3.1983625197410586, test acc: 0.07799999415874481\n",
      "epoch: 523, loss: 3.1131770968437196, acc: 0.18728004395961761, test loss: 3.197442302703857, test acc: 0.07759998738765717\n",
      "epoch: 524, loss: 3.1122256970405577, acc: 0.1881599724292755, test loss: 3.1968227362632753, test acc: 0.07919999212026596\n",
      "epoch: 525, loss: 3.112407033443451, acc: 0.18639999628067017, test loss: 3.1950913500785827, test acc: 0.08071999996900558\n",
      "epoch: 526, loss: 3.112958695888519, acc: 0.18839995563030243, test loss: 3.195755660533905, test acc: 0.08032002300024033\n",
      "epoch: 527, loss: 3.113933401107788, acc: 0.18696001172065735, test loss: 3.196326262950897, test acc: 0.07959997653961182\n",
      "epoch: 528, loss: 3.1123063015937804, acc: 0.1902400404214859, test loss: 3.1948685002326966, test acc: 0.08008001744747162\n",
      "epoch: 529, loss: 3.1139018368721008, acc: 0.18504002690315247, test loss: 3.196108727455139, test acc: 0.07824001461267471\n",
      "epoch: 530, loss: 3.1123192262649537, acc: 0.1868000626564026, test loss: 3.196562693119049, test acc: 0.07864001393318176\n",
      "epoch: 531, loss: 3.1118348169326784, acc: 0.1884000301361084, test loss: 3.195855987071991, test acc: 0.07983999699354172\n",
      "epoch: 532, loss: 3.113331775665283, acc: 0.1881600171327591, test loss: 3.1966992568969728, test acc: 0.07928000390529633\n",
      "epoch: 533, loss: 3.1135174942016604, acc: 0.18536002933979034, test loss: 3.1952914118766786, test acc: 0.07896000891923904\n",
      "epoch: 534, loss: 3.114106209278107, acc: 0.1858399659395218, test loss: 3.196339712142944, test acc: 0.08104001730680466\n",
      "epoch: 535, loss: 3.1134167647361757, acc: 0.18855996429920197, test loss: 3.1967113471031188, test acc: 0.07871999591588974\n",
      "epoch: 536, loss: 3.1126890850067137, acc: 0.18967996537685394, test loss: 3.198065893650055, test acc: 0.07656001299619675\n",
      "epoch: 537, loss: 3.110932049751282, acc: 0.19112002849578857, test loss: 3.195674681663513, test acc: 0.0809599980711937\n",
      "epoch: 538, loss: 3.111470184326172, acc: 0.19087998569011688, test loss: 3.1946642804145813, test acc: 0.08136000484228134\n",
      "epoch: 539, loss: 3.113262732028961, acc: 0.18800002336502075, test loss: 3.194355866909027, test acc: 0.08208000659942627\n",
      "epoch: 540, loss: 3.11212082862854, acc: 0.1895199716091156, test loss: 3.196720676422119, test acc: 0.07832001149654388\n",
      "epoch: 541, loss: 3.1109846138954165, acc: 0.18984000384807587, test loss: 3.196122567653656, test acc: 0.0785599946975708\n",
      "epoch: 542, loss: 3.112196516990662, acc: 0.1873600035905838, test loss: 3.195587167739868, test acc: 0.07920000702142715\n",
      "epoch: 543, loss: 3.113465218544006, acc: 0.18664000928401947, test loss: 3.195906944274902, test acc: 0.08008000254631042\n",
      "epoch: 544, loss: 3.1126693296432495, acc: 0.18712005019187927, test loss: 3.1948909831047057, test acc: 0.08224000036716461\n",
      "epoch: 545, loss: 3.112015926837921, acc: 0.18967999517917633, test loss: 3.1942004776000976, test acc: 0.08184000849723816\n",
      "epoch: 546, loss: 3.1138651156425476, acc: 0.18696001172065735, test loss: 3.1985428953170776, test acc: 0.07592001557350159\n",
      "epoch: 547, loss: 3.1131308341026305, acc: 0.1858399510383606, test loss: 3.1956141352653504, test acc: 0.07863999903202057\n",
      "epoch: 548, loss: 3.1129854273796083, acc: 0.18712002038955688, test loss: 3.19493524312973, test acc: 0.08048000186681747\n",
      "epoch: 549, loss: 3.1130638003349302, acc: 0.18727998435497284, test loss: 3.197378616333008, test acc: 0.07888001203536987\n",
      "epoch: 550, loss: 3.1150290417671203, acc: 0.1863199919462204, test loss: 3.1966988372802736, test acc: 0.07760003209114075\n",
      "epoch: 551, loss: 3.116472239494324, acc: 0.18264004588127136, test loss: 3.1971046805381773, test acc: 0.08096001297235489\n",
      "epoch: 552, loss: 3.1127224469184878, acc: 0.18839998543262482, test loss: 3.1976361083984375, test acc: 0.07896002382040024\n",
      "epoch: 553, loss: 3.1130297660827635, acc: 0.18424001336097717, test loss: 3.196541073322296, test acc: 0.07863999903202057\n",
      "epoch: 554, loss: 3.1109723949432375, acc: 0.18960006535053253, test loss: 3.1947797417640684, test acc: 0.08319999277591705\n",
      "epoch: 555, loss: 3.1136028122901918, acc: 0.184160053730011, test loss: 3.1964836287498475, test acc: 0.07976001501083374\n",
      "epoch: 556, loss: 3.1121864175796508, acc: 0.18823999166488647, test loss: 3.196670136451721, test acc: 0.08000002056360245\n",
      "epoch: 557, loss: 3.113421003818512, acc: 0.18584001064300537, test loss: 3.1945687985420226, test acc: 0.08088000118732452\n",
      "epoch: 558, loss: 3.112765941619873, acc: 0.18743999302387238, test loss: 3.1961754965782165, test acc: 0.07944001257419586\n",
      "epoch: 559, loss: 3.113731548786163, acc: 0.1857600212097168, test loss: 3.196214706897736, test acc: 0.07959999889135361\n",
      "epoch: 560, loss: 3.112743833065033, acc: 0.18567998707294464, test loss: 3.1952173137664794, test acc: 0.07967999577522278\n",
      "epoch: 561, loss: 3.1129612159729003, acc: 0.1873600333929062, test loss: 3.194753096103668, test acc: 0.07984001189470291\n",
      "epoch: 562, loss: 3.1127019906044007, acc: 0.18903999030590057, test loss: 3.196087338924408, test acc: 0.08056001365184784\n",
      "epoch: 563, loss: 3.1131126713752746, acc: 0.18608000874519348, test loss: 3.1967984771728517, test acc: 0.0788000151515007\n",
      "epoch: 564, loss: 3.112950098514557, acc: 0.18640001118183136, test loss: 3.194597611427307, test acc: 0.0811999961733818\n",
      "epoch: 565, loss: 3.1132570552825927, acc: 0.18775995075702667, test loss: 3.1961483788490295, test acc: 0.07952000200748444\n",
      "epoch: 566, loss: 3.113697428703308, acc: 0.18672002851963043, test loss: 3.196958644390106, test acc: 0.07976000010967255\n",
      "epoch: 567, loss: 3.1154524087905884, acc: 0.18408004939556122, test loss: 3.1952645897865297, test acc: 0.08144000172615051\n",
      "epoch: 568, loss: 3.1160333824157713, acc: 0.17992003262043, test loss: 3.195237283706665, test acc: 0.08183998614549637\n",
      "epoch: 569, loss: 3.1133830428123472, acc: 0.1882399320602417, test loss: 3.1952927136421203, test acc: 0.08128000795841217\n",
      "epoch: 570, loss: 3.113236129283905, acc: 0.186319962143898, test loss: 3.1977014327049256, test acc: 0.07832000404596329\n",
      "epoch: 571, loss: 3.1137723088264466, acc: 0.18696004152297974, test loss: 3.1955247068405153, test acc: 0.07823999971151352\n",
      "epoch: 572, loss: 3.115785367488861, acc: 0.18272006511688232, test loss: 3.1959187626838683, test acc: 0.08151999115943909\n",
      "epoch: 573, loss: 3.1152873611450196, acc: 0.18224003911018372, test loss: 3.194577317237854, test acc: 0.08032000064849854\n",
      "epoch: 574, loss: 3.113820264339447, acc: 0.18432000279426575, test loss: 3.1974363851547243, test acc: 0.07648000121116638\n",
      "epoch: 575, loss: 3.114286940097809, acc: 0.1854400336742401, test loss: 3.1949253058433533, test acc: 0.08088000118732452\n",
      "epoch: 576, loss: 3.112740731239319, acc: 0.18624000251293182, test loss: 3.1958881616592407, test acc: 0.07912000268697739\n",
      "epoch: 577, loss: 3.1140559458732606, acc: 0.1860799938440323, test loss: 3.1973444986343384, test acc: 0.07799999415874481\n",
      "epoch: 578, loss: 3.1134604477882384, acc: 0.18480001389980316, test loss: 3.1972913718223572, test acc: 0.0793600082397461\n",
      "epoch: 579, loss: 3.1148594188690186, acc: 0.18384003639221191, test loss: 3.1982613086700438, test acc: 0.07623999565839767\n",
      "epoch: 580, loss: 3.115934889316559, acc: 0.18400004506111145, test loss: 3.1958355355262755, test acc: 0.07840001583099365\n",
      "epoch: 581, loss: 3.114124047756195, acc: 0.18480001389980316, test loss: 3.197687892913818, test acc: 0.07712001353502274\n",
      "epoch: 582, loss: 3.1153324675559997, acc: 0.18424002826213837, test loss: 3.1940955305099488, test acc: 0.08112001419067383\n",
      "epoch: 583, loss: 3.11338397026062, acc: 0.1855199784040451, test loss: 3.194861454963684, test acc: 0.08096001297235489\n",
      "epoch: 584, loss: 3.1139497780799865, acc: 0.1876799762248993, test loss: 3.1962741088867186, test acc: 0.08079999685287476\n",
      "epoch: 585, loss: 3.1138906121253966, acc: 0.1868799924850464, test loss: 3.1960352301597594, test acc: 0.08016000688076019\n",
      "epoch: 586, loss: 3.1134402632713316, acc: 0.1852799952030182, test loss: 3.196830382347107, test acc: 0.07959999144077301\n",
      "epoch: 587, loss: 3.113691501617432, acc: 0.1876000165939331, test loss: 3.196681332588196, test acc: 0.07927999645471573\n",
      "epoch: 588, loss: 3.1141040921211243, acc: 0.18704000115394592, test loss: 3.197218744754791, test acc: 0.07879999279975891\n",
      "epoch: 589, loss: 3.114694347381592, acc: 0.18432003259658813, test loss: 3.196989371776581, test acc: 0.07976000010967255\n",
      "epoch: 590, loss: 3.1146489810943603, acc: 0.18575997650623322, test loss: 3.1966043639183046, test acc: 0.07888000458478928\n",
      "epoch: 591, loss: 3.114838407039642, acc: 0.18528005480766296, test loss: 3.1955308985710142, test acc: 0.07944001257419586\n",
      "epoch: 592, loss: 3.11327819108963, acc: 0.18728004395961761, test loss: 3.1974683094024656, test acc: 0.07912000268697739\n",
      "epoch: 593, loss: 3.11336510181427, acc: 0.18863999843597412, test loss: 3.1962315440177917, test acc: 0.07912001758813858\n",
      "epoch: 594, loss: 3.1133877444267273, acc: 0.18536005914211273, test loss: 3.1957902097702027, test acc: 0.07872001081705093\n",
      "epoch: 595, loss: 3.1132839035987856, acc: 0.18703998625278473, test loss: 3.1982264161109923, test acc: 0.0793600082397461\n",
      "epoch: 596, loss: 3.113398861885071, acc: 0.18872001767158508, test loss: 3.194308695793152, test acc: 0.08288002014160156\n",
      "epoch: 597, loss: 3.112385587692261, acc: 0.1884000152349472, test loss: 3.197726142406464, test acc: 0.07863999903202057\n",
      "epoch: 598, loss: 3.114587817192078, acc: 0.1836799830198288, test loss: 3.197474789619446, test acc: 0.07768000662326813\n",
      "epoch: 599, loss: 3.113513796329498, acc: 0.18624001741409302, test loss: 3.1964898753166198, test acc: 0.08048000931739807\n",
      "epoch: 600, loss: 3.1121338272094725, acc: 0.1895199865102768, test loss: 3.195633566379547, test acc: 0.08007999509572983\n",
      "epoch: 601, loss: 3.112577373981476, acc: 0.18824002146720886, test loss: 3.1946362042427063, test acc: 0.08016000688076019\n",
      "epoch: 602, loss: 3.111557147502899, acc: 0.18904000520706177, test loss: 3.196853663921356, test acc: 0.08048003166913986\n",
      "epoch: 603, loss: 3.112304928302765, acc: 0.18824000656604767, test loss: 3.1953676557540893, test acc: 0.07912002503871918\n",
      "epoch: 604, loss: 3.1146071338653565, acc: 0.18655996024608612, test loss: 3.196156883239746, test acc: 0.08056000620126724\n",
      "epoch: 605, loss: 3.114232876300812, acc: 0.18440000712871552, test loss: 3.1961326718330385, test acc: 0.07951999455690384\n",
      "epoch: 606, loss: 3.1125588846206664, acc: 0.18727998435497284, test loss: 3.1956231784820557, test acc: 0.08039999008178711\n",
      "epoch: 607, loss: 3.1135064268112185, acc: 0.18512003123760223, test loss: 3.195203106403351, test acc: 0.08344000577926636\n",
      "epoch: 608, loss: 3.1140185713768007, acc: 0.18512003123760223, test loss: 3.2002373576164245, test acc: 0.0711200088262558\n",
      "epoch: 609, loss: 3.1130365324020386, acc: 0.18752005696296692, test loss: 3.1955409097671508, test acc: 0.0785599946975708\n",
      "epoch: 610, loss: 3.1145153045654297, acc: 0.18376000225543976, test loss: 3.197108232975006, test acc: 0.07888000458478928\n",
      "epoch: 611, loss: 3.1128541326522825, acc: 0.18960000574588776, test loss: 3.1968934392929076, test acc: 0.07824001461267471\n",
      "epoch: 612, loss: 3.1127914142608644, acc: 0.18639995157718658, test loss: 3.194427540302277, test acc: 0.0820000171661377\n",
      "epoch: 613, loss: 3.112004039287567, acc: 0.18768000602722168, test loss: 3.1950990343093872, test acc: 0.0788000077009201\n",
      "epoch: 614, loss: 3.1128327083587646, acc: 0.1863199919462204, test loss: 3.1966230964660642, test acc: 0.07888001948595047\n",
      "epoch: 615, loss: 3.112686131000519, acc: 0.18696002662181854, test loss: 3.1965185475349425, test acc: 0.08056000620126724\n",
      "epoch: 616, loss: 3.1131412839889525, acc: 0.18695996701717377, test loss: 3.1966391468048094, test acc: 0.08031999319791794\n",
      "epoch: 617, loss: 3.1119658541679383, acc: 0.1873599737882614, test loss: 3.1966876339912416, test acc: 0.07903999835252762\n",
      "epoch: 618, loss: 3.114365074634552, acc: 0.18496005237102509, test loss: 3.1967967891693116, test acc: 0.07840000838041306\n",
      "epoch: 619, loss: 3.112942962646484, acc: 0.18592005968093872, test loss: 3.195742838382721, test acc: 0.07976000010967255\n",
      "epoch: 620, loss: 3.1134005045890807, acc: 0.18623998761177063, test loss: 3.196422526836395, test acc: 0.0804000198841095\n",
      "epoch: 621, loss: 3.1137818336486816, acc: 0.18528001010417938, test loss: 3.1959807658195496, test acc: 0.08160000294446945\n",
      "epoch: 622, loss: 3.113026740550995, acc: 0.18824002146720886, test loss: 3.198425703048706, test acc: 0.07632001489400864\n",
      "epoch: 623, loss: 3.115638942718506, acc: 0.18320001661777496, test loss: 3.1962243938446044, test acc: 0.08160000294446945\n",
      "epoch: 624, loss: 3.114316608905792, acc: 0.18480007350444794, test loss: 3.1963927149772644, test acc: 0.08024001121520996\n",
      "epoch: 625, loss: 3.1148385977745057, acc: 0.18655996024608612, test loss: 3.195391073226929, test acc: 0.08111998438835144\n",
      "epoch: 626, loss: 3.112477912902832, acc: 0.18775998055934906, test loss: 3.194900481700897, test acc: 0.07999999821186066\n",
      "epoch: 627, loss: 3.11495521068573, acc: 0.18431998789310455, test loss: 3.1954557299613953, test acc: 0.08256000280380249\n",
      "epoch: 628, loss: 3.1116658735275267, acc: 0.19056007266044617, test loss: 3.195900008678436, test acc: 0.08080000430345535\n",
      "epoch: 629, loss: 3.113540222644806, acc: 0.18647997081279755, test loss: 3.1999818181991575, test acc: 0.07728000730276108\n",
      "epoch: 630, loss: 3.1114945125579836, acc: 0.18991997838020325, test loss: 3.197477762699127, test acc: 0.07959999889135361\n",
      "epoch: 631, loss: 3.115233941078186, acc: 0.1831199675798416, test loss: 3.196100342273712, test acc: 0.08087997883558273\n",
      "epoch: 632, loss: 3.113245804309845, acc: 0.18864001333713531, test loss: 3.1977846121788023, test acc: 0.0772000104188919\n",
      "epoch: 633, loss: 3.1126458144187925, acc: 0.18768003582954407, test loss: 3.1983762431144713, test acc: 0.07856001704931259\n",
      "epoch: 634, loss: 3.1148276567459106, acc: 0.1847200095653534, test loss: 3.1995451784133913, test acc: 0.07464000582695007\n",
      "epoch: 635, loss: 3.1128986573219297, acc: 0.18783999979496002, test loss: 3.1959305477142332, test acc: 0.08079998195171356\n",
      "epoch: 636, loss: 3.1142543077468874, acc: 0.18335996568202972, test loss: 3.196591157913208, test acc: 0.08008000999689102\n",
      "epoch: 637, loss: 3.1143759202957155, acc: 0.1853601038455963, test loss: 3.1948755764961243, test acc: 0.0809599831700325\n",
      "epoch: 638, loss: 3.1130057621002196, acc: 0.19032002985477448, test loss: 3.1981803345680238, test acc: 0.07791998982429504\n",
      "epoch: 639, loss: 3.1148979330062865, acc: 0.18351997435092926, test loss: 3.1966713547706602, test acc: 0.07704000174999237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\soken\\Documents\\22NM021S\\Master-s_thesis\\Ising\\supervised_learning_v3.ipynb セル 26\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/soken/Documents/22NM021S/Master-s_thesis/Ising/supervised_learning_v3.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/soken/Documents/22NM021S/Master-s_thesis/Ising/supervised_learning_v3.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m running_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/soken/Documents/22NM021S/Master-s_thesis/Ising/supervised_learning_v3.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/soken/Documents/22NM021S/Master-s_thesis/Ising/supervised_learning_v3.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/soken/Documents/22NM021S/Master-s_thesis/Ising/supervised_learning_v3.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\soken\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "num_epochs = 7500\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(output, dim=1)      # outputの一番値が大きい成分\n",
    "        targets = torch.argmax(targets, dim=1)  # targetの一番値が大きい成分\n",
    "        running_acc += torch.mean(pred.eq(targets).float()) # predとtargetが同じになった数の平均\n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader)   # 12500個の訓練データ全体での損失関数の平均\n",
    "    running_acc /= len(train_loader)    # 12500個の訓練データ全体での正解率\n",
    "    train_losses.append(running_loss)\n",
    "    train_accs.append(running_acc)\n",
    "    #\n",
    "    #   test loop\n",
    "    #\n",
    "    test_running_loss = 0.0\n",
    "    test_running_acc = 0.0\n",
    "    for test_inputs, test_targets in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        test_output = model(test_inputs)\n",
    "        test_loss = criterion(test_output, test_targets)\n",
    "        test_running_loss += test_loss.item()\n",
    "        test_pred = torch.argmax(test_output, dim=1)      # outputの一番値が大きい成分\n",
    "        test_targets = torch.argmax(test_targets, dim=1)  # targetの一番値が大きい成分\n",
    "        test_running_acc += torch.mean(test_pred.eq(test_targets).float()) # predとtargetが同じになった数の平均\n",
    "    test_running_loss /= len(test_loader)   # 12500個の訓練データ全体での損失関数の平均\n",
    "    test_running_acc /= len(test_loader)    # 12500個の訓練データ全体での正解率\n",
    "    test_losses.append(test_running_loss)\n",
    "    test_accs.append(test_running_acc)\n",
    "        \n",
    "    print(\"epoch: {}, loss: {}, acc: {}, test loss: {}, test acc: {}\".format(epoch, running_loss, running_acc, test_running_loss, test_running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と正解率のグラフの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 損失関数のグラフ\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# 正解率のグラフ\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_accs, label='Training Accuracy', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に予測値と正解がどうなっているか見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 20,  4, 19,  1, 11,  9, 15,  1, 15,  1, 14, 23, 11,  2, 19, 18, 18,\n",
      "        21, 18])\n"
     ]
    }
   ],
   "source": [
    "print(pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 12,  4, 19,  1, 11,  7, 19,  1, 15,  2,  5, 10, 11,  3, 13, 15, 15,\n",
      "        21, 23])\n"
     ]
    }
   ],
   "source": [
    "print(targets[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(params, \"model_1.prm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
