{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sugay\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cudaが使えるか確認\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUで作ったPickleのファイルは，CPUではそのままでは使えないため，工夫が必要．\n",
    "\n",
    "https://www.kunita-gamefactory.com/post/%E3%80%90pytorch%E3%80%91gpu%E3%81%A7%E8%A8%93%E7%B7%B4%E3%81%95%E3%81%9B%E3%81%9F%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92cpu%E3%81%A7%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%82%82%E3%81%86%E3%81%A8%E3%81%97%E3%81%9F%E3%81%8A%E8%A9%B1\n",
    "\n",
    "を真似したらうまくCPU上でもファイルを読み込むことができるようになった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "        \n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの相対パスを指定\n",
    "file_path = '../data storage/Ising_data_L16_v3.pkl'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "else:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = CPU_Unpickler(file).load()\n",
    "        \n",
    "# 読み込んだデータを個々の変数に分割\n",
    "spin_data, label_data = loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 磁化を計算\n",
    "# def magnetization(state):\n",
    "#     return np.mean(state)\n",
    "\n",
    "# for i in range(len(spin_data)):\n",
    "#     mag = magnetization(spin_data[i])\n",
    "#     if mag > 0:\n",
    "#         spin_data[i] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストをNumPy配列に変換\n",
    "spin_data_np = np.array(spin_data)\n",
    "label_data_np = np.array(label_data)\n",
    "\n",
    "# NumPy配列をPyTorchテンソルに変換\n",
    "spin_data_tensor = torch.from_numpy(spin_data_np)\n",
    "label_data_tensor = torch.from_numpy(label_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_16220\\612640267.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_16220\\612640267.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_16220\\612640267.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_16220\\612640267.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_test = torch.tensor(label_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 訓練とテストに分割(5:5)\n",
    "spin_train, spin_test, label_train, label_test = train_test_split(spin_data_tensor, label_data_tensor, test_size=0.5)\n",
    "\n",
    "# PyTorchのテンソルに変換\n",
    "spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
    "spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
    "label_train = torch.tensor(label_train, dtype=torch.float32)  \n",
    "label_test = torch.tensor(label_test, dtype=torch.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 20])\n",
      "torch.Size([20000, 20])\n"
     ]
    }
   ],
   "source": [
    "# one-hot表現に変換する関数\n",
    "def to_one_hot(data, num_classes):\n",
    "    # one-hotベクトルの初期化\n",
    "    one_hot = torch.zeros(len(data), num_classes)\n",
    "    print(one_hot.size())\n",
    "    # 各要素を20次元のone-hotベクトルに変換\n",
    "    for i, val in enumerate(data):\n",
    "        index = int(torch.round((val - 0.05) / 0.05))\n",
    "        one_hot[i, index] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "num_classes = 20\n",
    "\n",
    "# one-hot表現に変換\n",
    "one_hot_label_train = to_one_hot(label_train, num_classes=num_classes)\n",
    "one_hot_label_test = to_one_hot(label_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = TensorDataset(spin_train, one_hot_label_train)\n",
    "test_dataset = TensorDataset(spin_test, one_hot_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# データセットに前処理を適用\n",
    "transformed_train_dataset = [(transform(tensor_sample), label) for tensor_sample, label in train_dataset]\n",
    "transformed_test_dataset = [(transform(tensor_sample), label) for tensor_sample, label in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの設定\n",
    "train_loader = DataLoader(transformed_train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(transformed_test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCNN(\n",
      "  (fc1): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 16*16\n",
    "hidden_size = 3\n",
    "output_size = 20\n",
    "model = FCNN(input_size, hidden_size, output_size)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と最適化アルゴリズムの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   # クロスエントロピー誤差\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)     # Adam,L2正則化{, weight_decay=5e-4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.9931385886669157, acc: 0.05435002222657204, test loss: 2.989572557210922, test acc: 0.062450017780065536\n",
      "epoch: 1, loss: 2.983915168046951, acc: 0.09270001202821732, test loss: 2.9789605152606966, test acc: 0.09480000287294388\n",
      "epoch: 2, loss: 2.9717128801345827, acc: 0.10020004212856293, test loss: 2.9667674851417543, test acc: 0.10010001808404922\n",
      "epoch: 3, loss: 2.9623024225234986, acc: 0.10285000503063202, test loss: 2.960139808654785, test acc: 0.10005003213882446\n",
      "epoch: 4, loss: 2.957712632417679, acc: 0.10284999758005142, test loss: 2.956866105794907, test acc: 0.10005003213882446\n",
      "epoch: 5, loss: 2.9550425696372984, acc: 0.10284999012947083, test loss: 2.9547346341609955, test acc: 0.10005003213882446\n",
      "epoch: 6, loss: 2.9532451701164244, acc: 0.10285002738237381, test loss: 2.9533122742176054, test acc: 0.10005003213882446\n",
      "epoch: 7, loss: 2.9518923592567443, acc: 0.10285000503063202, test loss: 2.9522075700759887, test acc: 0.10005003213882446\n",
      "epoch: 8, loss: 2.9508158135414124, acc: 0.1028500497341156, test loss: 2.951323789358139, test acc: 0.10005003213882446\n",
      "epoch: 9, loss: 2.9499944818019865, acc: 0.10284999758005142, test loss: 2.9507110214233396, test acc: 0.10020002722740173\n",
      "epoch: 10, loss: 2.949297099113464, acc: 0.10330003499984741, test loss: 2.9502762258052826, test acc: 0.10065001249313354\n",
      "epoch: 11, loss: 2.9487040150165558, acc: 0.10405002534389496, test loss: 2.9498769307136534, test acc: 0.10140001028776169\n",
      "epoch: 12, loss: 2.9482053184509276, acc: 0.10540001094341278, test loss: 2.9495136737823486, test acc: 0.10250002890825272\n",
      "epoch: 13, loss: 2.9477210915088654, acc: 0.10830004513263702, test loss: 2.9491888988018036, test acc: 0.10345001518726349\n",
      "epoch: 14, loss: 2.9473292410373686, acc: 0.11000002175569534, test loss: 2.9490283453464508, test acc: 0.1067500188946724\n",
      "epoch: 15, loss: 2.9469379246234895, acc: 0.11110000312328339, test loss: 2.948866206407547, test acc: 0.10405004769563675\n",
      "epoch: 16, loss: 2.946506856679916, acc: 0.11445001512765884, test loss: 2.9485232973098756, test acc: 0.10460004955530167\n",
      "epoch: 17, loss: 2.9460697209835054, acc: 0.1177000030875206, test loss: 2.9484563422203065, test acc: 0.10555002093315125\n",
      "epoch: 18, loss: 2.945664200782776, acc: 0.11830002069473267, test loss: 2.9480712819099426, test acc: 0.10680001974105835\n",
      "epoch: 19, loss: 2.9451223111152647, acc: 0.12094996124505997, test loss: 2.9478840899467467, test acc: 0.10705001652240753\n",
      "epoch: 20, loss: 2.9447008681297304, acc: 0.12115000933408737, test loss: 2.9478317594528196, test acc: 0.10860003530979156\n",
      "epoch: 21, loss: 2.944216854572296, acc: 0.1221499890089035, test loss: 2.947400870323181, test acc: 0.1084500253200531\n",
      "epoch: 22, loss: 2.9435065817832946, acc: 0.12360002845525742, test loss: 2.947297929525375, test acc: 0.10960003733634949\n",
      "epoch: 23, loss: 2.9430507290363312, acc: 0.12270007282495499, test loss: 2.9469527518749237, test acc: 0.10900003463029861\n",
      "epoch: 24, loss: 2.9425117778778076, acc: 0.12290000170469284, test loss: 2.946936184167862, test acc: 0.1089000329375267\n",
      "epoch: 25, loss: 2.9421211290359497, acc: 0.1250000149011612, test loss: 2.9465439438819887, test acc: 0.10870001465082169\n",
      "epoch: 26, loss: 2.9415096819400786, acc: 0.12505002319812775, test loss: 2.9463359570503234, test acc: 0.10865003615617752\n",
      "epoch: 27, loss: 2.940890794992447, acc: 0.12600000202655792, test loss: 2.9463514065742493, test acc: 0.10935002565383911\n",
      "epoch: 28, loss: 2.9403310191631316, acc: 0.12485004216432571, test loss: 2.945932182073593, test acc: 0.10725004971027374\n",
      "epoch: 29, loss: 2.9398123621940613, acc: 0.12515000998973846, test loss: 2.9456271111965178, test acc: 0.10990005731582642\n",
      "epoch: 30, loss: 2.939165564775467, acc: 0.12544995546340942, test loss: 2.9453962445259094, test acc: 0.10564999282360077\n",
      "epoch: 31, loss: 2.9388246500492095, acc: 0.12514998018741608, test loss: 2.9451895356178284, test acc: 0.10975005477666855\n",
      "epoch: 32, loss: 2.9382555103302, acc: 0.1269499659538269, test loss: 2.94495689868927, test acc: 0.10990002751350403\n",
      "epoch: 33, loss: 2.9377054882049563, acc: 0.1264999955892563, test loss: 2.9450062584877013, test acc: 0.11065002530813217\n",
      "epoch: 34, loss: 2.9374189019203185, acc: 0.12775000929832458, test loss: 2.9448764169216157, test acc: 0.10950002819299698\n",
      "epoch: 35, loss: 2.936819912195206, acc: 0.12505000829696655, test loss: 2.9446981728076933, test acc: 0.11010005325078964\n",
      "epoch: 36, loss: 2.9364361357688904, acc: 0.1266000121831894, test loss: 2.9447044467926027, test acc: 0.11020003259181976\n",
      "epoch: 37, loss: 2.9361123526096344, acc: 0.12839995324611664, test loss: 2.944349820613861, test acc: 0.11000002175569534\n",
      "epoch: 38, loss: 2.9357556748390197, acc: 0.12764999270439148, test loss: 2.944100956916809, test acc: 0.11020004004240036\n",
      "epoch: 39, loss: 2.9353621661663056, acc: 0.1265999674797058, test loss: 2.943932603597641, test acc: 0.111300028860569\n",
      "epoch: 40, loss: 2.9349477767944334, acc: 0.12894999980926514, test loss: 2.9439499378204346, test acc: 0.10950005799531937\n",
      "epoch: 41, loss: 2.9346094298362733, acc: 0.12914994359016418, test loss: 2.9436621403694154, test acc: 0.11105002462863922\n",
      "epoch: 42, loss: 2.9344168138504028, acc: 0.12835000455379486, test loss: 2.943730914592743, test acc: 0.111300028860569\n",
      "epoch: 43, loss: 2.9340792167186738, acc: 0.12804996967315674, test loss: 2.943354893922806, test acc: 0.1116500273346901\n",
      "epoch: 44, loss: 2.9339000952243803, acc: 0.12930001318454742, test loss: 2.9433786273002625, test acc: 0.11085005849599838\n",
      "epoch: 45, loss: 2.933326381444931, acc: 0.1297999918460846, test loss: 2.9432524061203003, test acc: 0.11100003123283386\n",
      "epoch: 46, loss: 2.9330711460113523, acc: 0.13259999454021454, test loss: 2.942912170886993, test acc: 0.11155001819133759\n",
      "epoch: 47, loss: 2.9329147577285766, acc: 0.13159997761249542, test loss: 2.9428128373622893, test acc: 0.1121000200510025\n",
      "epoch: 48, loss: 2.932427259683609, acc: 0.1325499713420868, test loss: 2.942688851356506, test acc: 0.11205001920461655\n",
      "epoch: 49, loss: 2.9322124779224397, acc: 0.13249997794628143, test loss: 2.94269446849823, test acc: 0.11275002360343933\n",
      "epoch: 50, loss: 2.931994233131409, acc: 0.13239997625350952, test loss: 2.9425234925746917, test acc: 0.11320000886917114\n",
      "epoch: 51, loss: 2.931744599342346, acc: 0.1335499882698059, test loss: 2.9421830368041992, test acc: 0.11260005831718445\n",
      "epoch: 52, loss: 2.9315468096733093, acc: 0.13280005753040314, test loss: 2.9423099327087403, test acc: 0.11345001310110092\n",
      "epoch: 53, loss: 2.931226544380188, acc: 0.133899986743927, test loss: 2.9421186983585357, test acc: 0.11280001699924469\n",
      "epoch: 54, loss: 2.9307606279850007, acc: 0.13479995727539062, test loss: 2.9417615723609924, test acc: 0.11385001242160797\n",
      "epoch: 55, loss: 2.9306924712657927, acc: 0.13465000689029694, test loss: 2.9418971407413483, test acc: 0.11345004290342331\n",
      "epoch: 56, loss: 2.930631569623947, acc: 0.13589997589588165, test loss: 2.941784927845001, test acc: 0.11400000751018524\n",
      "epoch: 57, loss: 2.930114769935608, acc: 0.13644999265670776, test loss: 2.941793271303177, test acc: 0.11460001766681671\n",
      "epoch: 58, loss: 2.930404920578003, acc: 0.13654997944831848, test loss: 2.9413267838954926, test acc: 0.11425001919269562\n",
      "epoch: 59, loss: 2.930036180019379, acc: 0.1369999796152115, test loss: 2.941397057771683, test acc: 0.1153000071644783\n",
      "epoch: 60, loss: 2.929675121307373, acc: 0.1386999785900116, test loss: 2.941314117908478, test acc: 0.11545003205537796\n",
      "epoch: 61, loss: 2.929412716627121, acc: 0.13904999196529388, test loss: 2.9410319197177888, test acc: 0.1151999831199646\n",
      "epoch: 62, loss: 2.9294133985042574, acc: 0.13884997367858887, test loss: 2.9412592947483063, test acc: 0.11395002156496048\n",
      "epoch: 63, loss: 2.9290051651000977, acc: 0.1407499760389328, test loss: 2.94113743185997, test acc: 0.11509998142719269\n",
      "epoch: 64, loss: 2.9290191912651062, acc: 0.1399499773979187, test loss: 2.941005687713623, test acc: 0.11579994112253189\n",
      "epoch: 65, loss: 2.9287710189819336, acc: 0.13954998552799225, test loss: 2.9409376537799834, test acc: 0.116100013256073\n",
      "epoch: 66, loss: 2.928378527164459, acc: 0.13985000550746918, test loss: 2.940833331346512, test acc: 0.1159999817609787\n",
      "epoch: 67, loss: 2.9284159088134767, acc: 0.14174997806549072, test loss: 2.9407322108745575, test acc: 0.11720000207424164\n",
      "epoch: 68, loss: 2.928183853626251, acc: 0.14094997942447662, test loss: 2.9401839816570283, test acc: 0.11725002527236938\n",
      "epoch: 69, loss: 2.9280895519256593, acc: 0.14149998128414154, test loss: 2.9404674160480497, test acc: 0.11620000004768372\n",
      "epoch: 70, loss: 2.9279833567142486, acc: 0.14149999618530273, test loss: 2.9404324746131896, test acc: 0.11595003306865692\n",
      "epoch: 71, loss: 2.9274618327617645, acc: 0.14434999227523804, test loss: 2.9405218148231507, test acc: 0.11680004000663757\n",
      "epoch: 72, loss: 2.927673395872116, acc: 0.14184993505477905, test loss: 2.9402726817131044, test acc: 0.1157500147819519\n",
      "epoch: 73, loss: 2.9274820113182067, acc: 0.14430001378059387, test loss: 2.93958601474762, test acc: 0.11770003288984299\n",
      "epoch: 74, loss: 2.9272712326049803, acc: 0.14384999871253967, test loss: 2.9401726591587067, test acc: 0.11575005203485489\n",
      "epoch: 75, loss: 2.9273747849464415, acc: 0.14319996535778046, test loss: 2.9396308040618897, test acc: 0.11670004576444626\n",
      "epoch: 76, loss: 2.926885714530945, acc: 0.1436000019311905, test loss: 2.939905183315277, test acc: 0.11750002950429916\n",
      "epoch: 77, loss: 2.926853152513504, acc: 0.14314991235733032, test loss: 2.9396353375911715, test acc: 0.1169000118970871\n",
      "epoch: 78, loss: 2.9266278958320617, acc: 0.1435999721288681, test loss: 2.939796727895737, test acc: 0.11720002442598343\n",
      "epoch: 79, loss: 2.926318464279175, acc: 0.1447499692440033, test loss: 2.93935861825943, test acc: 0.11699999868869781\n",
      "epoch: 80, loss: 2.926170712709427, acc: 0.1442999690771103, test loss: 2.939214150905609, test acc: 0.1184999942779541\n",
      "epoch: 81, loss: 2.9257710444927216, acc: 0.1455499529838562, test loss: 2.9392619478702544, test acc: 0.11805003136396408\n",
      "epoch: 82, loss: 2.9257229781150818, acc: 0.14494995772838593, test loss: 2.9393938851356505, test acc: 0.11675003170967102\n",
      "epoch: 83, loss: 2.925428103208542, acc: 0.14494997262954712, test loss: 2.9393178534507753, test acc: 0.11840000003576279\n",
      "epoch: 84, loss: 2.925496574640274, acc: 0.14514996111392975, test loss: 2.9391745734214783, test acc: 0.11765003204345703\n",
      "epoch: 85, loss: 2.9254310488700868, acc: 0.14659994840621948, test loss: 2.9386508083343506, test acc: 0.11865003407001495\n",
      "epoch: 86, loss: 2.9251328325271606, acc: 0.14629997313022614, test loss: 2.9388333785533907, test acc: 0.1183500662446022\n",
      "epoch: 87, loss: 2.9248568880558015, acc: 0.14549998939037323, test loss: 2.939091799259186, test acc: 0.1170000284910202\n",
      "epoch: 88, loss: 2.9246052503585815, acc: 0.14629994332790375, test loss: 2.9389337384700776, test acc: 0.11834998428821564\n",
      "epoch: 89, loss: 2.92474134683609, acc: 0.14494997262954712, test loss: 2.939041472673416, test acc: 0.11764998733997345\n",
      "epoch: 90, loss: 2.924796209335327, acc: 0.1459999829530716, test loss: 2.9390739011764526, test acc: 0.11899996548891068\n",
      "epoch: 91, loss: 2.9245299887657166, acc: 0.1464499533176422, test loss: 2.938967914581299, test acc: 0.1180499941110611\n",
      "epoch: 92, loss: 2.924394438266754, acc: 0.1466999500989914, test loss: 2.9387514793872835, test acc: 0.1181500181555748\n",
      "epoch: 93, loss: 2.9242627882957457, acc: 0.14555004239082336, test loss: 2.9386269199848174, test acc: 0.11785002052783966\n",
      "epoch: 94, loss: 2.9237079071998595, acc: 0.14650005102157593, test loss: 2.938364768028259, test acc: 0.11915002763271332\n",
      "epoch: 95, loss: 2.92364852309227, acc: 0.14695000648498535, test loss: 2.939099760055542, test acc: 0.11784996092319489\n",
      "epoch: 96, loss: 2.9238587296009064, acc: 0.14604999125003815, test loss: 2.9387308239936827, test acc: 0.1186000183224678\n",
      "epoch: 97, loss: 2.923805787563324, acc: 0.14795000851154327, test loss: 2.9386251020431517, test acc: 0.11820001900196075\n",
      "epoch: 98, loss: 2.923554856777191, acc: 0.14839999377727509, test loss: 2.9391622626781464, test acc: 0.11739999800920486\n",
      "epoch: 99, loss: 2.922981741428375, acc: 0.1471499502658844, test loss: 2.9389776468276976, test acc: 0.11834999918937683\n",
      "epoch: 100, loss: 2.9231641471385954, acc: 0.14729996025562286, test loss: 2.9383208465576174, test acc: 0.11800000071525574\n",
      "epoch: 101, loss: 2.92287363409996, acc: 0.1475999802350998, test loss: 2.938481042385101, test acc: 0.11839998513460159\n",
      "epoch: 102, loss: 2.9222933638095854, acc: 0.1472499817609787, test loss: 2.9383647334575653, test acc: 0.11930003762245178\n",
      "epoch: 103, loss: 2.9222927391529083, acc: 0.14880001544952393, test loss: 2.9385524582862854, test acc: 0.11849996447563171\n",
      "epoch: 104, loss: 2.9223398303985597, acc: 0.1520499587059021, test loss: 2.9381104362010957, test acc: 0.1185000017285347\n",
      "epoch: 105, loss: 2.922503160238266, acc: 0.14694996178150177, test loss: 2.938351265192032, test acc: 0.11764994263648987\n",
      "epoch: 106, loss: 2.922088657617569, acc: 0.14629997313022614, test loss: 2.938128831386566, test acc: 0.11944995820522308\n",
      "epoch: 107, loss: 2.922035073041916, acc: 0.1491999328136444, test loss: 2.9382606875896453, test acc: 0.11944998055696487\n",
      "epoch: 108, loss: 2.9217625856399536, acc: 0.1476999670267105, test loss: 2.9389568769931795, test acc: 0.11754997074604034\n",
      "epoch: 109, loss: 2.9219706332683564, acc: 0.14720001816749573, test loss: 2.9379381012916563, test acc: 0.12034999579191208\n",
      "epoch: 110, loss: 2.9217392194271086, acc: 0.14934997260570526, test loss: 2.938104662895203, test acc: 0.11910000443458557\n",
      "epoch: 111, loss: 2.921076616048813, acc: 0.14875000715255737, test loss: 2.9381446039676664, test acc: 0.11889998614788055\n",
      "epoch: 112, loss: 2.921322230100632, acc: 0.1500999629497528, test loss: 2.9382974636554717, test acc: 0.13304997980594635\n",
      "epoch: 113, loss: 2.9211339211463927, acc: 0.14994996786117554, test loss: 2.937902858257294, test acc: 0.1287499964237213\n",
      "epoch: 114, loss: 2.9208749103546143, acc: 0.15035001933574677, test loss: 2.9378480279445647, test acc: 0.13274997472763062\n",
      "epoch: 115, loss: 2.9209401190280913, acc: 0.14969997107982635, test loss: 2.938470678329468, test acc: 0.11874999850988388\n",
      "epoch: 116, loss: 2.9210828459262848, acc: 0.150799959897995, test loss: 2.9383168518543243, test acc: 0.1188499927520752\n",
      "epoch: 117, loss: 2.920618280172348, acc: 0.14919999241828918, test loss: 2.937988580465317, test acc: 0.119549959897995\n",
      "epoch: 118, loss: 2.9203284418582918, acc: 0.15049999952316284, test loss: 2.9384165608882906, test acc: 0.127299964427948\n",
      "epoch: 119, loss: 2.920168024301529, acc: 0.15125000476837158, test loss: 2.938436607122421, test acc: 0.12499995529651642\n",
      "epoch: 120, loss: 2.920177696943283, acc: 0.1510000079870224, test loss: 2.9379520988464356, test acc: 0.13409997522830963\n",
      "epoch: 121, loss: 2.9201622021198275, acc: 0.15149997174739838, test loss: 2.9377985167503358, test acc: 0.12749998271465302\n",
      "epoch: 122, loss: 2.9199632596969605, acc: 0.15234994888305664, test loss: 2.9380821537971498, test acc: 0.12404996901750565\n",
      "epoch: 123, loss: 2.9196396291255953, acc: 0.15004998445510864, test loss: 2.9381281423568724, test acc: 0.11755000054836273\n",
      "epoch: 124, loss: 2.919593433141708, acc: 0.149399995803833, test loss: 2.9375447034835815, test acc: 0.13245001435279846\n",
      "epoch: 125, loss: 2.91950110912323, acc: 0.1515999734401703, test loss: 2.938008803129196, test acc: 0.13399997353553772\n",
      "epoch: 126, loss: 2.919095846414566, acc: 0.15259996056556702, test loss: 2.9380571711063386, test acc: 0.12519995868206024\n",
      "epoch: 127, loss: 2.9187733614444733, acc: 0.1528000384569168, test loss: 2.9380374121665955, test acc: 0.12610001862049103\n",
      "epoch: 128, loss: 2.9193195414543154, acc: 0.15384997427463531, test loss: 2.938460203409195, test acc: 0.12799997627735138\n",
      "epoch: 129, loss: 2.9188791024684906, acc: 0.14854998886585236, test loss: 2.9382093071937563, test acc: 0.12065000832080841\n",
      "epoch: 130, loss: 2.9186446380615236, acc: 0.15240003168582916, test loss: 2.938221365213394, test acc: 0.1258000284433365\n",
      "epoch: 131, loss: 2.9188176882267, acc: 0.15564994513988495, test loss: 2.9374838411808013, test acc: 0.12714996933937073\n",
      "epoch: 132, loss: 2.918883690834045, acc: 0.15039995312690735, test loss: 2.9379116821289064, test acc: 0.1170000284910202\n",
      "epoch: 133, loss: 2.9186451232433317, acc: 0.14984996616840363, test loss: 2.9382610630989077, test acc: 0.12240000814199448\n",
      "epoch: 134, loss: 2.9184493708610533, acc: 0.1538499891757965, test loss: 2.9380163812637328, test acc: 0.1248999685049057\n",
      "epoch: 135, loss: 2.918693516254425, acc: 0.15039996802806854, test loss: 2.937851252555847, test acc: 0.12469995766878128\n",
      "epoch: 136, loss: 2.9185808801651003, acc: 0.1516999453306198, test loss: 2.937148438692093, test acc: 0.1244499459862709\n",
      "epoch: 137, loss: 2.9181511068344115, acc: 0.1502498984336853, test loss: 2.937372678518295, test acc: 0.12644998729228973\n",
      "epoch: 138, loss: 2.918116863965988, acc: 0.15004996955394745, test loss: 2.937658795118332, test acc: 0.12304993718862534\n",
      "epoch: 139, loss: 2.917831197977066, acc: 0.15104997158050537, test loss: 2.937621349096298, test acc: 0.12669995427131653\n",
      "epoch: 140, loss: 2.9174457943439482, acc: 0.15514999628067017, test loss: 2.938565763235092, test acc: 0.12379994988441467\n",
      "epoch: 141, loss: 2.9178941690921785, acc: 0.1516999751329422, test loss: 2.938045778274536, test acc: 0.12364998459815979\n",
      "epoch: 142, loss: 2.9179365277290343, acc: 0.1512499898672104, test loss: 2.9376004254817962, test acc: 0.12230003625154495\n",
      "epoch: 143, loss: 2.9176776897907257, acc: 0.1516999900341034, test loss: 2.937185809612274, test acc: 0.12509998679161072\n",
      "epoch: 144, loss: 2.9178615486621857, acc: 0.1504499912261963, test loss: 2.9372823143005373, test acc: 0.12514998018741608\n",
      "epoch: 145, loss: 2.9171666288375855, acc: 0.15354996919631958, test loss: 2.9371679759025575, test acc: 0.1288999766111374\n",
      "epoch: 146, loss: 2.917312388420105, acc: 0.15344999730587006, test loss: 2.9375232708454133, test acc: 0.12589997053146362\n",
      "epoch: 147, loss: 2.9171613526344298, acc: 0.1536499410867691, test loss: 2.9379105949401856, test acc: 0.1296999752521515\n",
      "epoch: 148, loss: 2.917240648269653, acc: 0.15194998681545258, test loss: 2.9373764312267303, test acc: 0.12564998865127563\n",
      "epoch: 149, loss: 2.9167244017124174, acc: 0.15169991552829742, test loss: 2.9378666138648986, test acc: 0.12324996292591095\n",
      "epoch: 150, loss: 2.9169311726093294, acc: 0.150799959897995, test loss: 2.9375917410850523, test acc: 0.1237500011920929\n",
      "epoch: 151, loss: 2.916651225090027, acc: 0.1520499587059021, test loss: 2.9373460209369657, test acc: 0.12189997732639313\n",
      "epoch: 152, loss: 2.916771355867386, acc: 0.15279990434646606, test loss: 2.9376626026630404, test acc: 0.12314996868371964\n",
      "epoch: 153, loss: 2.91669713139534, acc: 0.15285000205039978, test loss: 2.937849735021591, test acc: 0.1306999921798706\n",
      "epoch: 154, loss: 2.916335868835449, acc: 0.15560002624988556, test loss: 2.9378455197811126, test acc: 0.12634998559951782\n",
      "epoch: 155, loss: 2.916464079618454, acc: 0.15564994513988495, test loss: 2.93761136174202, test acc: 0.12334997951984406\n",
      "epoch: 156, loss: 2.9164282143116, acc: 0.1529500037431717, test loss: 2.937975209951401, test acc: 0.1212499588727951\n",
      "epoch: 157, loss: 2.916704553365707, acc: 0.15209992229938507, test loss: 2.937912873029709, test acc: 0.1223999410867691\n",
      "epoch: 158, loss: 2.9162943518161772, acc: 0.1509999781847, test loss: 2.937677961587906, test acc: 0.12330000102519989\n",
      "epoch: 159, loss: 2.9167152571678163, acc: 0.1508999466896057, test loss: 2.9376333284378053, test acc: 0.12444999068975449\n",
      "epoch: 160, loss: 2.916550889015198, acc: 0.15229995548725128, test loss: 2.9373517060279846, test acc: 0.12354997545480728\n",
      "epoch: 161, loss: 2.9163805484771728, acc: 0.153999924659729, test loss: 2.9380129301548004, test acc: 0.12269994616508484\n",
      "epoch: 162, loss: 2.915907768011093, acc: 0.1537499874830246, test loss: 2.9372026598453522, test acc: 0.12385000288486481\n",
      "epoch: 163, loss: 2.9157016599178314, acc: 0.14959998428821564, test loss: 2.937138241529465, test acc: 0.12474995851516724\n",
      "epoch: 164, loss: 2.9156587886810303, acc: 0.15164996683597565, test loss: 2.9376938462257387, test acc: 0.12309998273849487\n",
      "epoch: 165, loss: 2.9157261455059054, acc: 0.15289995074272156, test loss: 2.9375131130218506, test acc: 0.12259993702173233\n",
      "epoch: 166, loss: 2.9160237956047057, acc: 0.1525498926639557, test loss: 2.9375294137001036, test acc: 0.12244997918605804\n",
      "epoch: 167, loss: 2.9158205938339234, acc: 0.15264999866485596, test loss: 2.937709481716156, test acc: 0.1219499334692955\n",
      "epoch: 168, loss: 2.915392678976059, acc: 0.15514995157718658, test loss: 2.9378056383132933, test acc: 0.1284000277519226\n",
      "epoch: 169, loss: 2.915756120681763, acc: 0.1530500203371048, test loss: 2.937356781959534, test acc: 0.12209995090961456\n",
      "epoch: 170, loss: 2.9158527243137358, acc: 0.15624995529651642, test loss: 2.9372768890857697, test acc: 0.12279994040727615\n",
      "epoch: 171, loss: 2.915296003818512, acc: 0.15525001287460327, test loss: 2.937432643175125, test acc: 0.1273999810218811\n",
      "epoch: 172, loss: 2.9149991714954377, acc: 0.15219993889331818, test loss: 2.937490175962448, test acc: 0.12124998867511749\n",
      "epoch: 173, loss: 2.915160539150238, acc: 0.1524999439716339, test loss: 2.9375579619407652, test acc: 0.12384997308254242\n",
      "epoch: 174, loss: 2.9149416494369507, acc: 0.15394997596740723, test loss: 2.9374301612377165, test acc: 0.13019999861717224\n",
      "epoch: 175, loss: 2.915065771341324, acc: 0.15414996445178986, test loss: 2.936992460489273, test acc: 0.12334997206926346\n",
      "epoch: 176, loss: 2.9147650814056396, acc: 0.15449997782707214, test loss: 2.937457436323166, test acc: 0.12689998745918274\n",
      "epoch: 177, loss: 2.9148582720756533, acc: 0.15369996428489685, test loss: 2.9372502958774565, test acc: 0.12144998461008072\n",
      "epoch: 178, loss: 2.9147419321537016, acc: 0.15150000154972076, test loss: 2.936877533197403, test acc: 0.12304992973804474\n",
      "epoch: 179, loss: 2.9148927009105683, acc: 0.1526000052690506, test loss: 2.936804939508438, test acc: 0.12444999814033508\n",
      "epoch: 180, loss: 2.914904556274414, acc: 0.1536499559879303, test loss: 2.936984466314316, test acc: 0.12564998865127563\n",
      "epoch: 181, loss: 2.9149174904823303, acc: 0.15254999697208405, test loss: 2.9373330676555636, test acc: 0.1253499835729599\n",
      "epoch: 182, loss: 2.9149302005767823, acc: 0.15294994413852692, test loss: 2.937128268480301, test acc: 0.12269996851682663\n",
      "epoch: 183, loss: 2.914419032335281, acc: 0.1531999558210373, test loss: 2.9371111154556275, test acc: 0.12244997918605804\n",
      "epoch: 184, loss: 2.914041860103607, acc: 0.15494994819164276, test loss: 2.93702069401741, test acc: 0.12645000219345093\n",
      "epoch: 185, loss: 2.9144728767871855, acc: 0.15424995124340057, test loss: 2.9371307933330537, test acc: 0.12174997478723526\n",
      "epoch: 186, loss: 2.914349637031555, acc: 0.15519997477531433, test loss: 2.93733673453331, test acc: 0.12149995565414429\n",
      "epoch: 187, loss: 2.9142544543743134, acc: 0.15314997732639313, test loss: 2.9370174086093903, test acc: 0.1269499957561493\n",
      "epoch: 188, loss: 2.91438783288002, acc: 0.15599995851516724, test loss: 2.9379522085189818, test acc: 0.12069998681545258\n",
      "epoch: 189, loss: 2.914197827577591, acc: 0.15359996259212494, test loss: 2.937260514497757, test acc: 0.12009995430707932\n",
      "epoch: 190, loss: 2.913687928915024, acc: 0.15584994852542877, test loss: 2.937293664216995, test acc: 0.12149994820356369\n",
      "epoch: 191, loss: 2.9143733191490173, acc: 0.15194997191429138, test loss: 2.937062215805054, test acc: 0.12244998663663864\n",
      "epoch: 192, loss: 2.914363989830017, acc: 0.15309996902942657, test loss: 2.938030105829239, test acc: 0.11445000767707825\n",
      "epoch: 193, loss: 2.914352513551712, acc: 0.15349997580051422, test loss: 2.9368417489528658, test acc: 0.12564997375011444\n",
      "epoch: 194, loss: 2.9142845237255095, acc: 0.15229997038841248, test loss: 2.937397849559784, test acc: 0.12199994921684265\n",
      "epoch: 195, loss: 2.914223381280899, acc: 0.1524999886751175, test loss: 2.937486003637314, test acc: 0.12349997460842133\n",
      "epoch: 196, loss: 2.914026600122452, acc: 0.15379999577999115, test loss: 2.9368527603149412, test acc: 0.12359993904829025\n",
      "epoch: 197, loss: 2.9134516537189485, acc: 0.15699994564056396, test loss: 2.936802760362625, test acc: 0.12314996868371964\n",
      "epoch: 198, loss: 2.9133615827560426, acc: 0.15464995801448822, test loss: 2.937358980178833, test acc: 0.12619999051094055\n",
      "epoch: 199, loss: 2.9137683153152465, acc: 0.15195000171661377, test loss: 2.937236374616623, test acc: 0.13114996254444122\n",
      "epoch: 200, loss: 2.9136914455890657, acc: 0.15379998087882996, test loss: 2.9368995332717898, test acc: 0.1285499930381775\n",
      "epoch: 201, loss: 2.9137262010574343, acc: 0.15234993398189545, test loss: 2.937205160856247, test acc: 0.12229997664690018\n",
      "epoch: 202, loss: 2.9138592398166656, acc: 0.1548999547958374, test loss: 2.9373958563804625, test acc: 0.12104994803667068\n",
      "epoch: 203, loss: 2.9138623225688933, acc: 0.15309995412826538, test loss: 2.9370053601264954, test acc: 0.12419997900724411\n",
      "epoch: 204, loss: 2.913690195083618, acc: 0.15560001134872437, test loss: 2.936880261898041, test acc: 0.12209993600845337\n",
      "epoch: 205, loss: 2.913639211654663, acc: 0.1540999561548233, test loss: 2.936888588666916, test acc: 0.1261499673128128\n",
      "epoch: 206, loss: 2.913140331506729, acc: 0.1566999852657318, test loss: 2.9366982543468474, test acc: 0.1223999559879303\n",
      "epoch: 207, loss: 2.913239094018936, acc: 0.15669995546340942, test loss: 2.936694885492325, test acc: 0.12344996631145477\n",
      "epoch: 208, loss: 2.91323308467865, acc: 0.1527000069618225, test loss: 2.936459058523178, test acc: 0.1256999522447586\n",
      "epoch: 209, loss: 2.91320023059845, acc: 0.15404993295669556, test loss: 2.936970669031143, test acc: 0.12219995260238647\n",
      "epoch: 210, loss: 2.9137548077106477, acc: 0.156700000166893, test loss: 2.936890914440155, test acc: 0.12724997103214264\n",
      "epoch: 211, loss: 2.9134378778934478, acc: 0.15690000355243683, test loss: 2.936884250640869, test acc: 0.1239999458193779\n",
      "epoch: 212, loss: 2.9129331624507904, acc: 0.1561499834060669, test loss: 2.9370833480358125, test acc: 0.12859995663166046\n",
      "epoch: 213, loss: 2.9130129086971284, acc: 0.1552499532699585, test loss: 2.936984875202179, test acc: 0.12349996715784073\n",
      "epoch: 214, loss: 2.913256641626358, acc: 0.15574997663497925, test loss: 2.936769732236862, test acc: 0.12604999542236328\n",
      "epoch: 215, loss: 2.9128279650211333, acc: 0.15494999289512634, test loss: 2.936995759010315, test acc: 0.12539999186992645\n",
      "epoch: 216, loss: 2.9125763177871704, acc: 0.15444990992546082, test loss: 2.9372147631645205, test acc: 0.12499996274709702\n",
      "epoch: 217, loss: 2.9132030975818632, acc: 0.15644998848438263, test loss: 2.936976829767227, test acc: 0.12469997256994247\n",
      "epoch: 218, loss: 2.9127108585834502, acc: 0.1565999537706375, test loss: 2.937522670030594, test acc: 0.13095001876354218\n",
      "epoch: 219, loss: 2.912610859870911, acc: 0.15674999356269836, test loss: 2.9368952465057374, test acc: 0.12344997376203537\n",
      "epoch: 220, loss: 2.9128065955638887, acc: 0.15554998815059662, test loss: 2.9365867364406584, test acc: 0.12094997614622116\n",
      "epoch: 221, loss: 2.912968006134033, acc: 0.15754997730255127, test loss: 2.93694987654686, test acc: 0.13509997725486755\n",
      "epoch: 222, loss: 2.912922637462616, acc: 0.15494993329048157, test loss: 2.9367551016807556, test acc: 0.12620003521442413\n",
      "epoch: 223, loss: 2.912706241607666, acc: 0.15459996461868286, test loss: 2.936460987329483, test acc: 0.12479998916387558\n",
      "epoch: 224, loss: 2.912832843065262, acc: 0.15509995818138123, test loss: 2.9364849376678466, test acc: 0.12654997408390045\n",
      "epoch: 225, loss: 2.9130670738220217, acc: 0.15554995834827423, test loss: 2.936469291448593, test acc: 0.12249994277954102\n",
      "epoch: 226, loss: 2.912887282371521, acc: 0.15325000882148743, test loss: 2.9367902755737303, test acc: 0.12090002745389938\n",
      "epoch: 227, loss: 2.91238023519516, acc: 0.15559996664524078, test loss: 2.936617648601532, test acc: 0.12390000373125076\n",
      "epoch: 228, loss: 2.9127039074897767, acc: 0.15324996411800385, test loss: 2.9365851533412934, test acc: 0.12200004607439041\n",
      "epoch: 229, loss: 2.912463368177414, acc: 0.15709996223449707, test loss: 2.936375640630722, test acc: 0.12689995765686035\n",
      "epoch: 230, loss: 2.9121172821521757, acc: 0.15519998967647552, test loss: 2.9357233262062072, test acc: 0.1261499673128128\n",
      "epoch: 231, loss: 2.912444155216217, acc: 0.15525004267692566, test loss: 2.936634443998337, test acc: 0.12665000557899475\n",
      "epoch: 232, loss: 2.9127720975875855, acc: 0.15419994294643402, test loss: 2.9363394010066988, test acc: 0.12479997426271439\n",
      "epoch: 233, loss: 2.912325143814087, acc: 0.15539999306201935, test loss: 2.936357946395874, test acc: 0.12309997528791428\n",
      "epoch: 234, loss: 2.9124508225917816, acc: 0.15539993345737457, test loss: 2.936773054599762, test acc: 0.12035002559423447\n",
      "epoch: 235, loss: 2.912474058866501, acc: 0.1541999876499176, test loss: 2.936130405664444, test acc: 0.12234997004270554\n",
      "epoch: 236, loss: 2.912268340587616, acc: 0.15584999322891235, test loss: 2.936756041049957, test acc: 0.12599997222423553\n",
      "epoch: 237, loss: 2.912061496973038, acc: 0.15679994225502014, test loss: 2.9366166591644287, test acc: 0.12780000269412994\n",
      "epoch: 238, loss: 2.912140337228775, acc: 0.15644997358322144, test loss: 2.9363863587379457, test acc: 0.12924997508525848\n",
      "epoch: 239, loss: 2.911954675912857, acc: 0.15664999186992645, test loss: 2.9365717720985414, test acc: 0.1229500025510788\n",
      "epoch: 240, loss: 2.9123021137714384, acc: 0.15649999678134918, test loss: 2.9369927740097044, test acc: 0.12259996682405472\n",
      "epoch: 241, loss: 2.9122916507720946, acc: 0.15659993886947632, test loss: 2.9367302119731904, test acc: 0.12389996647834778\n",
      "epoch: 242, loss: 2.9124807953834533, acc: 0.15449999272823334, test loss: 2.9360118758678437, test acc: 0.12664997577667236\n",
      "epoch: 243, loss: 2.912078447341919, acc: 0.15404999256134033, test loss: 2.936253675222397, test acc: 0.12344997376203537\n",
      "epoch: 244, loss: 2.911660248041153, acc: 0.15995000302791595, test loss: 2.9364029848575592, test acc: 0.12379994243383408\n",
      "epoch: 245, loss: 2.9121374452114104, acc: 0.15484999120235443, test loss: 2.9361571061611174, test acc: 0.12709994614124298\n",
      "epoch: 246, loss: 2.911677268743515, acc: 0.15584999322891235, test loss: 2.9360164070129393, test acc: 0.12589997053146362\n",
      "epoch: 247, loss: 2.911615422964096, acc: 0.1553499698638916, test loss: 2.9356625080108643, test acc: 0.12359999865293503\n",
      "epoch: 248, loss: 2.9116536951065064, acc: 0.15674997866153717, test loss: 2.9356212854385375, test acc: 0.12734997272491455\n",
      "epoch: 249, loss: 2.911659051179886, acc: 0.15509995818138123, test loss: 2.9360312783718108, test acc: 0.12129992246627808\n",
      "epoch: 250, loss: 2.9121301221847533, acc: 0.15484993159770966, test loss: 2.9358104526996613, test acc: 0.12450002878904343\n",
      "epoch: 251, loss: 2.9117464280128478, acc: 0.1554500311613083, test loss: 2.93609810590744, test acc: 0.1260499805212021\n",
      "epoch: 252, loss: 2.9116607165336608, acc: 0.15564997494220734, test loss: 2.9359244120121004, test acc: 0.1318499743938446\n",
      "epoch: 253, loss: 2.9119972372055054, acc: 0.15549999475479126, test loss: 2.935462939739227, test acc: 0.12664994597434998\n",
      "epoch: 254, loss: 2.911336201429367, acc: 0.1563999503850937, test loss: 2.9357207298278807, test acc: 0.12429994344711304\n",
      "epoch: 255, loss: 2.9111983728408815, acc: 0.15324997901916504, test loss: 2.934922137260437, test acc: 0.12469996511936188\n",
      "epoch: 256, loss: 2.9110808324813844, acc: 0.15449994802474976, test loss: 2.9349325633049013, test acc: 0.12464997172355652\n",
      "epoch: 257, loss: 2.9114757597446443, acc: 0.1563999205827713, test loss: 2.935711792707443, test acc: 0.13124997913837433\n",
      "epoch: 258, loss: 2.91128648519516, acc: 0.15669997036457062, test loss: 2.9350655734539033, test acc: 0.12239997833967209\n",
      "epoch: 259, loss: 2.9115834867954256, acc: 0.1561499536037445, test loss: 2.934301573038101, test acc: 0.12414997071027756\n",
      "epoch: 260, loss: 2.910870451927185, acc: 0.15814998745918274, test loss: 2.934441133737564, test acc: 0.1253499686717987\n",
      "epoch: 261, loss: 2.910624083280563, acc: 0.15769997239112854, test loss: 2.934620666503906, test acc: 0.12509998679161072\n",
      "epoch: 262, loss: 2.9112971735000612, acc: 0.1585499346256256, test loss: 2.934003814458847, test acc: 0.126399964094162\n",
      "epoch: 263, loss: 2.910467962026596, acc: 0.16064995527267456, test loss: 2.9338239812850953, test acc: 0.12779997289180756\n",
      "epoch: 264, loss: 2.910234783887863, acc: 0.15849997103214264, test loss: 2.9333429491519927, test acc: 0.12874995172023773\n",
      "epoch: 265, loss: 2.9103430736064912, acc: 0.16049998998641968, test loss: 2.9337725841999056, test acc: 0.12915000319480896\n",
      "epoch: 266, loss: 2.9098260617256164, acc: 0.1625499576330185, test loss: 2.933797994852066, test acc: 0.12799997627735138\n",
      "epoch: 267, loss: 2.9107395076751708, acc: 0.15924997627735138, test loss: 2.9337514209747315, test acc: 0.1287499964237213\n",
      "epoch: 268, loss: 2.910222989320755, acc: 0.16009996831417084, test loss: 2.9334081315994265, test acc: 0.12779994308948517\n",
      "epoch: 269, loss: 2.9098938834667205, acc: 0.16084998846054077, test loss: 2.933603539466858, test acc: 0.12694993615150452\n",
      "epoch: 270, loss: 2.910139185190201, acc: 0.1611500233411789, test loss: 2.9342186295986177, test acc: 0.13155001401901245\n",
      "epoch: 271, loss: 2.9105349349975587, acc: 0.16134996712207794, test loss: 2.933292019367218, test acc: 0.13054996728897095\n",
      "epoch: 272, loss: 2.909796423912048, acc: 0.16194993257522583, test loss: 2.9334998691082, test acc: 0.12974999845027924\n",
      "epoch: 273, loss: 2.90982573390007, acc: 0.16079993546009064, test loss: 2.932176567316055, test acc: 0.13234995305538177\n",
      "epoch: 274, loss: 2.9098257327079775, acc: 0.16249991953372955, test loss: 2.9328546786308287, test acc: 0.13274997472763062\n",
      "epoch: 275, loss: 2.9097473442554476, acc: 0.16099996864795685, test loss: 2.9326341688632964, test acc: 0.13255000114440918\n",
      "epoch: 276, loss: 2.9095822656154633, acc: 0.1622999608516693, test loss: 2.933202801942825, test acc: 0.12714999914169312\n",
      "epoch: 277, loss: 2.909950422048569, acc: 0.16124998033046722, test loss: 2.9331013798713683, test acc: 0.1290999799966812\n",
      "epoch: 278, loss: 2.909208734035492, acc: 0.1629999727010727, test loss: 2.932836421728134, test acc: 0.13269998133182526\n",
      "epoch: 279, loss: 2.90949658870697, acc: 0.16099996864795685, test loss: 2.931374977827072, test acc: 0.1342500001192093\n",
      "epoch: 280, loss: 2.9099253475666047, acc: 0.16029992699623108, test loss: 2.931635845899582, test acc: 0.13214999437332153\n",
      "epoch: 281, loss: 2.909460513591766, acc: 0.16179992258548737, test loss: 2.9317412889003753, test acc: 0.13280001282691956\n",
      "epoch: 282, loss: 2.909343626499176, acc: 0.16185002028942108, test loss: 2.931356198787689, test acc: 0.13009996712207794\n",
      "epoch: 283, loss: 2.909470295906067, acc: 0.1614999771118164, test loss: 2.931763463020325, test acc: 0.13019998371601105\n",
      "epoch: 284, loss: 2.908636734485626, acc: 0.16364994645118713, test loss: 2.931125713586807, test acc: 0.13220000267028809\n",
      "epoch: 285, loss: 2.9084674179553986, acc: 0.16204994916915894, test loss: 2.931503436565399, test acc: 0.1309499591588974\n",
      "epoch: 286, loss: 2.908939036130905, acc: 0.16289998590946198, test loss: 2.931031439304352, test acc: 0.1324499398469925\n",
      "epoch: 287, loss: 2.9085367810726166, acc: 0.16514991223812103, test loss: 2.929836257696152, test acc: 0.13809999823570251\n",
      "epoch: 288, loss: 2.908325483798981, acc: 0.16534997522830963, test loss: 2.9306805396080016, test acc: 0.134599968791008\n",
      "epoch: 289, loss: 2.907685537338257, acc: 0.1673499494791031, test loss: 2.930229332447052, test acc: 0.13454999029636383\n",
      "epoch: 290, loss: 2.9091845393180846, acc: 0.16204994916915894, test loss: 2.9291895020008085, test acc: 0.13934996724128723\n",
      "epoch: 291, loss: 2.9084046518802644, acc: 0.16354991495609283, test loss: 2.930033653974533, test acc: 0.13609996438026428\n",
      "epoch: 292, loss: 2.908230335712433, acc: 0.16520000994205475, test loss: 2.9298763310909273, test acc: 0.1357499659061432\n",
      "epoch: 293, loss: 2.9077729558944703, acc: 0.1664000153541565, test loss: 2.9295211625099182, test acc: 0.13615001738071442\n",
      "epoch: 294, loss: 2.907789853811264, acc: 0.16614991426467896, test loss: 2.9295682954788207, test acc: 0.13569997251033783\n",
      "epoch: 295, loss: 2.907761173248291, acc: 0.16819997131824493, test loss: 2.9294394850730896, test acc: 0.13705000281333923\n",
      "epoch: 296, loss: 2.9072371530532837, acc: 0.16634994745254517, test loss: 2.9303433465957642, test acc: 0.133449986577034\n",
      "epoch: 297, loss: 2.907069593667984, acc: 0.16614991426467896, test loss: 2.928457328081131, test acc: 0.13889998197555542\n",
      "epoch: 298, loss: 2.906679118871689, acc: 0.16749992966651917, test loss: 2.928344542980194, test acc: 0.13854999840259552\n",
      "epoch: 299, loss: 2.9071248674392702, acc: 0.16549994051456451, test loss: 2.9271350681781767, test acc: 0.14280001819133759\n",
      "epoch: 300, loss: 2.9069158470630647, acc: 0.16694995760917664, test loss: 2.9277119767665862, test acc: 0.139399915933609\n",
      "epoch: 301, loss: 2.90675173163414, acc: 0.16684995591640472, test loss: 2.9280681049823762, test acc: 0.13714993000030518\n",
      "epoch: 302, loss: 2.906282277107239, acc: 0.16884995996952057, test loss: 2.925850443840027, test acc: 0.14444993436336517\n",
      "epoch: 303, loss: 2.9070699286460875, acc: 0.168349951505661, test loss: 2.9253545200824735, test acc: 0.14539995789527893\n",
      "epoch: 304, loss: 2.905711243152618, acc: 0.17229992151260376, test loss: 2.924931842088699, test acc: 0.1464499980211258\n",
      "epoch: 305, loss: 2.905796180963516, acc: 0.1693999469280243, test loss: 2.9259642934799195, test acc: 0.14174991846084595\n",
      "epoch: 306, loss: 2.9055736553668976, acc: 0.17170000076293945, test loss: 2.924999358654022, test acc: 0.14719995856285095\n",
      "epoch: 307, loss: 2.9049743115901947, acc: 0.17249992489814758, test loss: 2.925601608753204, test acc: 0.14395001530647278\n",
      "epoch: 308, loss: 2.9054593884944917, acc: 0.17154991626739502, test loss: 2.9248437786102297, test acc: 0.14529995620250702\n",
      "epoch: 309, loss: 2.9044176387786864, acc: 0.17254997789859772, test loss: 2.9247865545749665, test acc: 0.14539992809295654\n",
      "epoch: 310, loss: 2.9046469163894653, acc: 0.17230001091957092, test loss: 2.9232667827606202, test acc: 0.14829997718334198\n",
      "epoch: 311, loss: 2.9044114553928377, acc: 0.17254993319511414, test loss: 2.922610704898834, test acc: 0.1488499492406845\n",
      "epoch: 312, loss: 2.9042826008796694, acc: 0.16999994218349457, test loss: 2.9246112692356108, test acc: 0.13774996995925903\n",
      "epoch: 313, loss: 2.9033926975727082, acc: 0.17559996247291565, test loss: 2.921968514919281, test acc: 0.15029993653297424\n",
      "epoch: 314, loss: 2.9046738862991335, acc: 0.17059995234012604, test loss: 2.9234825658798216, test acc: 0.1467999815940857\n",
      "epoch: 315, loss: 2.9037627732753752, acc: 0.1726999431848526, test loss: 2.9235256147384643, test acc: 0.1455499529838562\n",
      "epoch: 316, loss: 2.9041945815086363, acc: 0.17124994099140167, test loss: 2.9234836649894715, test acc: 0.14444997906684875\n",
      "epoch: 317, loss: 2.9028012216091157, acc: 0.1731499880552292, test loss: 2.921373107433319, test acc: 0.14904995262622833\n",
      "epoch: 318, loss: 2.902992044687271, acc: 0.1725499927997589, test loss: 2.9228246235847473, test acc: 0.14594992995262146\n",
      "epoch: 319, loss: 2.9038472414016723, acc: 0.17124995589256287, test loss: 2.921367291212082, test acc: 0.14889998733997345\n",
      "epoch: 320, loss: 2.90201650261879, acc: 0.17320001125335693, test loss: 2.9223946475982667, test acc: 0.14544996619224548\n",
      "epoch: 321, loss: 2.9021587800979614, acc: 0.17309992015361786, test loss: 2.9220815718173982, test acc: 0.1470499336719513\n",
      "epoch: 322, loss: 2.902084000110626, acc: 0.1732499897480011, test loss: 2.9218158602714537, test acc: 0.14719994366168976\n",
      "epoch: 323, loss: 2.902807219028473, acc: 0.17219997942447662, test loss: 2.922333984375, test acc: 0.15109997987747192\n",
      "epoch: 324, loss: 2.902454979419708, acc: 0.17254991829395294, test loss: 2.9203786444664, test acc: 0.14944995939731598\n",
      "epoch: 325, loss: 2.9022646498680116, acc: 0.1730499267578125, test loss: 2.9237204015254976, test acc: 0.13944995403289795\n",
      "epoch: 326, loss: 2.901995950937271, acc: 0.17399993538856506, test loss: 2.9215961265563966, test acc: 0.14524994790554047\n",
      "epoch: 327, loss: 2.902151598930359, acc: 0.17299993336200714, test loss: 2.9205361902713776, test acc: 0.14619997143745422\n",
      "epoch: 328, loss: 2.9026479971408845, acc: 0.17165005207061768, test loss: 2.9221232628822325, test acc: 0.1413499414920807\n",
      "epoch: 329, loss: 2.9013235461711884, acc: 0.17504997551441193, test loss: 2.9207348799705506, test acc: 0.14824998378753662\n",
      "epoch: 330, loss: 2.9014179062843324, acc: 0.17194990813732147, test loss: 2.923913282155991, test acc: 0.13095000386238098\n",
      "epoch: 331, loss: 2.901233822107315, acc: 0.1741999387741089, test loss: 2.922051873207092, test acc: 0.14084994792938232\n",
      "epoch: 332, loss: 2.901116545200348, acc: 0.1734999418258667, test loss: 2.9210727882385252, test acc: 0.15289999544620514\n",
      "epoch: 333, loss: 2.9012395894527434, acc: 0.17444996535778046, test loss: 2.9210560381412507, test acc: 0.14324992895126343\n",
      "epoch: 334, loss: 2.9011482536792754, acc: 0.17260000109672546, test loss: 2.919114158153534, test acc: 0.14719997346401215\n",
      "epoch: 335, loss: 2.9003634798526763, acc: 0.17904992401599884, test loss: 2.9224257814884185, test acc: 0.1366499662399292\n",
      "epoch: 336, loss: 2.900507012605667, acc: 0.1723499447107315, test loss: 2.920354573726654, test acc: 0.1523999273777008\n",
      "epoch: 337, loss: 2.9008319103717803, acc: 0.17559997737407684, test loss: 2.9203756761550905, test acc: 0.13699999451637268\n",
      "epoch: 338, loss: 2.901175379753113, acc: 0.17629995942115784, test loss: 2.920313849449158, test acc: 0.15494999289512634\n",
      "epoch: 339, loss: 2.9012265479564667, acc: 0.17170001566410065, test loss: 2.919797424077988, test acc: 0.14605002105236053\n",
      "epoch: 340, loss: 2.901074028015137, acc: 0.17439985275268555, test loss: 2.918394275903702, test acc: 0.15849994122982025\n",
      "epoch: 341, loss: 2.900922328233719, acc: 0.17279988527297974, test loss: 2.9190968906879426, test acc: 0.14474999904632568\n",
      "epoch: 342, loss: 2.9003031611442567, acc: 0.1708499789237976, test loss: 2.9195166671276094, test acc: 0.14504998922348022\n",
      "epoch: 343, loss: 2.900180473327637, acc: 0.17524999380111694, test loss: 2.9196531915664674, test acc: 0.14329996705055237\n",
      "epoch: 344, loss: 2.8994792091846464, acc: 0.1748998612165451, test loss: 2.9196767032146456, test acc: 0.15849994122982025\n",
      "epoch: 345, loss: 2.9006179010868074, acc: 0.17329998314380646, test loss: 2.920196877717972, test acc: 0.13714995980262756\n",
      "epoch: 346, loss: 2.8997782564163206, acc: 0.17319995164871216, test loss: 2.9209306335449217, test acc: 0.1461000144481659\n",
      "epoch: 347, loss: 2.90071771979332, acc: 0.17414997518062592, test loss: 2.9232711148262025, test acc: 0.13854999840259552\n",
      "epoch: 348, loss: 2.9010029447078707, acc: 0.1686999499797821, test loss: 2.919854553937912, test acc: 0.14354997873306274\n",
      "epoch: 349, loss: 2.90010040640831, acc: 0.172449991106987, test loss: 2.9206725478172304, test acc: 0.14744995534420013\n",
      "epoch: 350, loss: 2.900352646112442, acc: 0.17219997942447662, test loss: 2.921705275774002, test acc: 0.14565002918243408\n",
      "epoch: 351, loss: 2.89905970454216, acc: 0.17514991760253906, test loss: 2.9196275532245637, test acc: 0.14920000731945038\n",
      "epoch: 352, loss: 2.8994877886772157, acc: 0.17424997687339783, test loss: 2.9208662045001983, test acc: 0.14995001256465912\n",
      "epoch: 353, loss: 2.9002172219753266, acc: 0.17265000939369202, test loss: 2.918269441127777, test acc: 0.14614996314048767\n",
      "epoch: 354, loss: 2.899003255367279, acc: 0.17659993469715118, test loss: 2.9188782739639283, test acc: 0.15464992821216583\n",
      "epoch: 355, loss: 2.899288375377655, acc: 0.17334994673728943, test loss: 2.9189145398139953, test acc: 0.1377999484539032\n",
      "epoch: 356, loss: 2.8990145313739775, acc: 0.17649990320205688, test loss: 2.9175423049926756, test acc: 0.14544996619224548\n",
      "epoch: 357, loss: 2.8980536210536956, acc: 0.17669999599456787, test loss: 2.9194736456871033, test acc: 0.1513499617576599\n",
      "epoch: 358, loss: 2.899381376504898, acc: 0.1760999709367752, test loss: 2.917622226476669, test acc: 0.14684998989105225\n",
      "epoch: 359, loss: 2.8985823655128478, acc: 0.17664998769760132, test loss: 2.9165722048282623, test acc: 0.14869995415210724\n",
      "epoch: 360, loss: 2.899681931734085, acc: 0.1763499230146408, test loss: 2.9174958860874174, test acc: 0.14614996314048767\n",
      "epoch: 361, loss: 2.8995785629749298, acc: 0.17389993369579315, test loss: 2.922541860342026, test acc: 0.12529999017715454\n",
      "epoch: 362, loss: 2.8998210299015046, acc: 0.1731499284505844, test loss: 2.9168443882465365, test acc: 0.1474500149488449\n",
      "epoch: 363, loss: 2.8993990206718445, acc: 0.17339999973773956, test loss: 2.9172436881065367, test acc: 0.1444999724626541\n",
      "epoch: 364, loss: 2.8989357781410217, acc: 0.1737499237060547, test loss: 2.9183624601364135, test acc: 0.15814994275569916\n",
      "epoch: 365, loss: 2.8982764160633088, acc: 0.17664991319179535, test loss: 2.9179101288318634, test acc: 0.15344996750354767\n",
      "epoch: 366, loss: 2.8993573701381683, acc: 0.17249999940395355, test loss: 2.9170941948890685, test acc: 0.14174997806549072\n",
      "epoch: 367, loss: 2.899141074419022, acc: 0.17284993827342987, test loss: 2.916739753484726, test acc: 0.14719994366168976\n",
      "epoch: 368, loss: 2.8988824141025544, acc: 0.1741998791694641, test loss: 2.918023136854172, test acc: 0.13609999418258667\n",
      "epoch: 369, loss: 2.898198308944702, acc: 0.1759999394416809, test loss: 2.9199263572692873, test acc: 0.14409999549388885\n",
      "epoch: 370, loss: 2.8984918105602264, acc: 0.17584998905658722, test loss: 2.915923422574997, test acc: 0.14749999344348907\n",
      "epoch: 371, loss: 2.8983450925350187, acc: 0.17274999618530273, test loss: 2.9181778597831727, test acc: 0.1479499638080597\n",
      "epoch: 372, loss: 2.899373919963837, acc: 0.17194995284080505, test loss: 2.921133427619934, test acc: 0.141449972987175\n",
      "epoch: 373, loss: 2.8992506873607637, acc: 0.1732999086380005, test loss: 2.9163733100891114, test acc: 0.14864997565746307\n",
      "epoch: 374, loss: 2.8988303816318512, acc: 0.17624996602535248, test loss: 2.9168149530887604, test acc: 0.16064995527267456\n",
      "epoch: 375, loss: 2.8973351526260376, acc: 0.17674991488456726, test loss: 2.916640536785126, test acc: 0.14044997096061707\n",
      "epoch: 376, loss: 2.898150644302368, acc: 0.17464998364448547, test loss: 2.9168016469478606, test acc: 0.1425999402999878\n",
      "epoch: 377, loss: 2.898514914512634, acc: 0.17434999346733093, test loss: 2.918250994682312, test acc: 0.1507999747991562\n",
      "epoch: 378, loss: 2.8980598402023316, acc: 0.17624996602535248, test loss: 2.9149885082244875, test acc: 0.1482999175786972\n",
      "epoch: 379, loss: 2.898501754999161, acc: 0.1747499406337738, test loss: 2.9185007107257843, test acc: 0.14934994280338287\n",
      "epoch: 380, loss: 2.898955726623535, acc: 0.17294996976852417, test loss: 2.9156296038627625, test acc: 0.14229999482631683\n",
      "epoch: 381, loss: 2.8977094316482543, acc: 0.17789992690086365, test loss: 2.9198202967643736, test acc: 0.14159998297691345\n",
      "epoch: 382, loss: 2.8993152058124543, acc: 0.17135006189346313, test loss: 2.9167782402038576, test acc: 0.15959994494915009\n",
      "epoch: 383, loss: 2.897486426830292, acc: 0.17524990439414978, test loss: 2.9176038932800292, test acc: 0.153299942612648\n",
      "epoch: 384, loss: 2.8975523126125338, acc: 0.17629998922348022, test loss: 2.9153787541389464, test acc: 0.14505000412464142\n",
      "epoch: 385, loss: 2.8971067786216738, acc: 0.17869998514652252, test loss: 2.9161597907543184, test acc: 0.14474992454051971\n",
      "epoch: 386, loss: 2.896592744588852, acc: 0.17859996855258942, test loss: 2.9178338479995727, test acc: 0.1478499472141266\n",
      "epoch: 387, loss: 2.896378072500229, acc: 0.18024995923042297, test loss: 2.9179974853992463, test acc: 0.1394999772310257\n",
      "epoch: 388, loss: 2.8967581784725187, acc: 0.17559990286827087, test loss: 2.9173672223091125, test acc: 0.14914995431900024\n",
      "epoch: 389, loss: 2.8986194908618925, acc: 0.17239995300769806, test loss: 2.916026917695999, test acc: 0.15754994750022888\n",
      "epoch: 390, loss: 2.896926313638687, acc: 0.17674991488456726, test loss: 2.9153381049633027, test acc: 0.15564998984336853\n",
      "epoch: 391, loss: 2.896501572132111, acc: 0.17734995484352112, test loss: 2.9204463160037992, test acc: 0.14684997498989105\n",
      "epoch: 392, loss: 2.8981251299381254, acc: 0.17289991676807404, test loss: 2.9161862814426422, test acc: 0.15004998445510864\n",
      "epoch: 393, loss: 2.8969369649887087, acc: 0.175899937748909, test loss: 2.9154950523376466, test acc: 0.156499981880188\n",
      "epoch: 394, loss: 2.8973704993724825, acc: 0.17604990303516388, test loss: 2.9171982085704804, test acc: 0.1544499546289444\n",
      "epoch: 395, loss: 2.898389356136322, acc: 0.17289994657039642, test loss: 2.9168445825576783, test acc: 0.14984998106956482\n",
      "epoch: 396, loss: 2.8959593176841736, acc: 0.1811000108718872, test loss: 2.916187090873718, test acc: 0.1504499763250351\n",
      "epoch: 397, loss: 2.8967621219158173, acc: 0.17534999549388885, test loss: 2.915040838718414, test acc: 0.15674999356269836\n",
      "epoch: 398, loss: 2.896527725458145, acc: 0.17854994535446167, test loss: 2.916921327114105, test acc: 0.14894993603229523\n",
      "epoch: 399, loss: 2.897346452474594, acc: 0.17694993317127228, test loss: 2.915313478708267, test acc: 0.1445000171661377\n",
      "epoch: 400, loss: 2.896166276931763, acc: 0.17809993028640747, test loss: 2.9189589071273803, test acc: 0.1475999802350998\n",
      "epoch: 401, loss: 2.8971002209186554, acc: 0.1745498776435852, test loss: 2.9166519117355345, test acc: 0.15369994938373566\n",
      "epoch: 402, loss: 2.895947505235672, acc: 0.1767999529838562, test loss: 2.9200695300102235, test acc: 0.1453000009059906\n",
      "epoch: 403, loss: 2.896479802131653, acc: 0.17834998667240143, test loss: 2.915248404741287, test acc: 0.14649996161460876\n",
      "epoch: 404, loss: 2.896498259305954, acc: 0.17699997127056122, test loss: 2.916531219482422, test acc: 0.15154995024204254\n",
      "epoch: 405, loss: 2.8965617275238036, acc: 0.17699991166591644, test loss: 2.9167709767818453, test acc: 0.15269997715950012\n",
      "epoch: 406, loss: 2.895945905447006, acc: 0.17574995756149292, test loss: 2.9157242500782012, test acc: 0.14030000567436218\n",
      "epoch: 407, loss: 2.895696314573288, acc: 0.1755499392747879, test loss: 2.9183956742286683, test acc: 0.12690000236034393\n",
      "epoch: 408, loss: 2.8960611355304717, acc: 0.1781499832868576, test loss: 2.9168340599536897, test acc: 0.1457999348640442\n",
      "epoch: 409, loss: 2.8975090408325195, acc: 0.1754000335931778, test loss: 2.914621661901474, test acc: 0.15429997444152832\n",
      "epoch: 410, loss: 2.89603476524353, acc: 0.17764991521835327, test loss: 2.917470608949661, test acc: 0.1509000062942505\n",
      "epoch: 411, loss: 2.8969301879405975, acc: 0.17469993233680725, test loss: 2.9149952757358553, test acc: 0.15344995260238647\n",
      "epoch: 412, loss: 2.895383280515671, acc: 0.17729990184307098, test loss: 2.914419721364975, test acc: 0.15514998137950897\n",
      "epoch: 413, loss: 2.896618974208832, acc: 0.17494992911815643, test loss: 2.915725144147873, test acc: 0.15639996528625488\n",
      "epoch: 414, loss: 2.8956005001068115, acc: 0.17869991064071655, test loss: 2.9151161324977877, test acc: 0.1376999318599701\n",
      "epoch: 415, loss: 2.8956839299201964, acc: 0.17844995856285095, test loss: 2.914797639846802, test acc: 0.1602499634027481\n",
      "epoch: 416, loss: 2.896382315158844, acc: 0.1739499270915985, test loss: 2.914287021160126, test acc: 0.15509991347789764\n",
      "epoch: 417, loss: 2.8956281363964083, acc: 0.18039987981319427, test loss: 2.9159421050548553, test acc: 0.14774999022483826\n",
      "epoch: 418, loss: 2.895605700016022, acc: 0.17685002088546753, test loss: 2.9136370003223417, test acc: 0.15994992852210999\n",
      "epoch: 419, loss: 2.895436239242554, acc: 0.17654991149902344, test loss: 2.9178642582893373, test acc: 0.1367499679327011\n",
      "epoch: 420, loss: 2.8949941527843475, acc: 0.1766999214887619, test loss: 2.91371689081192, test acc: 0.1547500044107437\n",
      "epoch: 421, loss: 2.8962942826747895, acc: 0.17489999532699585, test loss: 2.913163192272186, test acc: 0.15659993886947632\n",
      "epoch: 422, loss: 2.89449395775795, acc: 0.18159997463226318, test loss: 2.913595190048218, test acc: 0.15989996492862701\n",
      "epoch: 423, loss: 2.8950178670883178, acc: 0.178399920463562, test loss: 2.9171047103405, test acc: 0.14629995822906494\n",
      "epoch: 424, loss: 2.896139303445816, acc: 0.1783999651670456, test loss: 2.914771182537079, test acc: 0.15189999341964722\n",
      "epoch: 425, loss: 2.896102865934372, acc: 0.17729993164539337, test loss: 2.913400101661682, test acc: 0.15839996933937073\n",
      "epoch: 426, loss: 2.894501489400864, acc: 0.17984998226165771, test loss: 2.9150937783718107, test acc: 0.15529993176460266\n",
      "epoch: 427, loss: 2.8951605558395386, acc: 0.17669996619224548, test loss: 2.913270696401596, test acc: 0.1399499773979187\n",
      "epoch: 428, loss: 2.8976346492767333, acc: 0.17169995605945587, test loss: 2.914470819234848, test acc: 0.13920006155967712\n",
      "epoch: 429, loss: 2.8960077142715455, acc: 0.1768999695777893, test loss: 2.9151262998580934, test acc: 0.1521499752998352\n",
      "epoch: 430, loss: 2.8949699783325196, acc: 0.17899999022483826, test loss: 2.914389590024948, test acc: 0.15619999170303345\n",
      "epoch: 431, loss: 2.895436415672302, acc: 0.17809993028640747, test loss: 2.9130151534080504, test acc: 0.1465499848127365\n",
      "epoch: 432, loss: 2.8943855571746826, acc: 0.18029990792274475, test loss: 2.913350594043732, test acc: 0.1533999741077423\n",
      "epoch: 433, loss: 2.894771645069122, acc: 0.17739997804164886, test loss: 2.9166692531108858, test acc: 0.15064997971057892\n",
      "epoch: 434, loss: 2.896342819929123, acc: 0.17520000040531158, test loss: 2.9136049234867096, test acc: 0.1422499716281891\n",
      "epoch: 435, loss: 2.8946079766750334, acc: 0.17829997837543488, test loss: 2.9127976512908935, test acc: 0.1573999524116516\n",
      "epoch: 436, loss: 2.894954972267151, acc: 0.17895002663135529, test loss: 2.9130333781242372, test acc: 0.15914997458457947\n",
      "epoch: 437, loss: 2.8960005700588227, acc: 0.17639999091625214, test loss: 2.9128501749038698, test acc: 0.1456499993801117\n",
      "epoch: 438, loss: 2.895290821790695, acc: 0.17504999041557312, test loss: 2.9195840060710907, test acc: 0.14334994554519653\n",
      "epoch: 439, loss: 2.8973789548873903, acc: 0.1725998818874359, test loss: 2.916688095331192, test acc: 0.1278500109910965\n",
      "epoch: 440, loss: 2.8953325653076174, acc: 0.17705003917217255, test loss: 2.9123976254463195, test acc: 0.15685001015663147\n",
      "epoch: 441, loss: 2.894225252866745, acc: 0.17909997701644897, test loss: 2.914398764371872, test acc: 0.15805000066757202\n",
      "epoch: 442, loss: 2.893815578222275, acc: 0.18099990487098694, test loss: 2.9174169158935546, test acc: 0.14834998548030853\n",
      "epoch: 443, loss: 2.8954488968849184, acc: 0.17864996194839478, test loss: 2.9153228390216825, test acc: 0.14969997107982635\n",
      "epoch: 444, loss: 2.8957596814632414, acc: 0.1766999065876007, test loss: 2.9138516449928282, test acc: 0.14069999754428864\n",
      "epoch: 445, loss: 2.8937081265449525, acc: 0.18109993636608124, test loss: 2.9152084410190584, test acc: 0.13260000944137573\n",
      "epoch: 446, loss: 2.8958265352249146, acc: 0.1787499189376831, test loss: 2.912324151992798, test acc: 0.15759998559951782\n",
      "epoch: 447, loss: 2.8943856596946715, acc: 0.1774999499320984, test loss: 2.912770656347275, test acc: 0.1580999791622162\n",
      "epoch: 448, loss: 2.89541818857193, acc: 0.1783999800682068, test loss: 2.9149308037757873, test acc: 0.15554994344711304\n",
      "epoch: 449, loss: 2.894686527252197, acc: 0.17754992842674255, test loss: 2.9148566234111786, test acc: 0.15059997141361237\n",
      "epoch: 450, loss: 2.895526007413864, acc: 0.1748998761177063, test loss: 2.9158763599395754, test acc: 0.1479499191045761\n",
      "epoch: 451, loss: 2.894319245815277, acc: 0.17879998683929443, test loss: 2.9118947803974153, test acc: 0.16065002977848053\n",
      "epoch: 452, loss: 2.894063390493393, acc: 0.17744992673397064, test loss: 2.912926059961319, test acc: 0.15435001254081726\n",
      "epoch: 453, loss: 2.8935063886642456, acc: 0.17899996042251587, test loss: 2.914066845178604, test acc: 0.14294999837875366\n",
      "epoch: 454, loss: 2.894656944274902, acc: 0.17764997482299805, test loss: 2.9113963294029235, test acc: 0.15929991006851196\n",
      "epoch: 455, loss: 2.8943350100517273, acc: 0.17674989998340607, test loss: 2.911859897375107, test acc: 0.15989996492862701\n",
      "epoch: 456, loss: 2.898626534938812, acc: 0.17124995589256287, test loss: 2.919797399044037, test acc: 0.14609995484352112\n",
      "epoch: 457, loss: 2.894039821624756, acc: 0.17904998362064362, test loss: 2.9126049733161925, test acc: 0.14354996383190155\n",
      "epoch: 458, loss: 2.893953263759613, acc: 0.17729996144771576, test loss: 2.911887493133545, test acc: 0.14315000176429749\n",
      "epoch: 459, loss: 2.8938334584236145, acc: 0.17794997990131378, test loss: 2.9136865627765656, test acc: 0.140299990773201\n",
      "epoch: 460, loss: 2.8942656576633454, acc: 0.1787499189376831, test loss: 2.9118146002292633, test acc: 0.15809999406337738\n",
      "epoch: 461, loss: 2.8940138494968415, acc: 0.17799995839595795, test loss: 2.9130989682674406, test acc: 0.15104995667934418\n",
      "epoch: 462, loss: 2.8941870307922364, acc: 0.17629991471767426, test loss: 2.911289492845535, test acc: 0.15949998795986176\n",
      "epoch: 463, loss: 2.89358749628067, acc: 0.17659997940063477, test loss: 2.9129821956157684, test acc: 0.15839999914169312\n",
      "epoch: 464, loss: 2.893651648759842, acc: 0.17995001375675201, test loss: 2.914150115251541, test acc: 0.15514999628067017\n",
      "epoch: 465, loss: 2.894277037382126, acc: 0.17779996991157532, test loss: 2.9111328387260436, test acc: 0.1521499752998352\n",
      "epoch: 466, loss: 2.8938863563537596, acc: 0.17864984273910522, test loss: 2.9109848272800445, test acc: 0.15720000863075256\n",
      "epoch: 467, loss: 2.893501884937286, acc: 0.1816999614238739, test loss: 2.911853114366531, test acc: 0.1465499848127365\n",
      "epoch: 468, loss: 2.8940598356723783, acc: 0.17984998226165771, test loss: 2.9118461656570434, test acc: 0.15820001065731049\n",
      "epoch: 469, loss: 2.8940367841720582, acc: 0.18045002222061157, test loss: 2.9108984673023226, test acc: 0.16529996693134308\n",
      "epoch: 470, loss: 2.8930793416500094, acc: 0.17939996719360352, test loss: 2.9117932784557343, test acc: 0.1568499654531479\n",
      "epoch: 471, loss: 2.8936962985992434, acc: 0.1783999651670456, test loss: 2.9118393051624296, test acc: 0.14340002834796906\n",
      "epoch: 472, loss: 2.8934508204460143, acc: 0.17739999294281006, test loss: 2.9129594349861145, test acc: 0.15754996240139008\n",
      "epoch: 473, loss: 2.896264274120331, acc: 0.17254997789859772, test loss: 2.9149399471282957, test acc: 0.1532999575138092\n",
      "epoch: 474, loss: 2.893126673698425, acc: 0.17914992570877075, test loss: 2.912073812484741, test acc: 0.15359991788864136\n",
      "epoch: 475, loss: 2.8934449529647828, acc: 0.17674997448921204, test loss: 2.910632425546646, test acc: 0.16314995288848877\n",
      "epoch: 476, loss: 2.892062430381775, acc: 0.1811499446630478, test loss: 2.9128279066085816, test acc: 0.1496499478816986\n",
      "epoch: 477, loss: 2.8941177880764006, acc: 0.17754989862442017, test loss: 2.9101069629192353, test acc: 0.15784995257854462\n",
      "epoch: 478, loss: 2.8929851698875426, acc: 0.17764991521835327, test loss: 2.9109366047382355, test acc: 0.15474995970726013\n",
      "epoch: 479, loss: 2.8926030480861664, acc: 0.1805998980998993, test loss: 2.91113849401474, test acc: 0.15709996223449707\n",
      "epoch: 480, loss: 2.892464475631714, acc: 0.178600013256073, test loss: 2.9107276046276094, test acc: 0.15754999220371246\n",
      "epoch: 481, loss: 2.89347621679306, acc: 0.17944984138011932, test loss: 2.91017294883728, test acc: 0.15924996137619019\n",
      "epoch: 482, loss: 2.892269016504288, acc: 0.1792999505996704, test loss: 2.911147426366806, test acc: 0.15484999120235443\n",
      "epoch: 483, loss: 2.8931751430034636, acc: 0.17844997346401215, test loss: 2.913526564836502, test acc: 0.155349999666214\n",
      "epoch: 484, loss: 2.8931990492343904, acc: 0.1804499477148056, test loss: 2.9130506563186644, test acc: 0.15085001289844513\n",
      "epoch: 485, loss: 2.8923706090450287, acc: 0.18019992113113403, test loss: 2.9101523208618163, test acc: 0.1551000028848648\n",
      "epoch: 486, loss: 2.892244151830673, acc: 0.1807500422000885, test loss: 2.9103402745723725, test acc: 0.15810002386569977\n",
      "epoch: 487, loss: 2.89360582113266, acc: 0.177749902009964, test loss: 2.910978388786316, test acc: 0.14239998161792755\n",
      "epoch: 488, loss: 2.8931724429130554, acc: 0.17869992554187775, test loss: 2.9093240225315093, test acc: 0.15424996614456177\n",
      "epoch: 489, loss: 2.892119121551514, acc: 0.18064996600151062, test loss: 2.9106236374378205, test acc: 0.14924994111061096\n",
      "epoch: 490, loss: 2.891891825199127, acc: 0.1792999804019928, test loss: 2.908900144100189, test acc: 0.1539999544620514\n",
      "epoch: 491, loss: 2.8920985877513887, acc: 0.1792999804019928, test loss: 2.9116301095485686, test acc: 0.15254999697208405\n",
      "epoch: 492, loss: 2.891860543489456, acc: 0.18069995939731598, test loss: 2.909842824935913, test acc: 0.15925000607967377\n",
      "epoch: 493, loss: 2.893005324602127, acc: 0.17989994585514069, test loss: 2.9101575243473055, test acc: 0.15609996020793915\n",
      "epoch: 494, loss: 2.894236023426056, acc: 0.17689992487430573, test loss: 2.917631195783615, test acc: 0.1447499692440033\n",
      "epoch: 495, loss: 2.892383165359497, acc: 0.17904996871948242, test loss: 2.9110262966156006, test acc: 0.15699996054172516\n",
      "epoch: 496, loss: 2.893699514865875, acc: 0.17604996263980865, test loss: 2.909537479877472, test acc: 0.16350004076957703\n",
      "epoch: 497, loss: 2.892418074607849, acc: 0.17900002002716064, test loss: 2.909865678548813, test acc: 0.16034996509552002\n",
      "epoch: 498, loss: 2.8940890681743623, acc: 0.17729990184307098, test loss: 2.909975731372833, test acc: 0.15709999203681946\n",
      "epoch: 499, loss: 2.891291434764862, acc: 0.18004991114139557, test loss: 2.9092585372924806, test acc: 0.1597999930381775\n",
      "epoch: 500, loss: 2.891770626306534, acc: 0.1816999614238739, test loss: 2.91275381565094, test acc: 0.15079998970031738\n",
      "epoch: 501, loss: 2.8919178545475006, acc: 0.18024994432926178, test loss: 2.9087914776802064, test acc: 0.1587999314069748\n",
      "epoch: 502, loss: 2.8908685314655305, acc: 0.1816999465227127, test loss: 2.9102296102046967, test acc: 0.15349997580051422\n",
      "epoch: 503, loss: 2.891707537174225, acc: 0.17925003170967102, test loss: 2.912418974637985, test acc: 0.15485000610351562\n",
      "epoch: 504, loss: 2.8913376331329346, acc: 0.18234993517398834, test loss: 2.910388616323471, test acc: 0.15669991075992584\n",
      "epoch: 505, loss: 2.8916507077217104, acc: 0.17994998395442963, test loss: 2.914356368780136, test acc: 0.14974993467330933\n",
      "epoch: 506, loss: 2.890971299409866, acc: 0.18184997141361237, test loss: 2.9094052958488463, test acc: 0.15730001032352448\n",
      "epoch: 507, loss: 2.8909363090991973, acc: 0.18264995515346527, test loss: 2.9097726476192474, test acc: 0.16014999151229858\n",
      "epoch: 508, loss: 2.8918105292320253, acc: 0.18119996786117554, test loss: 2.9155515027046204, test acc: 0.12894999980926514\n",
      "epoch: 509, loss: 2.8908276808261872, acc: 0.1816999465227127, test loss: 2.907686986923218, test acc: 0.1550999879837036\n",
      "epoch: 510, loss: 2.8913799011707306, acc: 0.18045000731945038, test loss: 2.9086282730102537, test acc: 0.16509997844696045\n",
      "epoch: 511, loss: 2.891799463033676, acc: 0.1802498996257782, test loss: 2.9080750477313995, test acc: 0.15839999914169312\n",
      "epoch: 512, loss: 2.8914765977859496, acc: 0.18169988691806793, test loss: 2.9080115830898285, test acc: 0.1574999839067459\n",
      "epoch: 513, loss: 2.89236542224884, acc: 0.17880003154277802, test loss: 2.9161934196949004, test acc: 0.1434999704360962\n",
      "epoch: 514, loss: 2.891707615852356, acc: 0.1801498979330063, test loss: 2.9091603684425356, test acc: 0.15609994530677795\n",
      "epoch: 515, loss: 2.8910023736953736, acc: 0.18184997141361237, test loss: 2.9097020721435545, test acc: 0.1586499959230423\n",
      "epoch: 516, loss: 2.892157142162323, acc: 0.18124991655349731, test loss: 2.908513488769531, test acc: 0.15989993512630463\n",
      "epoch: 517, loss: 2.890156320333481, acc: 0.1823998987674713, test loss: 2.9139132142066955, test acc: 0.14539998769760132\n",
      "epoch: 518, loss: 2.8910036635398866, acc: 0.1822998821735382, test loss: 2.9107778775691986, test acc: 0.1372000277042389\n",
      "epoch: 519, loss: 2.891110037565231, acc: 0.18105001747608185, test loss: 2.908040475845337, test acc: 0.16564998030662537\n",
      "epoch: 520, loss: 2.8908811235427856, acc: 0.18330006301403046, test loss: 2.907863653898239, test acc: 0.16030003130435944\n",
      "epoch: 521, loss: 2.8916961443424225, acc: 0.17984996736049652, test loss: 2.908491005897522, test acc: 0.16199995577335358\n",
      "epoch: 522, loss: 2.89236550450325, acc: 0.17840003967285156, test loss: 2.9091069412231447, test acc: 0.15784995257854462\n",
      "epoch: 523, loss: 2.889823303222656, acc: 0.18539996445178986, test loss: 2.9074092638492584, test acc: 0.1629999577999115\n",
      "epoch: 524, loss: 2.8909549844264983, acc: 0.18394993245601654, test loss: 2.909368883371353, test acc: 0.15765000879764557\n",
      "epoch: 525, loss: 2.8899216043949125, acc: 0.1868499219417572, test loss: 2.9129779815673826, test acc: 0.1480499655008316\n",
      "epoch: 526, loss: 2.890852479934692, acc: 0.18444989621639252, test loss: 2.9084509551525115, test acc: 0.1627499908208847\n",
      "epoch: 527, loss: 2.8910839378833773, acc: 0.18319997191429138, test loss: 2.907871813774109, test acc: 0.15859998762607574\n",
      "epoch: 528, loss: 2.8898244559764863, acc: 0.18509992957115173, test loss: 2.9112251091003416, test acc: 0.14990000426769257\n",
      "epoch: 529, loss: 2.891674975156784, acc: 0.18285003304481506, test loss: 2.9094891583919527, test acc: 0.16009999811649323\n",
      "epoch: 530, loss: 2.892781215906143, acc: 0.17850002646446228, test loss: 2.9072145295143126, test acc: 0.1661999672651291\n",
      "epoch: 531, loss: 2.8904913020133973, acc: 0.18394988775253296, test loss: 2.910573871135712, test acc: 0.15450000762939453\n",
      "epoch: 532, loss: 2.8909773242473604, acc: 0.18564996123313904, test loss: 2.9103866708278656, test acc: 0.1662999540567398\n",
      "epoch: 533, loss: 2.8915072619915008, acc: 0.18279995024204254, test loss: 2.9085507369041443, test acc: 0.15879997611045837\n",
      "epoch: 534, loss: 2.8904299461841583, acc: 0.18389992415905, test loss: 2.9076402294635773, test acc: 0.16230003535747528\n",
      "epoch: 535, loss: 2.8907322335243224, acc: 0.18289995193481445, test loss: 2.910495399236679, test acc: 0.13864995539188385\n",
      "epoch: 536, loss: 2.8909299433231355, acc: 0.18149994313716888, test loss: 2.9144152247905732, test acc: 0.15154996514320374\n",
      "epoch: 537, loss: 2.8901689112186433, acc: 0.1842000037431717, test loss: 2.910450098514557, test acc: 0.15874996781349182\n",
      "epoch: 538, loss: 2.8905667304992675, acc: 0.18599992990493774, test loss: 2.910799856185913, test acc: 0.15655000507831573\n",
      "epoch: 539, loss: 2.890553550720215, acc: 0.18504999577999115, test loss: 2.908959002494812, test acc: 0.15755002200603485\n",
      "epoch: 540, loss: 2.8904215443134307, acc: 0.1855999380350113, test loss: 2.9087374889850617, test acc: 0.15314993262290955\n",
      "epoch: 541, loss: 2.891828157901764, acc: 0.1816999316215515, test loss: 2.9099370503425597, test acc: 0.1593000292778015\n",
      "epoch: 542, loss: 2.8918957901000977, acc: 0.18019995093345642, test loss: 2.9073977625370024, test acc: 0.16079993546009064\n",
      "epoch: 543, loss: 2.8920193469524382, acc: 0.18314999341964722, test loss: 2.9115650284290315, test acc: 0.1538499891757965\n",
      "epoch: 544, loss: 2.890455973148346, acc: 0.18335001170635223, test loss: 2.90667019367218, test acc: 0.16074997186660767\n",
      "epoch: 545, loss: 2.889920264482498, acc: 0.18749995529651642, test loss: 2.908847870826721, test acc: 0.15754999220371246\n",
      "epoch: 546, loss: 2.8917401850223543, acc: 0.17889997363090515, test loss: 2.9088836991786957, test acc: 0.1626499593257904\n",
      "epoch: 547, loss: 2.890523928403854, acc: 0.18209996819496155, test loss: 2.910165251493454, test acc: 0.15869997441768646\n",
      "epoch: 548, loss: 2.8902709889411926, acc: 0.18449993431568146, test loss: 2.9087580072879793, test acc: 0.1586499810218811\n",
      "epoch: 549, loss: 2.890141624212265, acc: 0.18634988367557526, test loss: 2.911803119182587, test acc: 0.13764996826648712\n",
      "epoch: 550, loss: 2.8904794883728027, acc: 0.1851499378681183, test loss: 2.9069146275520326, test acc: 0.16425001621246338\n",
      "epoch: 551, loss: 2.8914989030361173, acc: 0.1829499453306198, test loss: 2.910351130962372, test acc: 0.13844995200634003\n",
      "epoch: 552, loss: 2.891093727350235, acc: 0.18219996988773346, test loss: 2.906914269924164, test acc: 0.16169999539852142\n",
      "epoch: 553, loss: 2.889372419118881, acc: 0.1857999563217163, test loss: 2.907648713588715, test acc: 0.1577499955892563\n",
      "epoch: 554, loss: 2.8894123697280882, acc: 0.18684998154640198, test loss: 2.910123919248581, test acc: 0.15669995546340942\n",
      "epoch: 555, loss: 2.889326934814453, acc: 0.18554995954036713, test loss: 2.909915701150894, test acc: 0.1583499312400818\n",
      "epoch: 556, loss: 2.890840445756912, acc: 0.18144990503787994, test loss: 2.9094138860702516, test acc: 0.1587500423192978\n",
      "epoch: 557, loss: 2.8907785701751707, acc: 0.18069994449615479, test loss: 2.910400706529617, test acc: 0.1540999859571457\n",
      "epoch: 558, loss: 2.890575029850006, acc: 0.18269993364810944, test loss: 2.911347452402115, test acc: 0.1356000304222107\n",
      "epoch: 559, loss: 2.890451055765152, acc: 0.1837499588727951, test loss: 2.910644150972366, test acc: 0.13564996421337128\n",
      "epoch: 560, loss: 2.8905384159088134, acc: 0.18109995126724243, test loss: 2.9141616725921633, test acc: 0.1502000093460083\n",
      "epoch: 561, loss: 2.889837157726288, acc: 0.18439999222755432, test loss: 2.906867364645004, test acc: 0.16199997067451477\n",
      "epoch: 562, loss: 2.8892186844348906, acc: 0.18540002405643463, test loss: 2.9084651815891265, test acc: 0.1575000286102295\n",
      "epoch: 563, loss: 2.890436978340149, acc: 0.18129995465278625, test loss: 2.9071928322315217, test acc: 0.16079993546009064\n",
      "epoch: 564, loss: 2.889437003135681, acc: 0.18474999070167542, test loss: 2.907894879579544, test acc: 0.15994997322559357\n",
      "epoch: 565, loss: 2.889441831111908, acc: 0.18619994819164276, test loss: 2.909534788131714, test acc: 0.15624995529651642\n",
      "epoch: 566, loss: 2.88907212138176, acc: 0.18574993312358856, test loss: 2.907929675579071, test acc: 0.16379991173744202\n",
      "epoch: 567, loss: 2.889675190448761, acc: 0.18684998154640198, test loss: 2.9078651797771453, test acc: 0.15644994378089905\n",
      "epoch: 568, loss: 2.890920557975769, acc: 0.18174996972084045, test loss: 2.91226589679718, test acc: 0.157399982213974\n",
      "epoch: 569, loss: 2.890260412693024, acc: 0.18434995412826538, test loss: 2.906492247581482, test acc: 0.16280001401901245\n",
      "epoch: 570, loss: 2.8895336997509005, acc: 0.18505001068115234, test loss: 2.9083218014240266, test acc: 0.14454996585845947\n",
      "epoch: 571, loss: 2.892593125104904, acc: 0.18069995939731598, test loss: 2.908836382627487, test acc: 0.15619994699954987\n",
      "epoch: 572, loss: 2.889613106250763, acc: 0.18449994921684265, test loss: 2.9062547135353087, test acc: 0.16269995272159576\n",
      "epoch: 573, loss: 2.889651411771774, acc: 0.18459999561309814, test loss: 2.90649374127388, test acc: 0.16124999523162842\n",
      "epoch: 574, loss: 2.889439145326614, acc: 0.18814991414546967, test loss: 2.9099782526493074, test acc: 0.13825005292892456\n",
      "epoch: 575, loss: 2.8898179161548616, acc: 0.18149986863136292, test loss: 2.9118746948242187, test acc: 0.14499996602535248\n",
      "epoch: 576, loss: 2.8894085597991945, acc: 0.18379995226860046, test loss: 2.9070902073383333, test acc: 0.16079998016357422\n",
      "epoch: 577, loss: 2.889121220111847, acc: 0.18450000882148743, test loss: 2.9094595324993135, test acc: 0.15629997849464417\n",
      "epoch: 578, loss: 2.8905123031139373, acc: 0.18424995243549347, test loss: 2.9066136491298677, test acc: 0.16204997897148132\n",
      "epoch: 579, loss: 2.890082588195801, acc: 0.18709991872310638, test loss: 2.9081027030944826, test acc: 0.16049998998641968\n",
      "epoch: 580, loss: 2.8893205177783967, acc: 0.18355002999305725, test loss: 2.9059473502635957, test acc: 0.1656000018119812\n",
      "epoch: 581, loss: 2.889666391611099, acc: 0.1881500482559204, test loss: 2.9069109427928925, test acc: 0.1636999547481537\n",
      "epoch: 582, loss: 2.889478875398636, acc: 0.18599994480609894, test loss: 2.906599760055542, test acc: 0.16164998710155487\n",
      "epoch: 583, loss: 2.888954484462738, acc: 0.1869499385356903, test loss: 2.90705495595932, test acc: 0.1606999635696411\n",
      "epoch: 584, loss: 2.889109054803848, acc: 0.1862998902797699, test loss: 2.9084017872810364, test acc: 0.1604999601840973\n",
      "epoch: 585, loss: 2.8912784373760223, acc: 0.18199993669986725, test loss: 2.9089898145198823, test acc: 0.15994995832443237\n",
      "epoch: 586, loss: 2.8888483214378358, acc: 0.187749981880188, test loss: 2.908104909658432, test acc: 0.15524998307228088\n",
      "epoch: 587, loss: 2.890422796010971, acc: 0.18254996836185455, test loss: 2.909036309719086, test acc: 0.16209997236728668\n",
      "epoch: 588, loss: 2.8890561485290527, acc: 0.18674995005130768, test loss: 2.907073961496353, test acc: 0.16409996151924133\n",
      "epoch: 589, loss: 2.889463642835617, acc: 0.18644995987415314, test loss: 2.9106324779987336, test acc: 0.15394999086856842\n",
      "epoch: 590, loss: 2.8889091169834136, acc: 0.1867000013589859, test loss: 2.906211391687393, test acc: 0.16375002264976501\n",
      "epoch: 591, loss: 2.889104996919632, acc: 0.18764998018741608, test loss: 2.9090719020366667, test acc: 0.15754997730255127\n",
      "epoch: 592, loss: 2.888508042097092, acc: 0.1896999180316925, test loss: 2.906801146268845, test acc: 0.16554996371269226\n",
      "epoch: 593, loss: 2.8902573692798614, acc: 0.1840999573469162, test loss: 2.9085924661159517, test acc: 0.16349990665912628\n",
      "epoch: 594, loss: 2.889464873075485, acc: 0.18754999339580536, test loss: 2.9062853062152865, test acc: 0.16639995574951172\n",
      "epoch: 595, loss: 2.888274129629135, acc: 0.1877499222755432, test loss: 2.91081264257431, test acc: 0.15794996917247772\n",
      "epoch: 596, loss: 2.8883164596557616, acc: 0.1895999014377594, test loss: 2.9066192531585693, test acc: 0.16809992492198944\n",
      "epoch: 597, loss: 2.8883904123306277, acc: 0.18979987502098083, test loss: 2.9085404860973356, test acc: 0.156949982047081\n",
      "epoch: 598, loss: 2.8904315769672393, acc: 0.18309994041919708, test loss: 2.9074811446666717, test acc: 0.166299968957901\n",
      "epoch: 599, loss: 2.8880067694187166, acc: 0.1909000277519226, test loss: 2.907882115840912, test acc: 0.1585499793291092\n",
      "epoch: 600, loss: 2.8882998168468474, acc: 0.18874992430210114, test loss: 2.9063558185100558, test acc: 0.1603499948978424\n",
      "epoch: 601, loss: 2.8888524174690247, acc: 0.1867499202489853, test loss: 2.905623663663864, test acc: 0.17014993727207184\n",
      "epoch: 602, loss: 2.8885014629364014, acc: 0.18720002472400665, test loss: 2.9062119483947755, test acc: 0.16944995522499084\n",
      "epoch: 603, loss: 2.8888421154022215, acc: 0.1883499175310135, test loss: 2.9113638651371003, test acc: 0.13369999825954437\n",
      "epoch: 604, loss: 2.890148230791092, acc: 0.1854500025510788, test loss: 2.907224122285843, test acc: 0.16555003821849823\n",
      "epoch: 605, loss: 2.887883279323578, acc: 0.1893499195575714, test loss: 2.907434084415436, test acc: 0.157150000333786\n",
      "epoch: 606, loss: 2.8887735664844514, acc: 0.1869499236345291, test loss: 2.908329213857651, test acc: 0.15949994325637817\n",
      "epoch: 607, loss: 2.887750673294067, acc: 0.1909499168395996, test loss: 2.905502157211304, test acc: 0.16909989714622498\n",
      "epoch: 608, loss: 2.8900727939605715, acc: 0.1839999407529831, test loss: 2.9052976286411285, test acc: 0.1687999963760376\n",
      "epoch: 609, loss: 2.88755273103714, acc: 0.19184993207454681, test loss: 2.90655863404274, test acc: 0.16559986770153046\n",
      "epoch: 610, loss: 2.8889757788181303, acc: 0.1871999353170395, test loss: 2.9055244016647337, test acc: 0.16704992949962616\n",
      "epoch: 611, loss: 2.8880369222164153, acc: 0.18949994444847107, test loss: 2.9059080159664155, test acc: 0.1669999063014984\n",
      "epoch: 612, loss: 2.8886754047870635, acc: 0.1855999380350113, test loss: 2.9074119114875794, test acc: 0.15999993681907654\n",
      "epoch: 613, loss: 2.8877244806289672, acc: 0.1904999315738678, test loss: 2.9067671203613283, test acc: 0.16109995543956757\n",
      "epoch: 614, loss: 2.8900283014774324, acc: 0.18630002439022064, test loss: 2.9097620022296904, test acc: 0.14204998314380646\n",
      "epoch: 615, loss: 2.887967483997345, acc: 0.18904992938041687, test loss: 2.905551450252533, test acc: 0.1684999316930771\n",
      "epoch: 616, loss: 2.889937833547592, acc: 0.18220002949237823, test loss: 2.907651972770691, test acc: 0.16304998099803925\n",
      "epoch: 617, loss: 2.887475003004074, acc: 0.1887999325990677, test loss: 2.908223842382431, test acc: 0.15389996767044067\n",
      "epoch: 618, loss: 2.890363794565201, acc: 0.18400000035762787, test loss: 2.9093465495109556, test acc: 0.15949994325637817\n",
      "epoch: 619, loss: 2.888567988872528, acc: 0.18739990890026093, test loss: 2.9072513353824614, test acc: 0.16429996490478516\n",
      "epoch: 620, loss: 2.8893928360939025, acc: 0.18794992566108704, test loss: 2.905383163690567, test acc: 0.1686999350786209\n",
      "epoch: 621, loss: 2.8878153657913206, acc: 0.19209998846054077, test loss: 2.907103316783905, test acc: 0.16149991750717163\n",
      "epoch: 622, loss: 2.8867485892772673, acc: 0.1937500238418579, test loss: 2.9059436357021333, test acc: 0.16409997642040253\n",
      "epoch: 623, loss: 2.887312921285629, acc: 0.19120000302791595, test loss: 2.9063307774066924, test acc: 0.1645999550819397\n",
      "epoch: 624, loss: 2.888552050590515, acc: 0.18930000066757202, test loss: 2.906263253688812, test acc: 0.16759993135929108\n",
      "epoch: 625, loss: 2.8872669517993925, acc: 0.1905999332666397, test loss: 2.906988078355789, test acc: 0.16214995086193085\n",
      "epoch: 626, loss: 2.8874090790748594, acc: 0.18944990634918213, test loss: 2.912168424129486, test acc: 0.14169998466968536\n",
      "epoch: 627, loss: 2.88812157869339, acc: 0.1901499629020691, test loss: 2.9059314560890197, test acc: 0.17104998230934143\n",
      "epoch: 628, loss: 2.888891192674637, acc: 0.18654993176460266, test loss: 2.906621140241623, test acc: 0.16329994797706604\n",
      "epoch: 629, loss: 2.889574567079544, acc: 0.18484999239444733, test loss: 2.9057793641090393, test acc: 0.16739988327026367\n",
      "epoch: 630, loss: 2.887952243089676, acc: 0.1897999346256256, test loss: 2.906885493993759, test acc: 0.1645500212907791\n",
      "epoch: 631, loss: 2.8889886808395384, acc: 0.18924999237060547, test loss: 2.9058320450782777, test acc: 0.16874995827674866\n",
      "epoch: 632, loss: 2.888159365653992, acc: 0.18774999678134918, test loss: 2.905538649559021, test acc: 0.16944995522499084\n",
      "epoch: 633, loss: 2.8867116916179656, acc: 0.19209997355937958, test loss: 2.9050590896606447, test acc: 0.17444993555545807\n",
      "epoch: 634, loss: 2.888672981262207, acc: 0.18764999508857727, test loss: 2.9075582909584043, test acc: 0.166549950838089\n",
      "epoch: 635, loss: 2.891825639009476, acc: 0.18184997141361237, test loss: 2.905237938165665, test acc: 0.1677999496459961\n",
      "epoch: 636, loss: 2.8893963623046877, acc: 0.184549942612648, test loss: 2.908190828561783, test acc: 0.1610499620437622\n",
      "epoch: 637, loss: 2.8869963490962984, acc: 0.19309991598129272, test loss: 2.908306164741516, test acc: 0.15984992682933807\n",
      "epoch: 638, loss: 2.8887618839740754, acc: 0.1881999373435974, test loss: 2.9105678570270537, test acc: 0.15234996378421783\n",
      "epoch: 639, loss: 2.8875717127323153, acc: 0.19159993529319763, test loss: 2.909940068721771, test acc: 0.15429995954036713\n",
      "epoch: 640, loss: 2.887156445980072, acc: 0.19089989364147186, test loss: 2.907761609554291, test acc: 0.16009998321533203\n",
      "epoch: 641, loss: 2.889379140138626, acc: 0.1864999383687973, test loss: 2.904481230974197, test acc: 0.1699499636888504\n",
      "epoch: 642, loss: 2.888210562467575, acc: 0.18664993345737457, test loss: 2.9077032613754272, test acc: 0.16159996390342712\n",
      "epoch: 643, loss: 2.887410169839859, acc: 0.18879994750022888, test loss: 2.9053253185749055, test acc: 0.1714499443769455\n",
      "epoch: 644, loss: 2.886561681032181, acc: 0.19099991023540497, test loss: 2.906879130601883, test acc: 0.16504991054534912\n",
      "epoch: 645, loss: 2.888012931346893, acc: 0.1880999058485031, test loss: 2.906674462556839, test acc: 0.16794995963573456\n",
      "epoch: 646, loss: 2.8866050338745115, acc: 0.19204998016357422, test loss: 2.9110728299617765, test acc: 0.14094997942447662\n",
      "epoch: 647, loss: 2.8870040202140808, acc: 0.18844999372959137, test loss: 2.904640843868256, test acc: 0.1714499443769455\n",
      "epoch: 648, loss: 2.889293067455292, acc: 0.1879999190568924, test loss: 2.9065319859981535, test acc: 0.1676500141620636\n",
      "epoch: 649, loss: 2.8874330282211305, acc: 0.1898999810218811, test loss: 2.907026834487915, test acc: 0.158099964261055\n",
      "epoch: 650, loss: 2.887635760307312, acc: 0.18810005486011505, test loss: 2.90622655749321, test acc: 0.17044994235038757\n",
      "epoch: 651, loss: 2.8886631822586057, acc: 0.18669992685317993, test loss: 2.9068134582042693, test acc: 0.16429995000362396\n",
      "epoch: 652, loss: 2.8883812022209168, acc: 0.1873999834060669, test loss: 2.9077774453163148, test acc: 0.1443999707698822\n",
      "epoch: 653, loss: 2.8875988399982453, acc: 0.19185000658035278, test loss: 2.905145491361618, test acc: 0.16944994032382965\n",
      "epoch: 654, loss: 2.8880565667152407, acc: 0.18839991092681885, test loss: 2.9080737245082857, test acc: 0.14324995875358582\n",
      "epoch: 655, loss: 2.887122954130173, acc: 0.19044998288154602, test loss: 2.90619699716568, test acc: 0.1490500122308731\n",
      "epoch: 656, loss: 2.8863983821868895, acc: 0.1930999606847763, test loss: 2.9051393342018126, test acc: 0.1695999652147293\n",
      "epoch: 657, loss: 2.887784881591797, acc: 0.18884991109371185, test loss: 2.9074248564243317, test acc: 0.16249996423721313\n",
      "epoch: 658, loss: 2.8874426436424256, acc: 0.1894499808549881, test loss: 2.915321251153946, test acc: 0.13414999842643738\n",
      "epoch: 659, loss: 2.889311763048172, acc: 0.18259994685649872, test loss: 2.906689010858536, test acc: 0.16609995067119598\n",
      "epoch: 660, loss: 2.88695317029953, acc: 0.19409991800785065, test loss: 2.90661781668663, test acc: 0.16564995050430298\n",
      "epoch: 661, loss: 2.8882567262649537, acc: 0.1888999193906784, test loss: 2.905294716358185, test acc: 0.16599993407726288\n",
      "epoch: 662, loss: 2.885914866924286, acc: 0.19474992156028748, test loss: 2.912538117170334, test acc: 0.14014999568462372\n",
      "epoch: 663, loss: 2.8866936790943147, acc: 0.19284996390342712, test loss: 2.904309242963791, test acc: 0.17284998297691345\n",
      "epoch: 664, loss: 2.8869436049461363, acc: 0.18974986672401428, test loss: 2.904703825712204, test acc: 0.1734999716281891\n",
      "epoch: 665, loss: 2.8878371942043306, acc: 0.18929994106292725, test loss: 2.9053123450279235, test acc: 0.1669999659061432\n",
      "epoch: 666, loss: 2.8873386251926423, acc: 0.18974998593330383, test loss: 2.9050291991233825, test acc: 0.1702999621629715\n",
      "epoch: 667, loss: 2.885688774585724, acc: 0.1937999576330185, test loss: 2.9049568617343904, test acc: 0.17024998366832733\n",
      "epoch: 668, loss: 2.886134845018387, acc: 0.19364991784095764, test loss: 2.9049326586723327, test acc: 0.17270000278949738\n",
      "epoch: 669, loss: 2.886992828845978, acc: 0.18995000422000885, test loss: 2.9087242710590364, test acc: 0.1440999060869217\n",
      "epoch: 670, loss: 2.8876314878463747, acc: 0.19079986214637756, test loss: 2.9045363450050354, test acc: 0.16764996945858002\n",
      "epoch: 671, loss: 2.8876645517349244, acc: 0.18979990482330322, test loss: 2.906896448135376, test acc: 0.16604997217655182\n",
      "epoch: 672, loss: 2.886807751655579, acc: 0.19179993867874146, test loss: 2.9059497344493868, test acc: 0.16789990663528442\n",
      "epoch: 673, loss: 2.8875170803070067, acc: 0.1860499531030655, test loss: 2.9042568588256836, test acc: 0.17194993793964386\n",
      "epoch: 674, loss: 2.8866811740398406, acc: 0.1915498971939087, test loss: 2.9103601181507113, test acc: 0.1503499150276184\n",
      "epoch: 675, loss: 2.886366400718689, acc: 0.19179996848106384, test loss: 2.904089630842209, test acc: 0.17075002193450928\n",
      "epoch: 676, loss: 2.8863604867458346, acc: 0.19224995374679565, test loss: 2.90604483962059, test acc: 0.15049998462200165\n",
      "epoch: 677, loss: 2.8882897078990935, acc: 0.18904997408390045, test loss: 2.909930019378662, test acc: 0.13989999890327454\n",
      "epoch: 678, loss: 2.88671496629715, acc: 0.19014999270439148, test loss: 2.9048405408859255, test acc: 0.17229998111724854\n",
      "epoch: 679, loss: 2.886508346796036, acc: 0.1906999796628952, test loss: 2.910073311328888, test acc: 0.1577499508857727\n",
      "epoch: 680, loss: 2.8871203100681306, acc: 0.1916499137878418, test loss: 2.913922197818756, test acc: 0.1456500142812729\n",
      "epoch: 681, loss: 2.886684082746506, acc: 0.19419996440410614, test loss: 2.914419618844986, test acc: 0.14524996280670166\n",
      "epoch: 682, loss: 2.8851446115970614, acc: 0.19529989361763, test loss: 2.9047366988658907, test acc: 0.16554996371269226\n",
      "epoch: 683, loss: 2.8865194404125214, acc: 0.1949499100446701, test loss: 2.910879434347153, test acc: 0.13924996554851532\n",
      "epoch: 684, loss: 2.885923441648483, acc: 0.19579994678497314, test loss: 2.9095613622665404, test acc: 0.1609499305486679\n",
      "epoch: 685, loss: 2.886610469818115, acc: 0.19269989430904388, test loss: 2.9073766589164736, test acc: 0.14479996263980865\n",
      "epoch: 686, loss: 2.887609157562256, acc: 0.18939989805221558, test loss: 2.905346108675003, test acc: 0.16919998824596405\n",
      "epoch: 687, loss: 2.885254828929901, acc: 0.1955498903989792, test loss: 2.9040110540390014, test acc: 0.16904999315738678\n",
      "epoch: 688, loss: 2.8862463283538817, acc: 0.1918499618768692, test loss: 2.9080489671230314, test acc: 0.15839996933937073\n",
      "epoch: 689, loss: 2.8876464521884917, acc: 0.18884985148906708, test loss: 2.915668796300888, test acc: 0.14664998650550842\n",
      "epoch: 690, loss: 2.885160210132599, acc: 0.19604989886283875, test loss: 2.9030360889434816, test acc: 0.17274992167949677\n",
      "epoch: 691, loss: 2.885742928981781, acc: 0.19159989058971405, test loss: 2.905834640264511, test acc: 0.13055001199245453\n",
      "epoch: 692, loss: 2.8851660466194153, acc: 0.19569998979568481, test loss: 2.9047335493564606, test acc: 0.16470004618167877\n",
      "epoch: 693, loss: 2.8859592974185944, acc: 0.19274990260601044, test loss: 2.9103849792480467, test acc: 0.15599994361400604\n",
      "epoch: 694, loss: 2.8878536581993104, acc: 0.18870000541210175, test loss: 2.9033866107463835, test acc: 0.17315001785755157\n",
      "epoch: 695, loss: 2.8865537214279176, acc: 0.1941499561071396, test loss: 2.9030397081375123, test acc: 0.16989992558956146\n",
      "epoch: 696, loss: 2.885675560235977, acc: 0.19394999742507935, test loss: 2.9068951654434203, test acc: 0.15904997289180756\n",
      "epoch: 697, loss: 2.8873442900180817, acc: 0.19024990499019623, test loss: 2.9037172913551332, test acc: 0.16904999315738678\n",
      "epoch: 698, loss: 2.886168957948685, acc: 0.19229991734027863, test loss: 2.907972277402878, test acc: 0.15749996900558472\n",
      "epoch: 699, loss: 2.886585544347763, acc: 0.18994992971420288, test loss: 2.909336886405945, test acc: 0.15809999406337738\n",
      "epoch: 700, loss: 2.886758713722229, acc: 0.18899992108345032, test loss: 2.9056523597240447, test acc: 0.16360002756118774\n",
      "epoch: 701, loss: 2.8847595751285553, acc: 0.1944999098777771, test loss: 2.9064106130599976, test acc: 0.15149997174739838\n",
      "epoch: 702, loss: 2.887318377494812, acc: 0.18739992380142212, test loss: 2.9080301129817965, test acc: 0.15954998135566711\n",
      "epoch: 703, loss: 2.8866078305244445, acc: 0.1900499165058136, test loss: 2.9043462908267976, test acc: 0.1694999486207962\n",
      "epoch: 704, loss: 2.887170925140381, acc: 0.18934999406337738, test loss: 2.905296835899353, test acc: 0.1673000156879425\n",
      "epoch: 705, loss: 2.88475194811821, acc: 0.19564996659755707, test loss: 2.903280016183853, test acc: 0.1734999418258667\n",
      "epoch: 706, loss: 2.8865718877315523, acc: 0.1921999603509903, test loss: 2.9030851590633393, test acc: 0.16979999840259552\n",
      "epoch: 707, loss: 2.884745546579361, acc: 0.19584991037845612, test loss: 2.9031773936748504, test acc: 0.17264997959136963\n",
      "epoch: 708, loss: 2.8850384545326233, acc: 0.19389991462230682, test loss: 2.9037907886505128, test acc: 0.1682499647140503\n",
      "epoch: 709, loss: 2.8852747201919557, acc: 0.19224992394447327, test loss: 2.9081132566928862, test acc: 0.1457500010728836\n",
      "epoch: 710, loss: 2.886504783630371, acc: 0.19030000269412994, test loss: 2.9025022304058075, test acc: 0.17174997925758362\n",
      "epoch: 711, loss: 2.886604243516922, acc: 0.18664994835853577, test loss: 2.9027051532268526, test acc: 0.17239992320537567\n",
      "epoch: 712, loss: 2.885866129398346, acc: 0.19399994611740112, test loss: 2.9057948815822603, test acc: 0.1528499573469162\n",
      "epoch: 713, loss: 2.8897905063629152, acc: 0.18474993109703064, test loss: 2.910647900104523, test acc: 0.13814996182918549\n",
      "epoch: 714, loss: 2.885811234712601, acc: 0.18909990787506104, test loss: 2.9098986303806305, test acc: 0.15549999475479126\n",
      "epoch: 715, loss: 2.8857249331474306, acc: 0.19185000658035278, test loss: 2.9037216997146604, test acc: 0.15209998190402985\n",
      "epoch: 716, loss: 2.8882439517974854, acc: 0.18535000085830688, test loss: 2.9061861276626586, test acc: 0.1638999581336975\n",
      "epoch: 717, loss: 2.8849740183353423, acc: 0.192999929189682, test loss: 2.903232145309448, test acc: 0.15404996275901794\n",
      "epoch: 718, loss: 2.886778779029846, acc: 0.18634995818138123, test loss: 2.9048113787174223, test acc: 0.14819997549057007\n",
      "epoch: 719, loss: 2.8858589553833007, acc: 0.19239993393421173, test loss: 2.909457354545593, test acc: 0.15904994308948517\n",
      "epoch: 720, loss: 2.8867864632606506, acc: 0.1886499524116516, test loss: 2.904896994829178, test acc: 0.1487000286579132\n",
      "epoch: 721, loss: 2.8850862157344817, acc: 0.19384998083114624, test loss: 2.904727634191513, test acc: 0.1659499555826187\n",
      "epoch: 722, loss: 2.8870298361778257, acc: 0.18859992921352386, test loss: 2.905916197299957, test acc: 0.1625499576330185\n",
      "epoch: 723, loss: 2.8860043501853943, acc: 0.19164997339248657, test loss: 2.9019303977489472, test acc: 0.1753999888896942\n",
      "epoch: 724, loss: 2.885844258069992, acc: 0.19079992175102234, test loss: 2.905582057237625, test acc: 0.16599996387958527\n",
      "epoch: 725, loss: 2.8861447775363924, acc: 0.19114989042282104, test loss: 2.902323092222214, test acc: 0.17449992895126343\n",
      "epoch: 726, loss: 2.8846991837024687, acc: 0.19519995152950287, test loss: 2.9026225423812866, test acc: 0.17079994082450867\n",
      "epoch: 727, loss: 2.8851539087295532, acc: 0.19439992308616638, test loss: 2.912583578824997, test acc: 0.15909996628761292\n",
      "epoch: 728, loss: 2.8860909497737883, acc: 0.19099994003772736, test loss: 2.9074827241897583, test acc: 0.16249994933605194\n",
      "epoch: 729, loss: 2.8860150003433227, acc: 0.1886999011039734, test loss: 2.9026617777347563, test acc: 0.16979995369911194\n",
      "epoch: 730, loss: 2.8853172135353087, acc: 0.19444991648197174, test loss: 2.901949005126953, test acc: 0.17329993844032288\n",
      "epoch: 731, loss: 2.8848356223106384, acc: 0.1917000412940979, test loss: 2.91497655749321, test acc: 0.14654992520809174\n",
      "epoch: 732, loss: 2.886138585805893, acc: 0.19069990515708923, test loss: 2.9035822570323946, test acc: 0.1511499285697937\n",
      "epoch: 733, loss: 2.885685821771622, acc: 0.19274991750717163, test loss: 2.9050189769268036, test acc: 0.16329996287822723\n",
      "epoch: 734, loss: 2.8861166214942933, acc: 0.19199995696544647, test loss: 2.902696031332016, test acc: 0.1698000282049179\n",
      "epoch: 735, loss: 2.8870983362197875, acc: 0.18704994022846222, test loss: 2.9038563764095304, test acc: 0.15264996886253357\n",
      "epoch: 736, loss: 2.885204603672028, acc: 0.19290003180503845, test loss: 2.9010491693019866, test acc: 0.17670002579689026\n",
      "epoch: 737, loss: 2.8848737156391144, acc: 0.19255000352859497, test loss: 2.908113811016083, test acc: 0.15755000710487366\n",
      "epoch: 738, loss: 2.887613883018494, acc: 0.1871500015258789, test loss: 2.9039016830921174, test acc: 0.16839995980262756\n",
      "epoch: 739, loss: 2.8840692222118376, acc: 0.195099875330925, test loss: 2.9029932010173796, test acc: 0.16729995608329773\n",
      "epoch: 740, loss: 2.8852738428115843, acc: 0.19189997017383575, test loss: 2.9025405371189117, test acc: 0.15429998934268951\n",
      "epoch: 741, loss: 2.8841375482082365, acc: 0.1943999081850052, test loss: 2.902502957582474, test acc: 0.16874995827674866\n",
      "epoch: 742, loss: 2.885606127977371, acc: 0.19419996440410614, test loss: 2.9012509036064147, test acc: 0.17139989137649536\n",
      "epoch: 743, loss: 2.8852011120319365, acc: 0.19154991209506989, test loss: 2.9017874491214752, test acc: 0.17099998891353607\n",
      "epoch: 744, loss: 2.8856565392017366, acc: 0.19124992191791534, test loss: 2.9052039647102355, test acc: 0.1630999892950058\n",
      "epoch: 745, loss: 2.885712831020355, acc: 0.19029997289180756, test loss: 2.901399782896042, test acc: 0.17329996824264526\n",
      "epoch: 746, loss: 2.8841208624839783, acc: 0.19439998269081116, test loss: 2.9022373688220977, test acc: 0.17244993150234222\n",
      "epoch: 747, loss: 2.884345486164093, acc: 0.19294989109039307, test loss: 2.9021351814270018, test acc: 0.17239993810653687\n",
      "epoch: 748, loss: 2.8842164039611817, acc: 0.19439998269081116, test loss: 2.90811394572258, test acc: 0.15819993615150452\n",
      "epoch: 749, loss: 2.884492266178131, acc: 0.1918499767780304, test loss: 2.910284498929977, test acc: 0.14584995806217194\n",
      "epoch: 750, loss: 2.885437899827957, acc: 0.192799910902977, test loss: 2.902856855392456, test acc: 0.15209993720054626\n",
      "epoch: 751, loss: 2.885758092403412, acc: 0.1896999329328537, test loss: 2.9030001378059387, test acc: 0.16744999587535858\n",
      "epoch: 752, loss: 2.885536860227585, acc: 0.1903500109910965, test loss: 2.9019395577907563, test acc: 0.17159995436668396\n",
      "epoch: 753, loss: 2.8836287558078766, acc: 0.19789990782737732, test loss: 2.9046273267269136, test acc: 0.1699499487876892\n",
      "epoch: 754, loss: 2.8840198743343355, acc: 0.19534996151924133, test loss: 2.9035431134700773, test acc: 0.16984999179840088\n",
      "epoch: 755, loss: 2.884625345468521, acc: 0.19049996137619019, test loss: 2.9058442664146424, test acc: 0.16544994711875916\n",
      "epoch: 756, loss: 2.883213257789612, acc: 0.19909994304180145, test loss: 2.9029582929611206, test acc: 0.15379992127418518\n",
      "epoch: 757, loss: 2.8856538093090056, acc: 0.18894994258880615, test loss: 2.9084291195869447, test acc: 0.15579991042613983\n",
      "epoch: 758, loss: 2.8854782223701476, acc: 0.19164995849132538, test loss: 2.901806356906891, test acc: 0.17094998061656952\n",
      "epoch: 759, loss: 2.884278963804245, acc: 0.19454988837242126, test loss: 2.9028036081790924, test acc: 0.16969993710517883\n",
      "epoch: 760, loss: 2.8851912569999696, acc: 0.18954992294311523, test loss: 2.903258545398712, test acc: 0.15264996886253357\n",
      "epoch: 761, loss: 2.8868776142597197, acc: 0.18744991719722748, test loss: 2.9070292675495146, test acc: 0.16124996542930603\n",
      "epoch: 762, loss: 2.884997490644455, acc: 0.18984992802143097, test loss: 2.9087156438827515, test acc: 0.1563999354839325\n",
      "epoch: 763, loss: 2.885028986930847, acc: 0.19204989075660706, test loss: 2.907454842329025, test acc: 0.15924997627735138\n",
      "epoch: 764, loss: 2.8842694079875946, acc: 0.19174997508525848, test loss: 2.9018401777744294, test acc: 0.17209994792938232\n",
      "epoch: 765, loss: 2.884470224380493, acc: 0.1950000822544098, test loss: 2.9015010035037996, test acc: 0.1707499623298645\n",
      "epoch: 766, loss: 2.883303496837616, acc: 0.19819994270801544, test loss: 2.9013647484779357, test acc: 0.17469996213912964\n",
      "epoch: 767, loss: 2.8868222153186798, acc: 0.18744991719722748, test loss: 2.9089651036262514, test acc: 0.15589991211891174\n",
      "epoch: 768, loss: 2.8863859438896178, acc: 0.1888999193906784, test loss: 2.901279809474945, test acc: 0.17309994995594025\n",
      "epoch: 769, loss: 2.8841567277908324, acc: 0.19419994950294495, test loss: 2.902866464853287, test acc: 0.17429998517036438\n",
      "epoch: 770, loss: 2.8840879714488983, acc: 0.1951499730348587, test loss: 2.90359872341156, test acc: 0.16690002381801605\n",
      "epoch: 771, loss: 2.885524773597717, acc: 0.18894991278648376, test loss: 2.9025367856025697, test acc: 0.17114987969398499\n",
      "epoch: 772, loss: 2.884373095035553, acc: 0.19249993562698364, test loss: 2.902592500448227, test acc: 0.17069993913173676\n",
      "epoch: 773, loss: 2.887417550086975, acc: 0.18429994583129883, test loss: 2.9050190448760986, test acc: 0.14879998564720154\n",
      "epoch: 774, loss: 2.884115003347397, acc: 0.19364991784095764, test loss: 2.9055346095561982, test acc: 0.16459991037845612\n",
      "epoch: 775, loss: 2.8839737200737, acc: 0.19309993088245392, test loss: 2.902811017036438, test acc: 0.16799993813037872\n",
      "epoch: 776, loss: 2.8852349615097044, acc: 0.19134999811649323, test loss: 2.9018245196342467, test acc: 0.17149992287158966\n",
      "epoch: 777, loss: 2.8852048993110655, acc: 0.19099991023540497, test loss: 2.906347447633743, test acc: 0.16339991986751556\n",
      "epoch: 778, loss: 2.884146558046341, acc: 0.19350004196166992, test loss: 2.9036713147163393, test acc: 0.16899992525577545\n",
      "epoch: 779, loss: 2.8847787487506866, acc: 0.19284993410110474, test loss: 2.902755386829376, test acc: 0.16889993846416473\n",
      "epoch: 780, loss: 2.883752753734589, acc: 0.1967998743057251, test loss: 2.902422901391983, test acc: 0.1732499897480011\n",
      "epoch: 781, loss: 2.8839386904239657, acc: 0.19419994950294495, test loss: 2.9056449019908905, test acc: 0.15184997022151947\n",
      "epoch: 782, loss: 2.8855465233325956, acc: 0.1925499141216278, test loss: 2.901833734512329, test acc: 0.1723499745130539\n",
      "epoch: 783, loss: 2.88475589632988, acc: 0.190699964761734, test loss: 2.9100003004074098, test acc: 0.13739995658397675\n",
      "epoch: 784, loss: 2.8843357408046724, acc: 0.19164983928203583, test loss: 2.904025776386261, test acc: 0.14784997701644897\n",
      "epoch: 785, loss: 2.884296096563339, acc: 0.1933499574661255, test loss: 2.90386626124382, test acc: 0.1668500006198883\n",
      "epoch: 786, loss: 2.8829382932186127, acc: 0.1967499852180481, test loss: 2.9077804327011108, test acc: 0.16044993698596954\n",
      "epoch: 787, loss: 2.8855947971343996, acc: 0.19054996967315674, test loss: 2.9062032282352446, test acc: 0.14524996280670166\n",
      "epoch: 788, loss: 2.883527846336365, acc: 0.19239993393421173, test loss: 2.905842435359955, test acc: 0.14400002360343933\n",
      "epoch: 789, loss: 2.8828520607948303, acc: 0.1945999711751938, test loss: 2.9103882765769957, test acc: 0.13909995555877686\n",
      "epoch: 790, loss: 2.883734959363937, acc: 0.1937999129295349, test loss: 2.9053712987899782, test acc: 0.14709998667240143\n",
      "epoch: 791, loss: 2.8826881968975067, acc: 0.1959998905658722, test loss: 2.9047238874435424, test acc: 0.16314998269081116\n",
      "epoch: 792, loss: 2.8829308164119722, acc: 0.19694994390010834, test loss: 2.9030612111091614, test acc: 0.1698499470949173\n",
      "epoch: 793, loss: 2.8834980940818786, acc: 0.19444994628429413, test loss: 2.9058714389801024, test acc: 0.143249973654747\n",
      "epoch: 794, loss: 2.8820273780822756, acc: 0.19674986600875854, test loss: 2.9100227677822113, test acc: 0.15849997103214264\n",
      "epoch: 795, loss: 2.8860632038116454, acc: 0.1897999346256256, test loss: 2.904700754880905, test acc: 0.1656000018119812\n",
      "epoch: 796, loss: 2.88473699092865, acc: 0.1913500279188156, test loss: 2.9018122339248658, test acc: 0.16899995505809784\n",
      "epoch: 797, loss: 2.884214564561844, acc: 0.1922999620437622, test loss: 2.901876087188721, test acc: 0.17019996047019958\n",
      "epoch: 798, loss: 2.8846447682380676, acc: 0.1927500218153, test loss: 2.902990777492523, test acc: 0.16759993135929108\n",
      "epoch: 799, loss: 2.8836375057697294, acc: 0.1951499879360199, test loss: 2.9019965648651125, test acc: 0.17079994082450867\n",
      "epoch: 800, loss: 2.883600798845291, acc: 0.1951499581336975, test loss: 2.9020997738838195, test acc: 0.16844992339611053\n",
      "epoch: 801, loss: 2.883213086128235, acc: 0.1951999068260193, test loss: 2.9037148535251616, test acc: 0.14819994568824768\n",
      "epoch: 802, loss: 2.883070410490036, acc: 0.19669979810714722, test loss: 2.902682503461838, test acc: 0.1680999994277954\n",
      "epoch: 803, loss: 2.8834942507743837, acc: 0.19554990530014038, test loss: 2.902768701314926, test acc: 0.15119999647140503\n",
      "epoch: 804, loss: 2.883070003986359, acc: 0.19649991393089294, test loss: 2.901991069316864, test acc: 0.17354997992515564\n",
      "epoch: 805, loss: 2.884143068790436, acc: 0.19649991393089294, test loss: 2.9028277361392973, test acc: 0.15179993212223053\n",
      "epoch: 806, loss: 2.88514163851738, acc: 0.19064994156360626, test loss: 2.905245471000671, test acc: 0.14879994094371796\n",
      "epoch: 807, loss: 2.8839220547676088, acc: 0.19359995424747467, test loss: 2.9034688210487367, test acc: 0.16744999587535858\n",
      "epoch: 808, loss: 2.8857476544380187, acc: 0.18849998712539673, test loss: 2.902276266813278, test acc: 0.16974999010562897\n",
      "epoch: 809, loss: 2.884362728595734, acc: 0.19364990293979645, test loss: 2.9017022895812987, test acc: 0.16859999299049377\n",
      "epoch: 810, loss: 2.8837882328033446, acc: 0.19284984469413757, test loss: 2.905507379770279, test acc: 0.16179998219013214\n",
      "epoch: 811, loss: 2.8836460304260254, acc: 0.19504998624324799, test loss: 2.902592748403549, test acc: 0.15024994313716888\n",
      "epoch: 812, loss: 2.884162092208862, acc: 0.19269996881484985, test loss: 2.9017781126499176, test acc: 0.17264993488788605\n",
      "epoch: 813, loss: 2.8834314560890197, acc: 0.19454997777938843, test loss: 2.9023613047599794, test acc: 0.16864989697933197\n",
      "epoch: 814, loss: 2.884370403289795, acc: 0.19129996001720428, test loss: 2.9047405993938447, test acc: 0.16639995574951172\n",
      "epoch: 815, loss: 2.884148778915405, acc: 0.19349999725818634, test loss: 2.9046729969978333, test acc: 0.16350001096725464\n",
      "epoch: 816, loss: 2.882726469039917, acc: 0.19534990191459656, test loss: 2.902664374113083, test acc: 0.16909992694854736\n",
      "epoch: 817, loss: 2.8836593198776246, acc: 0.1921999603509903, test loss: 2.9060992109775543, test acc: 0.149199977517128\n",
      "epoch: 818, loss: 2.88222554564476, acc: 0.19735001027584076, test loss: 2.9055519950389863, test acc: 0.1298999935388565\n",
      "epoch: 819, loss: 2.8836093938350675, acc: 0.19300003349781036, test loss: 2.9013098669052124, test acc: 0.16999995708465576\n",
      "epoch: 820, loss: 2.88397611618042, acc: 0.19189991056919098, test loss: 2.9028961992263795, test acc: 0.16669997572898865\n",
      "epoch: 821, loss: 2.8819153356552123, acc: 0.19774989783763885, test loss: 2.902009869813919, test acc: 0.17439992725849152\n",
      "epoch: 822, loss: 2.8833850765228273, acc: 0.19550001621246338, test loss: 2.904909077882767, test acc: 0.15139994025230408\n",
      "epoch: 823, loss: 2.8839473843574526, acc: 0.1925499141216278, test loss: 2.9050920426845552, test acc: 0.1632000207901001\n",
      "epoch: 824, loss: 2.8820895171165466, acc: 0.19624994695186615, test loss: 2.904815809726715, test acc: 0.14824995398521423\n",
      "epoch: 825, loss: 2.8837140679359434, acc: 0.192999929189682, test loss: 2.905142434835434, test acc: 0.16549994051456451\n",
      "epoch: 826, loss: 2.8845281159877776, acc: 0.19169996678829193, test loss: 2.9026419830322268, test acc: 0.16614991426467896\n",
      "epoch: 827, loss: 2.883183227777481, acc: 0.19244998693466187, test loss: 2.9149968886375426, test acc: 0.14229996502399445\n",
      "epoch: 828, loss: 2.8841192650794985, acc: 0.19374991953372955, test loss: 2.9029019486904146, test acc: 0.1669999361038208\n",
      "epoch: 829, loss: 2.8848503792285918, acc: 0.1911499947309494, test loss: 2.9034824061393736, test acc: 0.16599993407726288\n",
      "epoch: 830, loss: 2.8811808955669402, acc: 0.1973000019788742, test loss: 2.9016463327407838, test acc: 0.17354997992515564\n",
      "epoch: 831, loss: 2.883945630788803, acc: 0.19204992055892944, test loss: 2.9180416643619536, test acc: 0.12965001165866852\n",
      "epoch: 832, loss: 2.8837637317180635, acc: 0.19325006008148193, test loss: 2.9025990676879885, test acc: 0.1675499677658081\n",
      "epoch: 833, loss: 2.8818187630176544, acc: 0.1994999647140503, test loss: 2.9070000755786896, test acc: 0.16159996390342712\n",
      "epoch: 834, loss: 2.882625848054886, acc: 0.1945999711751938, test loss: 2.908795051574707, test acc: 0.13894999027252197\n",
      "epoch: 835, loss: 2.8840431237220763, acc: 0.1918998658657074, test loss: 2.904970144033432, test acc: 0.14639995992183685\n",
      "epoch: 836, loss: 2.883095029592514, acc: 0.19614994525909424, test loss: 2.9014044153690337, test acc: 0.1690499186515808\n",
      "epoch: 837, loss: 2.8820530092716217, acc: 0.19649995863437653, test loss: 2.9049290478229524, test acc: 0.14709997177124023\n",
      "epoch: 838, loss: 2.885178036689758, acc: 0.1882498562335968, test loss: 2.902014844417572, test acc: 0.16809992492198944\n",
      "epoch: 839, loss: 2.882236713171005, acc: 0.19494988024234772, test loss: 2.9055988502502443, test acc: 0.14350000023841858\n",
      "epoch: 840, loss: 2.882365295886993, acc: 0.19404996931552887, test loss: 2.9009462559223174, test acc: 0.1724499762058258\n",
      "epoch: 841, loss: 2.882117499113083, acc: 0.1943998783826828, test loss: 2.9027200257778167, test acc: 0.15074993669986725\n",
      "epoch: 842, loss: 2.884319678544998, acc: 0.19294996559619904, test loss: 2.902377436161041, test acc: 0.17004993557929993\n",
      "epoch: 843, loss: 2.8830787444114687, acc: 0.19604995846748352, test loss: 2.9047311985492708, test acc: 0.14854995906352997\n",
      "epoch: 844, loss: 2.882236503362656, acc: 0.19389989972114563, test loss: 2.9017766761779784, test acc: 0.16684995591640472\n",
      "epoch: 845, loss: 2.883753254413605, acc: 0.19199997186660767, test loss: 2.9024190390110016, test acc: 0.16639995574951172\n",
      "epoch: 846, loss: 2.882628318071365, acc: 0.19709986448287964, test loss: 2.902454639673233, test acc: 0.1682499647140503\n",
      "epoch: 847, loss: 2.881951643228531, acc: 0.19654995203018188, test loss: 2.9014608466625216, test acc: 0.16799995303153992\n",
      "epoch: 848, loss: 2.883056082725525, acc: 0.19399988651275635, test loss: 2.900508989095688, test acc: 0.1702999323606491\n",
      "epoch: 849, loss: 2.8828404641151426, acc: 0.19504985213279724, test loss: 2.9018917965888975, test acc: 0.17305001616477966\n",
      "epoch: 850, loss: 2.884553415775299, acc: 0.18915000557899475, test loss: 2.9022704696655275, test acc: 0.16759997606277466\n",
      "epoch: 851, loss: 2.8810314881801604, acc: 0.19794993102550507, test loss: 2.9015012884140017, test acc: 0.17089995741844177\n",
      "epoch: 852, loss: 2.8848989748954774, acc: 0.19084997475147247, test loss: 2.9011038625240326, test acc: 0.17434993386268616\n",
      "epoch: 853, loss: 2.88356761097908, acc: 0.1924499273300171, test loss: 2.905933064222336, test acc: 0.15999993681907654\n",
      "epoch: 854, loss: 2.8823425006866454, acc: 0.1963498294353485, test loss: 2.9032133901119233, test acc: 0.1654999703168869\n",
      "epoch: 855, loss: 2.881867823600769, acc: 0.19614991545677185, test loss: 2.901118700504303, test acc: 0.17059998214244843\n",
      "epoch: 856, loss: 2.8819985139369964, acc: 0.19669990241527557, test loss: 2.9035230720043184, test acc: 0.1657000184059143\n",
      "epoch: 857, loss: 2.8811688411235807, acc: 0.19439998269081116, test loss: 2.904624866247177, test acc: 0.1634499579668045\n",
      "epoch: 858, loss: 2.8831415116786956, acc: 0.19209997355937958, test loss: 2.913507171869278, test acc: 0.1350000500679016\n",
      "epoch: 859, loss: 2.883955692052841, acc: 0.1912999004125595, test loss: 2.9077601981163026, test acc: 0.15780000388622284\n",
      "epoch: 860, loss: 2.8819168436527254, acc: 0.1953999549150467, test loss: 2.9022292959690095, test acc: 0.16770000755786896\n",
      "epoch: 861, loss: 2.882508397102356, acc: 0.1937999725341797, test loss: 2.9005699300765992, test acc: 0.17584995925426483\n",
      "epoch: 862, loss: 2.883127980232239, acc: 0.19439996778964996, test loss: 2.9033962118625642, test acc: 0.16564995050430298\n",
      "epoch: 863, loss: 2.8824056029319762, acc: 0.19569995999336243, test loss: 2.900535727739334, test acc: 0.17159995436668396\n",
      "epoch: 864, loss: 2.8815736210346223, acc: 0.19589994847774506, test loss: 2.9043224155902863, test acc: 0.16599993407726288\n",
      "epoch: 865, loss: 2.8836548709869385, acc: 0.19069990515708923, test loss: 2.910655964612961, test acc: 0.1483999788761139\n",
      "epoch: 866, loss: 2.880905727148056, acc: 0.19779998064041138, test loss: 2.9046074533462525, test acc: 0.1634499728679657\n",
      "epoch: 867, loss: 2.8809141373634337, acc: 0.19684994220733643, test loss: 2.901932854652405, test acc: 0.16959995031356812\n",
      "epoch: 868, loss: 2.883876464366913, acc: 0.19240005314350128, test loss: 2.9038371801376344, test acc: 0.16439998149871826\n",
      "epoch: 869, loss: 2.884723719358444, acc: 0.1879999041557312, test loss: 2.904890856742859, test acc: 0.16079996526241302\n",
      "epoch: 870, loss: 2.88206466794014, acc: 0.194149911403656, test loss: 2.901460562944412, test acc: 0.16849994659423828\n",
      "epoch: 871, loss: 2.8835075378417967, acc: 0.19104993343353271, test loss: 2.903937820196152, test acc: 0.16359996795654297\n",
      "epoch: 872, loss: 2.8833447217941286, acc: 0.19054993987083435, test loss: 2.902027641534805, test acc: 0.16649997234344482\n",
      "epoch: 873, loss: 2.8814596939086914, acc: 0.1958499401807785, test loss: 2.9037405717372895, test acc: 0.16614995896816254\n",
      "epoch: 874, loss: 2.8814319443702696, acc: 0.19624994695186615, test loss: 2.906260361671448, test acc: 0.16154995560646057\n",
      "epoch: 875, loss: 2.88294704079628, acc: 0.19249998033046722, test loss: 2.901829591989517, test acc: 0.1719999462366104\n",
      "epoch: 876, loss: 2.883471734523773, acc: 0.19054992496967316, test loss: 2.901938486099243, test acc: 0.16754993796348572\n",
      "epoch: 877, loss: 2.882144285440445, acc: 0.19394990801811218, test loss: 2.9012450432777404, test acc: 0.17155000567436218\n",
      "epoch: 878, loss: 2.8824843537807463, acc: 0.19319990277290344, test loss: 2.9020608246326445, test acc: 0.16684997081756592\n",
      "epoch: 879, loss: 2.8827472245693206, acc: 0.1933499574661255, test loss: 2.9053904068470002, test acc: 0.14289996027946472\n",
      "epoch: 880, loss: 2.882198451757431, acc: 0.1952999085187912, test loss: 2.902256804704666, test acc: 0.16644996404647827\n",
      "epoch: 881, loss: 2.8828399658203123, acc: 0.19169990718364716, test loss: 2.9017067527770997, test acc: 0.16714997589588165\n",
      "epoch: 882, loss: 2.884890251159668, acc: 0.18880000710487366, test loss: 2.9109565019607544, test acc: 0.15339994430541992\n",
      "epoch: 883, loss: 2.8837967348098754, acc: 0.19104993343353271, test loss: 2.9025521087646484, test acc: 0.1681499481201172\n",
      "epoch: 884, loss: 2.881718027591705, acc: 0.19539998471736908, test loss: 2.9011306166648865, test acc: 0.17204990983009338\n",
      "epoch: 885, loss: 2.8820820748806, acc: 0.1934499591588974, test loss: 2.9027193546295167, test acc: 0.14769993722438812\n",
      "epoch: 886, loss: 2.880815589427948, acc: 0.19524991512298584, test loss: 2.901689524650574, test acc: 0.16944998502731323\n",
      "epoch: 887, loss: 2.882068589925766, acc: 0.19639995694160461, test loss: 2.906861296892166, test acc: 0.1466999351978302\n",
      "epoch: 888, loss: 2.8831635677814482, acc: 0.19204996526241302, test loss: 2.903930290937424, test acc: 0.1638999730348587\n",
      "epoch: 889, loss: 2.8825493466854097, acc: 0.1946999579668045, test loss: 2.9013445484638214, test acc: 0.16909995675086975\n",
      "epoch: 890, loss: 2.880645651817322, acc: 0.19844989478588104, test loss: 2.9043168795108794, test acc: 0.1464499533176422\n",
      "epoch: 891, loss: 2.883528908491135, acc: 0.1911998987197876, test loss: 2.9013408601284025, test acc: 0.17039991915225983\n",
      "epoch: 892, loss: 2.8838270366191865, acc: 0.19159990549087524, test loss: 2.901224335432053, test acc: 0.1689499467611313\n",
      "epoch: 893, loss: 2.8843227326869965, acc: 0.19109994173049927, test loss: 2.905125266313553, test acc: 0.1512499302625656\n",
      "epoch: 894, loss: 2.882992831468582, acc: 0.19289997220039368, test loss: 2.9008753478527067, test acc: 0.1706499457359314\n",
      "epoch: 895, loss: 2.8832544779777525, acc: 0.19459988176822662, test loss: 2.9145625150203704, test acc: 0.1446000039577484\n",
      "epoch: 896, loss: 2.8822145044803618, acc: 0.192999929189682, test loss: 2.9027243757247927, test acc: 0.16569995880126953\n",
      "epoch: 897, loss: 2.8815813207626344, acc: 0.1950499713420868, test loss: 2.9009213840961454, test acc: 0.1722499430179596\n",
      "epoch: 898, loss: 2.882536725997925, acc: 0.19364997744560242, test loss: 2.9022346258163454, test acc: 0.1675499528646469\n",
      "epoch: 899, loss: 2.8806534028053283, acc: 0.1968999058008194, test loss: 2.906806560754776, test acc: 0.1578499972820282\n",
      "epoch: 900, loss: 2.882356050014496, acc: 0.19344988465309143, test loss: 2.9047086453437805, test acc: 0.1498999148607254\n",
      "epoch: 901, loss: 2.8818030440807343, acc: 0.19414997100830078, test loss: 2.9027504658699037, test acc: 0.17034994065761566\n",
      "epoch: 902, loss: 2.8807766234874723, acc: 0.19854997098445892, test loss: 2.9024722635746003, test acc: 0.1476999670267105\n",
      "epoch: 903, loss: 2.887118079662323, acc: 0.18534992635250092, test loss: 2.9051408898830413, test acc: 0.15029987692832947\n",
      "epoch: 904, loss: 2.8818859577178957, acc: 0.19454994797706604, test loss: 2.901112110614777, test acc: 0.17124994099140167\n",
      "epoch: 905, loss: 2.881643043756485, acc: 0.19374999403953552, test loss: 2.901349141597748, test acc: 0.1685999035835266\n",
      "epoch: 906, loss: 2.884094225168228, acc: 0.19139985740184784, test loss: 2.9024675250053407, test acc: 0.1654999703168869\n",
      "epoch: 907, loss: 2.8822045731544494, acc: 0.1935499757528305, test loss: 2.901968637704849, test acc: 0.16710001230239868\n",
      "epoch: 908, loss: 2.8822958159446714, acc: 0.19465002417564392, test loss: 2.901478587388992, test acc: 0.16969995200634003\n",
      "epoch: 909, loss: 2.8818224561214447, acc: 0.19364988803863525, test loss: 2.902270884513855, test acc: 0.17224997282028198\n",
      "epoch: 910, loss: 2.8822742879390715, acc: 0.19274990260601044, test loss: 2.9047882533073426, test acc: 0.16339999437332153\n",
      "epoch: 911, loss: 2.8836603713035585, acc: 0.19120000302791595, test loss: 2.900698838233948, test acc: 0.1707998812198639\n",
      "epoch: 912, loss: 2.882811005115509, acc: 0.19214992225170135, test loss: 2.9091696333885193, test acc: 0.15434999763965607\n",
      "epoch: 913, loss: 2.8843394827842714, acc: 0.18874992430210114, test loss: 2.9067766845226286, test acc: 0.1561499536037445\n",
      "epoch: 914, loss: 2.8850283777713774, acc: 0.18724992871284485, test loss: 2.9033117389678953, test acc: 0.14839999377727509\n",
      "epoch: 915, loss: 2.880612373352051, acc: 0.19569995999336243, test loss: 2.9028849637508394, test acc: 0.1508999466896057\n",
      "epoch: 916, loss: 2.884242341518402, acc: 0.19099989533424377, test loss: 2.907413047552109, test acc: 0.12709994614124298\n",
      "epoch: 917, loss: 2.8821139788627623, acc: 0.19394998252391815, test loss: 2.905549638271332, test acc: 0.15029992163181305\n",
      "epoch: 918, loss: 2.882644211053848, acc: 0.19524991512298584, test loss: 2.919574658870697, test acc: 0.14444999396800995\n",
      "epoch: 919, loss: 2.8820461583137513, acc: 0.19539998471736908, test loss: 2.9057330536842345, test acc: 0.14459997415542603\n",
      "epoch: 920, loss: 2.8828244149684905, acc: 0.1899000108242035, test loss: 2.901713664531708, test acc: 0.1689499467611313\n",
      "epoch: 921, loss: 2.879919068813324, acc: 0.19919995963573456, test loss: 2.9011224257946013, test acc: 0.16874998807907104\n",
      "epoch: 922, loss: 2.8815386283397673, acc: 0.195949986577034, test loss: 2.904471055269241, test acc: 0.14684996008872986\n",
      "epoch: 923, loss: 2.883461365699768, acc: 0.19174990057945251, test loss: 2.9002864849567414, test acc: 0.17219993472099304\n",
      "epoch: 924, loss: 2.8820583426952364, acc: 0.19484998285770416, test loss: 2.903310194015503, test acc: 0.16384996473789215\n",
      "epoch: 925, loss: 2.8820567321777344, acc: 0.19404985010623932, test loss: 2.9099687969684602, test acc: 0.1564999520778656\n",
      "epoch: 926, loss: 2.8810755145549773, acc: 0.19670003652572632, test loss: 2.9084647500514986, test acc: 0.15859998762607574\n",
      "epoch: 927, loss: 2.8826431953907012, acc: 0.19460001587867737, test loss: 2.9072178292274473, test acc: 0.16039995849132538\n",
      "epoch: 928, loss: 2.8840470016002655, acc: 0.18869997560977936, test loss: 2.9033895039558413, test acc: 0.16999998688697815\n",
      "epoch: 929, loss: 2.8828546142578126, acc: 0.1933499276638031, test loss: 2.900707142353058, test acc: 0.17149990797042847\n",
      "epoch: 930, loss: 2.8816377866268157, acc: 0.19684991240501404, test loss: 2.900894731283188, test acc: 0.1692499965429306\n",
      "epoch: 931, loss: 2.8815201437473297, acc: 0.1959499567747116, test loss: 2.9037511110305787, test acc: 0.14904999732971191\n",
      "epoch: 932, loss: 2.8845662343502045, acc: 0.19164997339248657, test loss: 2.902813707590103, test acc: 0.16634996235370636\n",
      "epoch: 933, loss: 2.881638914346695, acc: 0.19589996337890625, test loss: 2.9009372544288636, test acc: 0.16895000636577606\n",
      "epoch: 934, loss: 2.881386046409607, acc: 0.1931498944759369, test loss: 2.901942331790924, test acc: 0.1632000356912613\n",
      "epoch: 935, loss: 2.8799992382526396, acc: 0.19814981520175934, test loss: 2.9008217704296113, test acc: 0.17219993472099304\n",
      "epoch: 936, loss: 2.8796520936489105, acc: 0.1972000002861023, test loss: 2.9019938945770263, test acc: 0.1700998842716217\n",
      "epoch: 937, loss: 2.881355210542679, acc: 0.1944999098777771, test loss: 2.901500633955002, test acc: 0.168349951505661\n",
      "epoch: 938, loss: 2.88119243144989, acc: 0.1955999881029129, test loss: 2.9043808233737947, test acc: 0.14594998955726624\n",
      "epoch: 939, loss: 2.881337592601776, acc: 0.19554990530014038, test loss: 2.901201810836792, test acc: 0.16884998977184296\n",
      "epoch: 940, loss: 2.880756661891937, acc: 0.19529989361763, test loss: 2.9030654335021975, test acc: 0.1682000309228897\n",
      "epoch: 941, loss: 2.8824142169952394, acc: 0.19199997186660767, test loss: 2.9028876781463624, test acc: 0.16409999132156372\n",
      "epoch: 942, loss: 2.8831113147735596, acc: 0.19104993343353271, test loss: 2.9021173882484437, test acc: 0.14939993619918823\n",
      "epoch: 943, loss: 2.8802914917469025, acc: 0.198699951171875, test loss: 2.9026238358020784, test acc: 0.16544996201992035\n",
      "epoch: 944, loss: 2.883554401397705, acc: 0.1910499632358551, test loss: 2.9061020946502687, test acc: 0.15879997611045837\n",
      "epoch: 945, loss: 2.883862444162369, acc: 0.18925006687641144, test loss: 2.902395771741867, test acc: 0.16724996268749237\n",
      "epoch: 946, loss: 2.8802464282512665, acc: 0.19669997692108154, test loss: 2.9028195405006407, test acc: 0.16884995996952057\n",
      "epoch: 947, loss: 2.881430745124817, acc: 0.1949499249458313, test loss: 2.9005059587955473, test acc: 0.17079994082450867\n",
      "epoch: 948, loss: 2.880431475639343, acc: 0.19529996812343597, test loss: 2.904408082962036, test acc: 0.1646999716758728\n",
      "epoch: 949, loss: 2.8803086030483245, acc: 0.19569997489452362, test loss: 2.900776129961014, test acc: 0.1709999144077301\n",
      "epoch: 950, loss: 2.8802552437782287, acc: 0.19719989597797394, test loss: 2.9020374584198, test acc: 0.1665000170469284\n",
      "epoch: 951, loss: 2.88146537065506, acc: 0.19669987261295319, test loss: 2.903278422355652, test acc: 0.16439996659755707\n",
      "epoch: 952, loss: 2.8803970968723296, acc: 0.1984999030828476, test loss: 2.904899821281433, test acc: 0.16359995305538177\n",
      "epoch: 953, loss: 2.883543280363083, acc: 0.19024990499019623, test loss: 2.900723856687546, test acc: 0.16974994540214539\n",
      "epoch: 954, loss: 2.882040559053421, acc: 0.19214992225170135, test loss: 2.906450629234314, test acc: 0.16249994933605194\n",
      "epoch: 955, loss: 2.880691797733307, acc: 0.19670000672340393, test loss: 2.903689223527908, test acc: 0.14854994416236877\n",
      "epoch: 956, loss: 2.8804269897937775, acc: 0.19509996473789215, test loss: 2.9010960340499876, test acc: 0.16919992864131927\n",
      "epoch: 957, loss: 2.8831334173679353, acc: 0.1929999142885208, test loss: 2.9013884687423706, test acc: 0.1658499836921692\n",
      "epoch: 958, loss: 2.879950993061066, acc: 0.1978999525308609, test loss: 2.899687385559082, test acc: 0.17039993405342102\n",
      "epoch: 959, loss: 2.8806057059764862, acc: 0.19649995863437653, test loss: 2.902174650430679, test acc: 0.16544997692108154\n",
      "epoch: 960, loss: 2.8826239216327667, acc: 0.19244986772537231, test loss: 2.906781519651413, test acc: 0.1577499657869339\n",
      "epoch: 961, loss: 2.8813033330440523, acc: 0.19434991478919983, test loss: 2.9045312345027923, test acc: 0.1622999906539917\n",
      "epoch: 962, loss: 2.880108917951584, acc: 0.19324995577335358, test loss: 2.9022347033023834, test acc: 0.16600002348423004\n",
      "epoch: 963, loss: 2.879877163171768, acc: 0.19669991731643677, test loss: 2.9033142483234404, test acc: 0.16704995930194855\n",
      "epoch: 964, loss: 2.881050350666046, acc: 0.1950499564409256, test loss: 2.9044780695438384, test acc: 0.16124993562698364\n",
      "epoch: 965, loss: 2.8815217030048372, acc: 0.19494996964931488, test loss: 2.902100706100464, test acc: 0.17100000381469727\n",
      "epoch: 966, loss: 2.880975909233093, acc: 0.19609984755516052, test loss: 2.9017206382751466, test acc: 0.1683499962091446\n",
      "epoch: 967, loss: 2.8821415209770205, acc: 0.19189997017383575, test loss: 2.900970845222473, test acc: 0.16864997148513794\n",
      "epoch: 968, loss: 2.8806920611858366, acc: 0.19784989953041077, test loss: 2.904510564804077, test acc: 0.16290001571178436\n",
      "epoch: 969, loss: 2.8821674394607544, acc: 0.19165000319480896, test loss: 2.8998951733112337, test acc: 0.17124995589256287\n",
      "epoch: 970, loss: 2.8808707571029664, acc: 0.19464987516403198, test loss: 2.900046011209488, test acc: 0.1702999472618103\n",
      "epoch: 971, loss: 2.879253829717636, acc: 0.1998499482870102, test loss: 2.9018175280094147, test acc: 0.16724994778633118\n",
      "epoch: 972, loss: 2.881821657419205, acc: 0.1891498565673828, test loss: 2.9009913742542266, test acc: 0.1722499281167984\n",
      "epoch: 973, loss: 2.8798757922649383, acc: 0.19904987514019012, test loss: 2.9067461276054383, test acc: 0.15779995918273926\n",
      "epoch: 974, loss: 2.880091218948364, acc: 0.19995000958442688, test loss: 2.9027371680736542, test acc: 0.16585001349449158\n",
      "epoch: 975, loss: 2.880497370958328, acc: 0.19639995694160461, test loss: 2.903656425476074, test acc: 0.16479997336864471\n",
      "epoch: 976, loss: 2.8801240956783296, acc: 0.19614994525909424, test loss: 2.9055157470703126, test acc: 0.14225000143051147\n",
      "epoch: 977, loss: 2.879109171628952, acc: 0.19954995810985565, test loss: 2.9008882999420167, test acc: 0.16914993524551392\n",
      "epoch: 978, loss: 2.8814874029159547, acc: 0.19304998219013214, test loss: 2.905784025192261, test acc: 0.129799947142601\n",
      "epoch: 979, loss: 2.880485143661499, acc: 0.19584989547729492, test loss: 2.9034959149360655, test acc: 0.16414996981620789\n",
      "epoch: 980, loss: 2.8856433713436127, acc: 0.1860499382019043, test loss: 2.912103223800659, test acc: 0.1528499722480774\n",
      "epoch: 981, loss: 2.8814693057537077, acc: 0.19499997794628143, test loss: 2.9017120838165282, test acc: 0.1712999939918518\n",
      "epoch: 982, loss: 2.881385471820831, acc: 0.19189997017383575, test loss: 2.9002465796470642, test acc: 0.1694999635219574\n",
      "epoch: 983, loss: 2.8801211428642275, acc: 0.19709987938404083, test loss: 2.912609598636627, test acc: 0.13989996910095215\n",
      "epoch: 984, loss: 2.881384904384613, acc: 0.1962999552488327, test loss: 2.9007866764068604, test acc: 0.1719999462366104\n",
      "epoch: 985, loss: 2.880138839483261, acc: 0.19659996032714844, test loss: 2.9024846506118775, test acc: 0.16509994864463806\n",
      "epoch: 986, loss: 2.8810542106628416, acc: 0.19444994628429413, test loss: 2.9009903478622436, test acc: 0.16844996809959412\n",
      "epoch: 987, loss: 2.880147124528885, acc: 0.19644995033740997, test loss: 2.901661647558212, test acc: 0.1654999554157257\n",
      "epoch: 988, loss: 2.8826462733745575, acc: 0.1919499635696411, test loss: 2.900225011110306, test acc: 0.1706499308347702\n",
      "epoch: 989, loss: 2.8794558441638944, acc: 0.19614994525909424, test loss: 2.900009974241257, test acc: 0.17090000212192535\n",
      "epoch: 990, loss: 2.8794981706142426, acc: 0.19829994440078735, test loss: 2.9054062747955323, test acc: 0.16074995696544647\n",
      "epoch: 991, loss: 2.8810254096984864, acc: 0.19625000655651093, test loss: 2.9031010460853577, test acc: 0.1472499817609787\n",
      "epoch: 992, loss: 2.8800487124919893, acc: 0.19784997403621674, test loss: 2.9005065751075745, test acc: 0.1717999428510666\n",
      "epoch: 993, loss: 2.8804830396175385, acc: 0.1959499716758728, test loss: 2.9004265451431275, test acc: 0.17080001533031464\n",
      "epoch: 994, loss: 2.8809281957149504, acc: 0.1942499279975891, test loss: 2.9006130015850067, test acc: 0.1709999442100525\n",
      "epoch: 995, loss: 2.8807365953922273, acc: 0.1959998905658722, test loss: 2.905756367444992, test acc: 0.1606999784708023\n",
      "epoch: 996, loss: 2.8814013135433196, acc: 0.1933499574661255, test loss: 2.9058872854709623, test acc: 0.1580999791622162\n",
      "epoch: 997, loss: 2.8795120310783386, acc: 0.19994999468326569, test loss: 2.900945323705673, test acc: 0.16889995336532593\n",
      "epoch: 998, loss: 2.880048838853836, acc: 0.19635002315044403, test loss: 2.9033578300476073, test acc: 0.16384996473789215\n",
      "epoch: 999, loss: 2.8813804316520693, acc: 0.19384998083114624, test loss: 2.901546049118042, test acc: 0.16635000705718994\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(output, dim=1)      \n",
    "        targets = torch.argmax(targets, dim=1) \n",
    "        running_acc += torch.mean(pred.eq(targets).float().cpu()) \n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader)   \n",
    "    running_acc /= len(train_loader)    \n",
    "    train_losses.append(running_loss)\n",
    "    train_accs.append(running_acc)\n",
    "    #\n",
    "    #   test loop\n",
    "    #\n",
    "    test_running_loss = 0.0\n",
    "    test_running_acc = 0.0\n",
    "    for test_inputs, test_targets in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        test_output = model(test_inputs)\n",
    "        test_loss = criterion(test_output, test_targets)\n",
    "        test_running_loss += test_loss.item()\n",
    "        test_pred = torch.argmax(test_output, dim=1)      \n",
    "        test_targets = torch.argmax(test_targets, dim=1)  \n",
    "        test_running_acc += torch.mean(test_pred.eq(test_targets).float().cpu()) \n",
    "    test_running_loss /= len(test_loader)   \n",
    "    test_running_acc /= len(test_loader)    \n",
    "    test_losses.append(test_running_loss)\n",
    "    test_accs.append(test_running_acc)\n",
    "        \n",
    "    print(\"epoch: {}, loss: {}, acc: {}, test loss: {}, test acc: {}\".format(epoch, running_loss, running_acc, test_running_loss, test_running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data storage/prm_data_L16_FCNN_Nh3.pth\n"
     ]
    }
   ],
   "source": [
    "Ising_size = int(np.sqrt(input_size))\n",
    "class_name = type(model).__name__\n",
    "\n",
    "\n",
    "file_path = f'../data storage/prm_data_L{Ising_size}_{class_name}_Nh{hidden_size}.pth'\n",
    "print(file_path)\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
