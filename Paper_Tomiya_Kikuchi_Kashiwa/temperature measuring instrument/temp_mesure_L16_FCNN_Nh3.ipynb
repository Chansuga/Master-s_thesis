{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cudaが使えるか確認\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUで作ったPickleのファイルは，CPUではそのままでは使えないため，工夫が必要．\n",
    "\n",
    "https://www.kunita-gamefactory.com/post/%E3%80%90pytorch%E3%80%91gpu%E3%81%A7%E8%A8%93%E7%B7%B4%E3%81%95%E3%81%9B%E3%81%9F%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92cpu%E3%81%A7%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%82%82%E3%81%86%E3%81%A8%E3%81%97%E3%81%9F%E3%81%8A%E8%A9%B1\n",
    "\n",
    "を真似したらうまくCPU上でもファイルを読み込むことができるようになった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "        \n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの相対パスを指定\n",
    "file_path = '../data storage/Ising_data_L16_v3.pkl'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "else:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = CPU_Unpickler(file).load()\n",
    "        \n",
    "# 読み込んだデータを個々の変数に分割\n",
    "spin_data, label_data = loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 磁化を計算\n",
    "# def magnetization(state):\n",
    "#     return np.mean(state)\n",
    "\n",
    "# for i in range(len(spin_data)):\n",
    "#     mag = magnetization(spin_data[i])\n",
    "#     if mag > 0:\n",
    "#         spin_data[i] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストをNumPy配列に変換\n",
    "spin_data_np = np.array(spin_data)\n",
    "label_data_np = np.array(label_data)\n",
    "\n",
    "# NumPy配列をPyTorchテンソルに変換\n",
    "spin_data_tensor = torch.from_numpy(spin_data_np)\n",
    "label_data_tensor = torch.from_numpy(label_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータを訓練用とテスト用に分割(5:5)\n",
    "spin_train, spin_test, label_train, label_test = train_test_split(spin_data_tensor, label_data_tensor, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soken\\AppData\\Local\\Temp\\ipykernel_15912\\168836570.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
      "C:\\Users\\soken\\AppData\\Local\\Temp\\ipykernel_15912\\168836570.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
      "C:\\Users\\soken\\AppData\\Local\\Temp\\ipykernel_15912\\168836570.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train, dtype=torch.float32)\n",
      "C:\\Users\\soken\\AppData\\Local\\Temp\\ipykernel_15912\\168836570.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_test = torch.tensor(label_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# PyTorchのテンソルに変換\n",
    "spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
    "spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
    "label_train = torch.tensor(label_train, dtype=torch.float32)  \n",
    "label_test = torch.tensor(label_test, dtype=torch.float32)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解データはone-hot表現にする必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 20])\n",
      "torch.Size([20000, 20])\n"
     ]
    }
   ],
   "source": [
    "# テンソルを新しいテンソルに変換する関数を定義\n",
    "def to_one_hot(data, num_classes=20):\n",
    "    # one-hotベクトルの初期化\n",
    "    one_hot = torch.zeros(len(data), num_classes)\n",
    "    print(one_hot.size())\n",
    "    # 各要素を20次元のone-hotベクトルに変換\n",
    "    for i, val in enumerate(data):\n",
    "        index = int(torch.round((val - 0.05) / 0.05))\n",
    "        one_hot[i, index] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# label_train,temp_testをone-hotベクトルに変換\n",
    "one_hot_label_train = to_one_hot(label_train, num_classes=20)\n",
    "one_hot_label_test = to_one_hot(label_test, num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = TensorDataset(spin_train, one_hot_label_train)\n",
    "test_dataset = TensorDataset(spin_test, one_hot_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# データセットに前処理を適用\n",
    "transformed_train_dataset = [(transform(tensor_sample), label) for tensor_sample, label in train_dataset]\n",
    "transformed_test_dataset = [(transform(tensor_sample), label) for tensor_sample, label in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの設定\n",
    "train_loader = DataLoader(transformed_train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(transformed_test_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCNN(\n",
      "  (fc1): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 16*16\n",
    "hidden_size = 3\n",
    "output_size = 20\n",
    "model = FCNN(input_size, hidden_size, output_size)\n",
    "model.to(device)\n",
    "# モデルの概要表示\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と最適化アルゴリズムの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   # クロスエントロピー誤差\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)     # Adam,L2正則化{, weight_decay=5e-4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.982114176750183, acc: 0.08269992470741272, test loss: 2.966054104804993, test acc: 0.10350003838539124\n",
      "epoch: 1, loss: 2.958863051176071, acc: 0.10290005058050156, test loss: 2.954762621164322, test acc: 0.09805005043745041\n",
      "epoch: 2, loss: 2.9522410447597505, acc: 0.10200002789497375, test loss: 2.951401739835739, test acc: 0.0979500487446785\n",
      "epoch: 3, loss: 2.9496987159252166, acc: 0.10230010002851486, test loss: 2.949986057281494, test acc: 0.09860002994537354\n",
      "epoch: 4, loss: 2.9483461742401125, acc: 0.10240011662244797, test loss: 2.949053111076355, test acc: 0.09930005669593811\n",
      "epoch: 5, loss: 2.947365880727768, acc: 0.10440010577440262, test loss: 2.9483068642616272, test acc: 0.09990005940198898\n",
      "epoch: 6, loss: 2.946365038871765, acc: 0.10835004597902298, test loss: 2.947600288391113, test acc: 0.10235005617141724\n",
      "epoch: 7, loss: 2.9453415706157684, acc: 0.11315014213323593, test loss: 2.9467869360446928, test acc: 0.1078500896692276\n",
      "epoch: 8, loss: 2.9442682061195375, acc: 0.11814998835325241, test loss: 2.946217376232147, test acc: 0.1083001047372818\n",
      "epoch: 9, loss: 2.9428072118759157, acc: 0.12300008535385132, test loss: 2.9457259607315063, test acc: 0.11870007961988449\n",
      "epoch: 10, loss: 2.9412603690624235, acc: 0.1296001523733139, test loss: 2.9434445753097536, test acc: 0.12370006740093231\n",
      "epoch: 11, loss: 2.9396698977947233, acc: 0.13200007379055023, test loss: 2.944884552717209, test acc: 0.10399997979402542\n",
      "epoch: 12, loss: 2.938239017248154, acc: 0.13635000586509705, test loss: 2.9415053203105925, test acc: 0.13415005803108215\n",
      "epoch: 13, loss: 2.9370069551467894, acc: 0.1344500333070755, test loss: 2.941184650659561, test acc: 0.1119999811053276\n",
      "epoch: 14, loss: 2.93537157535553, acc: 0.13914994895458221, test loss: 2.9398608682155607, test acc: 0.13800010085105896\n",
      "epoch: 15, loss: 2.9342725093364717, acc: 0.13979999721050262, test loss: 2.9389159398078917, test acc: 0.13735008239746094\n",
      "epoch: 16, loss: 2.93389932179451, acc: 0.1404000073671341, test loss: 2.939051854133606, test acc: 0.13100002706050873\n",
      "epoch: 17, loss: 2.932280349731445, acc: 0.14325007796287537, test loss: 2.9368237657546996, test acc: 0.13940012454986572\n",
      "epoch: 18, loss: 2.931124046087265, acc: 0.14570006728172302, test loss: 2.937790334701538, test acc: 0.13220009207725525\n",
      "epoch: 19, loss: 2.9302226440906525, acc: 0.14925003051757812, test loss: 2.9359195408821104, test acc: 0.13860011100769043\n",
      "epoch: 20, loss: 2.9292282638549803, acc: 0.1462000459432602, test loss: 2.93463707780838, test acc: 0.14055009186267853\n",
      "epoch: 21, loss: 2.9286093864440916, acc: 0.14654994010925293, test loss: 2.934607893228531, test acc: 0.1401001214981079\n",
      "epoch: 22, loss: 2.9277455699443817, acc: 0.14819994568824768, test loss: 2.9361263077259063, test acc: 0.13220006227493286\n",
      "epoch: 23, loss: 2.9267372796535494, acc: 0.14859995245933533, test loss: 2.937844562292099, test acc: 0.12740002572536469\n",
      "epoch: 24, loss: 2.926712011575699, acc: 0.14759983122348785, test loss: 2.934056505680084, test acc: 0.13690006732940674\n",
      "epoch: 25, loss: 2.9256717841625215, acc: 0.14980001747608185, test loss: 2.9348258380889893, test acc: 0.13585005700588226\n",
      "epoch: 26, loss: 2.925229160785675, acc: 0.1488499939441681, test loss: 2.9320152304172518, test acc: 0.14120011031627655\n",
      "epoch: 27, loss: 2.9245318579673767, acc: 0.151150181889534, test loss: 2.934043253660202, test acc: 0.13480007648468018\n",
      "epoch: 28, loss: 2.923305110692978, acc: 0.1540500521659851, test loss: 2.9313955953121185, test acc: 0.14155010879039764\n",
      "epoch: 29, loss: 2.922706642627716, acc: 0.15335005521774292, test loss: 2.9311547231674195, test acc: 0.14070011675357819\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(output, dim=1)      \n",
    "        targets = torch.argmax(targets, dim=1) \n",
    "        running_acc += torch.mean(pred.eq(targets).float().cpu()) \n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader)   \n",
    "    running_acc /= len(train_loader)    \n",
    "    train_losses.append(running_loss)\n",
    "    train_accs.append(running_acc)\n",
    "    #\n",
    "    #   test loop\n",
    "    #\n",
    "    test_running_loss = 0.0\n",
    "    test_running_acc = 0.0\n",
    "    for test_inputs, test_targets in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        test_output = model(test_inputs)\n",
    "        test_loss = criterion(test_output, test_targets)\n",
    "        test_running_loss += test_loss.item()\n",
    "        test_pred = torch.argmax(test_output, dim=1)      \n",
    "        test_targets = torch.argmax(test_targets, dim=1)  \n",
    "        test_running_acc += torch.mean(test_pred.eq(test_targets).float().cpu()) \n",
    "    test_running_loss /= len(test_loader)   \n",
    "    test_running_acc /= len(test_loader)    \n",
    "    test_losses.append(test_running_loss)\n",
    "    test_accs.append(test_running_acc)\n",
    "        \n",
    "    print(\"epoch: {}, loss: {}, acc: {}, test loss: {}, test acc: {}\".format(epoch, running_loss, running_acc, test_running_loss, test_running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "FCNN\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "Ising_size = int(np.sqrt(input_size))\n",
    "print(Ising_size)\n",
    "class_name = type(model).__name__\n",
    "print(class_name)\n",
    "print(hidden_size)\n",
    "\n",
    "file_path = f'../data storage/prm_data_L{Ising_size}_{class_name}_Nh{hidden_size}.pth'\n",
    "\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
