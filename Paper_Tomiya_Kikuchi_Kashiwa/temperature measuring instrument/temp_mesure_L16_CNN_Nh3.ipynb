{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sugay\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cudaが使えるか確認\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUで作ったPickleのファイルは，CPUではそのままでは使えないため，工夫が必要．\n",
    "\n",
    "https://www.kunita-gamefactory.com/post/%E3%80%90pytorch%E3%80%91gpu%E3%81%A7%E8%A8%93%E7%B7%B4%E3%81%95%E3%81%9B%E3%81%9F%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92cpu%E3%81%A7%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%82%82%E3%81%86%E3%81%A8%E3%81%97%E3%81%9F%E3%81%8A%E8%A9%B1\n",
    "\n",
    "を真似したらうまくCPU上でもファイルを読み込むことができるようになった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "        \n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの相対パスを指定\n",
    "file_path = '../data storage/Ising_data_L16_v2.pkl'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "else:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = CPU_Unpickler(file).load()\n",
    "        \n",
    "# 読み込んだデータを個々の変数に分割\n",
    "spin_data, label_data = loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "磁化をすべて正に変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 磁化を計算\n",
    "# def magnetization(state):\n",
    "#     return np.mean(state)\n",
    "\n",
    "# for i in range(len(spin_data)):\n",
    "#     mag = magnetization(spin_data[i])\n",
    "#     if mag > 0:\n",
    "#         spin_data[i] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストをNumPy配列に変換\n",
    "spin_data_np = np.array(spin_data)\n",
    "label_data_np = np.array(label_data)\n",
    "\n",
    "# NumPy配列をPyTorchテンソルに変換\n",
    "spin_data_tensor = torch.from_numpy(spin_data_np)\n",
    "label_data_tensor = torch.from_numpy(label_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 1, 16, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spin_data_tensor = spin_data_tensor.unsqueeze(1)\n",
    "spin_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータを訓練用とテスト用に分割(5:5)\n",
    "spin_train, spin_test, label_train, label_test = train_test_split(spin_data_tensor, label_data_tensor, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_4536\\168836570.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_4536\\168836570.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_4536\\168836570.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_train = torch.tensor(label_train, dtype=torch.float32)\n",
      "C:\\Users\\sugay\\AppData\\Local\\Temp\\ipykernel_4536\\168836570.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_test = torch.tensor(label_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# PyTorchのテンソルに変換\n",
    "spin_train = torch.tensor(spin_train, dtype=torch.float32)\n",
    "spin_test = torch.tensor(spin_test, dtype=torch.float32)\n",
    "label_train = torch.tensor(label_train, dtype=torch.float32)  \n",
    "label_test = torch.tensor(label_test, dtype=torch.float32)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解データはone-hot表現にする必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルを新しいテンソルに変換する関数を定義\n",
    "def to_one_hot(data, num_classes=20):\n",
    "    # one-hotベクトルの初期化\n",
    "    one_hot = torch.zeros(len(data), num_classes)\n",
    "    # 各要素を20次元のone-hotベクトルに変換\n",
    "    for i, val in enumerate(data):\n",
    "        index = int(torch.round((val - 0.05) / 0.05))\n",
    "        one_hot[i, index] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# label_train,temp_testをone-hotベクトルに変換\n",
    "one_hot_label_train = to_one_hot(label_train, num_classes=20)\n",
    "one_hot_label_test = to_one_hot(label_test, num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_dataset = TensorDataset(spin_train, one_hot_label_train)\n",
    "test_dataset = TensorDataset(spin_test, one_hot_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# データセットに前処理を適用\n",
    "transformed_train_dataset = [(transform(tensor_sample), label) for tensor_sample, label in train_dataset]\n",
    "transformed_test_dataset = [(transform(tensor_sample), label) for tensor_sample, label in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの設定\n",
    "train_loader = DataLoader(transformed_train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(transformed_test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 前処理なしver\n",
    "# # DataLoaderの設定（バッチサイズ50）\n",
    "# train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # 畳み込み層\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2, stride=2, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=2, stride=2, bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=4, stride=4, bias=True)\n",
    "        \n",
    "        # 畳み込みの部分\n",
    "        self.conv = nn.Sequential(\n",
    "            self.conv1,   \n",
    "            nn.ReLU(inplace=True),\n",
    "            self.conv2,   \n",
    "            nn.ReLU(inplace=True),\n",
    "            self.conv3,   \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 全結合の部分\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3, output_size, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv2): Conv2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv3): Conv2d(32, 3, kernel_size=(4, 4), stride=(4, 4))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(32, 3, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=20, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 16*16\n",
    "output_size = 20\n",
    "\n",
    "model = CNN(output_size)\n",
    "model.to(device)\n",
    "# モデルの概要表示\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と最適化アルゴリズムの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   # クロスエントロピー誤差\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)     # Adam,L2正則化{, weight_decay=5e-4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.889665366077423, acc: 0.18440000712871552, test loss: 2.8569625540733337, test acc: 0.21565000712871552\n",
      "epoch: 1, loss: 2.8557348826408386, acc: 0.21960000693798065, test loss: 2.8453176785469054, test acc: 0.2212499976158142\n",
      "epoch: 2, loss: 2.8588135719299315, acc: 0.2168000042438507, test loss: 2.865742943572998, test acc: 0.21060000360012054\n",
      "epoch: 3, loss: 2.856329759693146, acc: 0.22005000710487366, test loss: 2.8627604972839356, test acc: 0.21529999375343323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sugay\\Desktop\\Master-s_thesis\\Paper_Tomiya_Kikuchi_Kashiwa\\temperature measuring instrument\\temp_mesure_L16_CNN_Nh3.ipynb セル 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sugay/Desktop/Master-s_thesis/Paper_Tomiya_Kikuchi_Kashiwa/temperature%20measuring%20instrument/temp_mesure_L16_CNN_Nh3.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sugay/Desktop/Master-s_thesis/Paper_Tomiya_Kikuchi_Kashiwa/temperature%20measuring%20instrument/temp_mesure_L16_CNN_Nh3.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, targets)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sugay/Desktop/Master-s_thesis/Paper_Tomiya_Kikuchi_Kashiwa/temperature%20measuring%20instrument/temp_mesure_L16_CNN_Nh3.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sugay/Desktop/Master-s_thesis/Paper_Tomiya_Kikuchi_Kashiwa/temperature%20measuring%20instrument/temp_mesure_L16_CNN_Nh3.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sugay/Desktop/Master-s_thesis/Paper_Tomiya_Kikuchi_Kashiwa/temperature%20measuring%20instrument/temp_mesure_L16_CNN_Nh3.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)      \n",
      "File \u001b[1;32mc:\\Users\\sugay\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\sugay\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(output, dim=1)      \n",
    "        targets = torch.argmax(targets, dim=1) \n",
    "        running_acc += torch.mean(pred.eq(targets).float().cpu()) \n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader)   \n",
    "    running_acc /= len(train_loader)    \n",
    "    train_losses.append(running_loss)\n",
    "    train_accs.append(running_acc)\n",
    "    #\n",
    "    #   test loop\n",
    "    #\n",
    "    test_running_loss = 0.0\n",
    "    test_running_acc = 0.0\n",
    "    for test_inputs, test_targets in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        test_output = model(test_inputs)\n",
    "        test_loss = criterion(test_output, test_targets)\n",
    "        test_running_loss += test_loss.item()\n",
    "        test_pred = torch.argmax(test_output, dim=1)      \n",
    "        test_targets = torch.argmax(test_targets, dim=1)  \n",
    "        test_running_acc += torch.mean(test_pred.eq(test_targets).float().cpu()) \n",
    "    test_running_loss /= len(test_loader)   \n",
    "    test_running_acc /= len(test_loader)    \n",
    "    test_losses.append(test_running_loss)\n",
    "    test_accs.append(test_running_acc)\n",
    "        \n",
    "    print(\"epoch: {}, loss: {}, acc: {}, test loss: {}, test acc: {}\".format(epoch, running_loss, running_acc, test_running_loss, test_running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ising_size = int(np.sqrt(input_size))\n",
    "hidden_size = 3\n",
    "class_name = type(model).__name__\n",
    "\n",
    "file_path = f'../data storage/prm_data_L{Ising_size}_{class_name}_Nh{hidden_size}.pth'\n",
    "\n",
    "# torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
