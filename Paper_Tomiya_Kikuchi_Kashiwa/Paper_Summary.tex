\documentclass[a4paper,11pt]{jsarticle}


% 数式
\usepackage{amsmath,amsfonts}
\usepackage{bm}
% 画像
\usepackage[dvipdfmx]{graphicx}


\begin{document}

\title{Phase transition encoded in neural network}
\author{須賀勇貴}
\date{最終更新日：\today}
\maketitle
これは，柏，菊池，富谷先生による論文"Phase transition encoded in neural network"を日本語でまとめたものである．\par

\subsection*{Abstract}
相転移検出を目的としたニューラルネットワークの一側面について論じる．そのために，まず温度でラベル付けされたイジング模型とPotss模型の配位データで温度を測定させるようにニューラルネットワークを学習させる．ここで，私たちは，機械に配位が秩序相にあるか無秩序相にあるかは明示的に与えていないことに注意．にもかかわらず，学習したニューラルネットワークはパラメータ(重みをバイアス)から臨界温度を特定することができる．私たちは，温度教師付きニューラルネットワークがどのような量を学習するかに注目することで，相転移の情報をどのように捉えるか理解しようと試みる．私たちの詳細な分析により，機械は訓練の程度に応じて異なる物理量を学習していることが明らかになった．この研究の主な観察は，学習済みニューラルネットワークの重みが温度に加えて，相転移の情報をどのように持つかということである．

\section{はじめに}
物質の相を探索することは，基礎となる微視的な物理システムの赤外線構造を明らかにするための最も重要なタスクである．これらの相は，理論の持つ対称性に基づいて分類される．いくつかの異なる相を含む理論では，相転移はそれらの境界で発生する．それらのうち，二次相転移の性質は次元の数によってのみ決定され，微視的な詳細とは独立した，大域的対称性の基礎理論，つまり，普遍性クラスによって分類される．しかしながら，実際のところ，微視的な理論データに基づいて位相を解析したり，相転移を検出したりすることは一般的に難しい課題である．なぜなら，それらの問題を正確に解決する，もしくは対応する赤外理論を特定することが難しいためである．したがって，これらの問題を解明するためには，数値的なアプローチが大量に取り組まれてきた．明白で主要な障害は，自由度の数が増加するにつれて，数値解析がますます困難になることである．\par
機械学習はコンピュータサイエンスの分野で急速に発展し，パターン認識，画像処理などで顕著な成功を収めてきた．最近では，機械学習が物理学のさまざまな分野でも適用されていることが目撃されている．相転移の検出は，機械学習が新たな進展を遂げる可能性がある興味深い例の一つであり，すでにスピンシステムなどの単純なモデルでいくつかアプローチが提案され，試験されている．これらの研究はでは，教師あり学習では[3-18]，教師なし学習では[19-24]などで行われている．ここでは，入力データはニューラルネットワークやそのトレーニングプロセスとは独立して準備されてる．例えば，モンテカルロシミュレーションや興味を持つ物理系に基づく実験データが，必要な入力データを提供する．\par
物理系の相境界を検出する一つのアプローチは，教師ありバイナリ分類である，ここでは，ニューラルネットワークがトレーニングされ，それが秩序だったり無秩序だったりする相を区別できるようになる．実際に，このアプローチは生データからいくつかのモデルの相転移を合理的に検出する[3]．最近では，潜在変数と物理量との間の暗黙のつながりに関する研究が広く行われ，機械学習技術のブラックボックス性を取り除く究極の目標に向けて進展が見られている[3, 12, 15, 18]．また，物理的な洞察に基づく特徴エンジニアリングの支援を受けて，トポロジカル相転移やBKT相転移などの非標準的な相転移も検出に成功した[10, 12, 14, 16, 25, 26]．\par
別の興味深いアプローチが[4]で提案され，相転移を検出するために，秩序パラメータの情報がトレーニングの結果としてニューラルネットワークの重みにエンコードされていると仮定している．彼らは教師あり機械学習を用いて，2次元のIsingモデルの臨界温度を特定しようとした．完全に連結したネットワークと畳み込みニューラルネットワークは，入力のスピン構造の目標温度を正確に予測できるようにトレーニングされた．驚くべきことに，トレーニング中に相転移に関する直接の情報を与えていないにもかかわらず，彼らは重みから相転移温度を抽出することに成功した．これは，ネットワークが温度の教師付き学習の過程で相転移を自発的に捉え，機械パラメータにエンコードしていることを意味しています．ただし，その結果の基本的なメカニズムは未解決のままである．\par
この記事の目的は，この相転移検出のメカニズムを理解することである．このアプローチが捉えるものを理解することは，それを他の未知のシステムに適用しようとする際に役立つ．異なる物理的メカニズムによって引き起こされる場合，例えば準長距離秩序（BKT転移）や位相など、正確に相転移を検出できない可能性があるからである，実際，この方法は，ニューラルネットワークのトレーニング方法によって異なる物理量，つまり入力構成の特徴を捉えることが分かる．アイデアを説明するために，温度予測の本質を具現化した簡略化されたニューラルネットワークの構造に注目する．

\section{臨界温度予測}
2次元イジング模型を考える．ハミルトニアンは
\begin{equation}
  H(\bm{\sigma}) = - J \sum_{\langle i,j \rangle} \sigma_i \sigma_j
\end{equation}
ここで，$J$は結合定数，$\sigma_i \in \{ -1, 1 \}$は，サイズが$L \times L$の正方格子のサイトに存在するスピンの自由度(スピン変数)で，周期的境界条件が課されている．和は最近接に対するサイトにわたる．結合定数をボルツマン重みに吸収させて，$e^{-KH}/Z, \ \ K = \beta J$とすることで，ハミルトニアンを次のように再定義する．
\begin{equation}
  H(\bm{\sigma}) = - \sum_{\langle i,j \rangle} \sigma_i \sigma_j
\end{equation}
ここで，$Z = \sum_{\bm{\sigma}}e^{-KH(\bm{\sigma})}$は分配関数である．\par
我々は，その2次相転移に関連する臨界温度を検出しようと試みている．しかしながら，最初は，教師あり順伝播ニューラルネットワークを用いて温度測定器を構築することを行う．その後，訓練されたニューラルネットワークの重みとバイアスを調べ，2次元イジング模型と3状態のポッツ模型の臨界温度を特定しようと試みる．\par
メトロポリス・ヘイスティングス法を用いて，各温度で2000個のイジングスピン配位を生成する．これらは，20の目標温度と一緒にニューラルネットワークに供給される．我々は，TENSORFLOWをバックエンドに使用したKERASパッケージを用いて，全結合ニューラルネットワークと畳み込みニューラルネットワークの2種類のアーキテクチャを実装した．\par
全結合ニューラルネットワーク：
\begin{equation}
  \left[
    \begin{aligned}
       & \mathcal{I} = \left\{ \{ \sigma_i \} \Big| \ \text{Ising configs on} \ L \times L \ \text{lattice.} \right\} \\
       & \downarrow
      \begin{cases}
        \text{Fully-connected (Dense) layer} \\
        \text{Softmax activation}
      \end{cases}                                                                            \\
       & x_a \in [0,1]^{N_h} \ \text{: hidden units}                                                                  \\
       & \downarrow
      \begin{cases}
        \text{Fully-connected (Dense) layer} \\
        \text{Softmax activation}
      \end{cases}                                                                            \\
       & y_K \in [0,1]^{N_o} \ \text{: output}
    \end{aligned}
    \right]
\end{equation}

入力の自由度は$\{ \sigma_i \} \ (i=1,2,\dots,L \times L)$で，イジングモデルの場合スピンに対応する．隠れ層のユニット$x_a \ (a=1,2,\dots,N_b)$は
\begin{equation}
  x_a = \text{softmax}(w_{ai}^{(1)}\sigma_i + b_a^{(1)})
  \equiv \frac{e^{w_{ai}^{(1)}\sigma_i + b_a^{(1)}}}{\sum_a e^{w_{ai}^{(1)}\sigma_i + b_a^{(1)}}}
\end{equation}
ここで，アインシュタインの縮約記法を使用していることに注意．2つ目の層は，重み$w_{\alpha a}^{(2)}$とバイアス$b_{\alpha}^{(2)}$に置き換えたものである．最終層の変数$y_K \ (K=1,2,\dots,N_o)$は隠れ層と同じ形をとる
\begin{equation}
  y_k = \text{softmax}(w_{ai}^{(2)}\sigma_i + b_a^{(2)})
\end{equation}
出力$\{ y_K \}$に基づいて，入力配位の温度は
\begin{equation}
  K^{\text{output}} = \underset{K} {\operatorname{argmin}} (y_K)
\end{equation}
で決定される．つまり，温度$\alpha$は確率分布$y_K$の中から最も値が高い成分採用されるということである．訓練は，Adamという最適化手法を用いて，交差エントロピー
\begin{equation}
  E(y_K,\bm{1}_{K=K_i^{\text{target}}}) = - \frac{1}{N_o}\sum_i \bm{1}_{K=K_i^{\text{target}}} \ln{y_k}
\end{equation}
という誤差関数を最小することによって実装した．ここで，$i$ は入力配位のラベルを示す．インジケーター関数は次のように定義される．\par
\begin{equation}
  \bm{1}_{K=K_i^{\text{target}}} =
  \begin{cases}
    1 & (K=K_i^{\text{target}}) \\
    0 & (\text{otherwize})
  \end{cases}
\end{equation}

畳み込みニューラルネットワークは3つの畳み込み層と最後の全結合層からなる式(\ref{})．\par
手順を示す．
\begin{enumerate}
  \item マルコフ連鎖モンテカルロ法より，配位を生成：
  \item ニューラルネットワークを温度計としてトレーニングする．入力はスピン配位，出力は予測温度．
  \item ニューラルネットワークに現れる学習済みの重みとバイアスを分析する．マシンパラメーターに相転移の情報がどのように含まれるかについて説明する．
\end{enumerate}

\subsection{2次元イジングモデル}
\begin{figure}[hb]
  \begin{center}
    \includegraphics[height=5cm]{image/Figure1.png}
    \caption{2次元イジング模型における訓練後の全結合層での重みの値とそれらを平均した値．横軸はそれぞれ入力配位の温度$K$を表している．縦軸は左の図はニューラルネットワークの隠れ層につながるエッジにおける重みの値に対応し，右の図はその重みの値を$K$での平均をとったものである．重みの平均の値は，厳密な臨界温度$K_{\text{c}}^{\text{exact}}\simeq 0.4407$付近で大きく変化していることが見て取れる．}
  \end{center}
\end{figure}
最初に，[4] (田中さんと富谷さんの論文)で既に研究されている2D Isingモデルを見ていく．このモデルは，100の目標温度で調査され，その重みは秩序変数のように振る舞い，すなわち自発的な磁化を示す．私たちの主な目的は臨界温度を定量的に推定することではなく，そのメカニズムを理解することなので，目標温度の数を20に減少させる．具体的には，$K = 0.05, 0.1, 0.15, \dots, 1.0$のような値である．ニューラルネットワークより教師あり機械学習を行った．

図１
2D Isingモデルの場合，全結合層の重みおよびそれらのトレーニング後の平均について説明する．横軸は入力構成の温度Kを表し，左のパネルの縦軸はニューラルネットワークの隠れユニットに接続された成分に対応している．右のパネルの縦軸は，各Kに対する重みの平均を示している．重みの平均値は，正確な臨界温度$K_e^{\text{exact}} \approx 0.4407$の周りで著しく変化している．


格子サイズは$L = 16$であり,隠れユニットの数は$N_h = 80$，そして$N_o = 20$はそれぞれ20の目標温度に対応している．臨界温度は正確に知られており，$K_c^{\text{exact}} = \frac{1}{2}\ln{\sqrt{2}+1} \approx 0.4407$である．訓練後の2層目の重みが図1である([4]より)．臨界温度は重みの和を$c_1 \tanh{[c_2(K-K_c)]}$でフィッティングしてパラメータ$c_1,c_2,c_3$を導くことで予想する．実験では格子サイズ$L=8,16,32$で行った．実際，最終的な重みの平均は秩序変数のように振る舞うようです（図1の右パネル）．次のセクションで重みの詳細な構造について議論します．


\subsection{2次元3状態ポッツ模型}
\begin{figure}[hb]
  \begin{center}
    \includegraphics[height=5cm]{image/Figure2.png}
    \caption{2次元3状態ポッツ模型における訓練後の全結合層での重みの値とそれらを平均した値．横軸はそれぞれ入力配位の温度$K$を表している．縦軸は左の図はニューラルネットワークの隠れ層につながるエッジにおける重みの値に対応し，右の図はその重みの値を$K$での平均をとったものである．重みの平均の値は，厳密な臨界温度$K_{\text{c}}^{\text{exact}}\simeq 1.0050$付近で大きく変化していることが見て取れる．}
  \end{center}
\end{figure}
2次元イジング模型の臨界温度の学習メカニズムの詳細に入る前に，別の例として2次元3状態ポッツモデルを見ていく．ハミルトニアンは以下のように表される．
\begin{equation}
  H(\{\Phi_i\}) = - \sum_{\langle i,j \rangle} \delta(\Phi_i, \Phi_j)
\end{equation}
ここで，$\Phi_i$は三つの値を取り,これはIsingスピン$\sigma_i$の一般化である．したがって，温度$K$でラベル付けされた構成$\{ \Phi_i \}$がニューラルネットワークの入力である．2次元3状態ポッツモデルは，単純なランダウ理論とは異なり，ゆらぎの影響で$K_c \approx 1.0050$で2次の位相転移を示すことが知られている[29-31]．

\section{DISCUSSION}
これまでに，第二層の重みの値の変化が位相転移を示していることを観察してきた．これは，臨界温度の情報がなんらかの形で訓練されたニューラルネットワークにコード化されていることを示唆している．以下では，訓練された全結合型／畳み込み型のニューラルネットワークを注意深く検証し，それらが入力構成の特徴として抽出する（物理的な）量が何であり，それが温度の予測や臨界温度の検出とどのように関連しているかを理解しようとする[15, 17]．

\subsection{ニューラルネットワークでエンコードされた磁化}
\begin{figure}[hb]
  \begin{center}
    \includegraphics[height=5cm]{image/Figure3.png}
    \caption{隠れ層のユニットが3つ$(N_h=3)$の学習済みニューラルネットワークにおける全結合層の重み$w_{Ka}^{(2)}$の値．横軸は2次元イジング模型での温度$K$を表している．臨界温度付近で構造に変化が起きていることがわかる．真ん中の重みだけ値が反対になっている．}
  \end{center}
\end{figure}
\begin{figure}[hb]
  \begin{center}
    \includegraphics[height=5cm]{image/Figure4.png}
    \caption{(a)第1層目の出力と入力配位の磁化の相関．横軸は入力配位の各サイトでの磁化，縦軸は$\tilde{x}_a=w_{ai}^{(1)}\sigma_i+b_a^{(1)}$である．(b)フィッティングしたもの．}
  \end{center}
\end{figure}
相転移の秩序変数が2D Isingモデルにおいて自発磁化であることから，学習後にはその情報がニューラルネットワークにエンコードされていることは自然なことと言える．定量的な論拠を示すために，2D Isingモデルの場合においてトレーニングされたニューラルネットワークの重みとバイアスを調査して，簡略化されたモデルを構築する．最初に、簡略化のために（2）の中の隠れユニットの数を80から3に減少させる．図3に示されているように，それでも臨界温度を捉えていることに気づく．\par
第二層をモデリングする前に，まず最初の層の特性を調査する．図4は，式（3）の第一層の出力$\tilde{x}_a \equiv w_{ai}^{(1)}\sigma_i + b_a^{(1)}$と，入力のIsingスピン配位の磁化密度との相関を示している．この観察から，我々はこれらの出力から，図4bに示すように，磁化$m(\{ \sigma \})$に対して線形な三つのラインでモデリングする．
\begin{equation}
  \tilde{x} =
  \begin{pmatrix}
    \tilde{x}_0 \\ \tilde{x}_1 \\ \tilde{x}_2
  \end{pmatrix}
  =
  \begin{pmatrix}
    -m \\ \epsilon \\ m
  \end{pmatrix}
\end{equation}
ここで，$\epsilon > 0$は定数である．さらに，活性化関数として，我々の目的のためにsoftmaxの代わりに最大値を1，それ以外を0にする最大値関数を使用している．この変更は最終結果に影響を与えない．$x_a = \max{(\tilde{x}_a)}$は次のベクトルを生成する．
\begin{equation}
  m<-\epsilon : x=
  \begin{pmatrix}
    1 \\ 0 \\ 0
  \end{pmatrix}, \
  -\epsilon \leq m < \epsilon : x=
  \begin{pmatrix}
    0 \\ 1 \\ 0
  \end{pmatrix}, \
  \epsilon \leq m : x=
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix},
\end{equation}
パラメータ$\epsilon$は、強磁性相と常磁性相を分離する閾値磁化と解釈できる[3].\par
三つの隠れユニットの磁化依存性を理解したら，次に第二層を分析する．この層の重みは図3に示されている．温度を低温，臨界温度，高温の三つの部分に分割する．それぞれ以下のベクトルで表される．
\begin{equation}
  \text{Low} \ \text{K} :
  \begin{pmatrix}
    1 \\ 0 \\ 0
  \end{pmatrix}, \
  \text{Critical} \ \text{K} :
  \begin{pmatrix}
    0 \\ 1 \\ 0
  \end{pmatrix}, \
  \text{High} \ \text{K} :
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix},
\end{equation}
No-dimensional output spaceにおいて．この手法により，出力次元$N_o$を実質的に3に削減する．図3に従って，我々は以下のように重みをパラメータ化する．
\begin{equation}
  w^{(2)} =
  \begin{pmatrix}
    -\Delta & 0       & -\Delta \\
    -\delta & -\delta & -\delta \\
    0       & -\Delta & 0
  \end{pmatrix}
\end{equation}
ここで，$\Delta > \delta > 0$．行列$w^{(2)}$の$Ka$成分は$w_{Ka}^{(2)}$に対応する．バイアスは重みよりもはるかに小さいため，無視する．厳密なパラメータ化は，以下の議論には必要ない．このとき，$y_K = \max{(\tilde{y}_K)}=\max{(w_{Ka}^{(2)}x_a+b_K^{(2)})}$は以下の出力を生む．
\begin{align}
  m<-\epsilon : y_K = \max(w_{K0}^{(2)}) =
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix}, \\
  -\epsilon \leq m < \epsilon : y_K = \max(w_{K1}^{(2)}) =
  \begin{pmatrix}
    1 \\ 0 \\ 0
  \end{pmatrix}, \\
  \epsilon \leq m : y_K = \max(w_{K2}^{(2)}) =
  \begin{pmatrix}
    0 \\ 0 \\ 1
  \end{pmatrix},
\end{align}
これらはそれぞれ，低温，高温，低温と予想される．$m<-\varepsilon$と$m>\varepsilon$のスピン配位は秩序相にあるため，正しく予想できている．また，$-\varepsilon \leq m < \varepsilon$もまた，正しく予想されている．ただし，この場合には中間温度，つまり臨界温度を検出することができない．さらに，式(\ref{}) に三つ以上の対象温度を導入しても，訓練されたニューラルネットワークは順序/無秩序相内の異なる温度を区別することができない．なぜなら，図3で示されているように，重み（およびバイアス）の上部ブランチはそれぞれ$K_c$より上と下でほぼ温度に依存しないからである．したがって，高温または低温のみを区別することができる．\par
これは，三つの隠れユニットにより単一の閾値パラメータ$\varepsilon$しか持たないことが原因かもれない．隠れユニットをさらに導入することで閾値パラメータの数を増やすことができる．しかし，これは温度の高い精度にはつながらない．実際，隠れユニットの数を増やしても、，二層の重みは温度依存性の二つのパターンしか示さない．つまり，多くは図3の青と緑の曲線のようにふるまい，残りはオレンジの振舞いを見せる．これは，図1で観察されている現象とまさに一致している．隠れユニットを増やすことは単に，すでに図3で観察された第二層の重みを複製する結果となり，その結果，導入された隠れユニットの数に関係なく，予測される温度は高いか低いかのどちらかとなる．\par
上記の分析から得られた結論は次の通りである．
ネットワークは，第一層の出力で示す磁化の情報を取得した．しかしながら，ニューラルネットワークは磁化に基づいて秩序相と無秩序相の違いを除いて，温度を区別することは難しい．機械学習パラメータの観点から見ると，これは第二層の重みが臨界温度の周りを除いて温度に依存しないという事実に起因している．

\subsection{エネルギーと温度の予測}
前のサブセクションで，2次元イジング模型の磁化が温度の教師ありニューラルネットワークに組み込まれていることがわかった．これにより，その重み構造から臨界温度を読み取ることが可能になる．臨界温度はうまく検出されているようだが，温度の予測自体についてはまだ議論していない．興味深いことに，2次元イジング模型の場合，上記の設定で温度学習の精度は理論的に$40.1\%$と計算できます．これは機械学習による温度予測の精度の上限を示しています（[32]．詳細は付録Aを参照）．ただし、全結合ニューラルネットワークによる温度予測のテスト精度は$16.8\%$であり，理論的に予測された$40.1\%$の精度にはほど遠い．磁化を抽出することで既に見てきたように，相転移の検出には必要なかったにもかかわらず，温度予測の観点からはより良いトレーニングが可能でした．温度の予測精度が向上するようなニューラルネットワークを構築できた場合，どのようなことが起こるのか？\par
この疑問に答えるために，次に示す畳み込みニューラルネットワークを使用して，より高い精度を実現した．
\begin{equation}
  \begin{bmatrix}
    \begin{aligned}
       & \mathcal{I} = \left\{ \{ \sigma_i \} \Big| \ \text{Ising configs on} \ L \times L \ \text{lattice.} \right\}        \\
       & \downarrow
      \begin{cases}
        \text{Convolution}_{[(s_1,s_1)\text{-filter}, \ (s_1,s_1)\text{-stride}, \ C_1\text{-channels}]} \\
        \text{ReLU activation}                                                                           \\
        \text{Convolution}_{[(s_2,s_2)\text{-filter}, \ (s_2,s_2)\text{-stride}, \ C_2\text{-channels}]} \\
        \text{ReLU activation}                                                                           \\
        \text{Convolution}_{[(s_3,s_3)\text{-filter}, \ (s_3,s_3)\text{-stride}, \ C_3\text{-channels}]} \\
        \text{ReLU activation}                                                                           \\
        \text{Flatten}
      \end{cases} \\
       & x_a \in [0,1]^{N_h} \ \text{: hidden units}                                                                         \\
       & \downarrow
      \begin{cases}
        \text{Fully-connected (Dense) layer} \\
        \text{Softmax activation}
      \end{cases}                                                                                   \\
       & y_K \in [0,1]^{N_o} \ \text{: output}
    \end{aligned}
  \end{bmatrix}
\end{equation}
このネットワークは，3つの畳み込み層から構成されている．それぞれフィルタサイズ$(s_i,s_i)$，ストライド$(s_i,s_i)$，そして，チャンネル数$C_i$であり，それらの間にはReLU関数が入る．その後，出力は全結合層に渡される．この層の入力と出力は私たち興味の対象であり，詳細に分析される．第二最終層の出力，記号で$x_a$と示されるものは，以下のように表される．
\begin{equation}
  x_a = \text{ReLU}(\tilde{x}_a) = \text{Mat}(\tilde{x}_a,0)
\end{equation}
これは$[0,\infty)$の範囲の値をとる．$\tilde{x}_a$は前の層のReLU関数の渡す前の出力である．その後、出力$x_a$は全結合層の入力として機能し,式(4)と(5)を通じて温度の予測を行う．







\end{document}